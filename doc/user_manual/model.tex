\section{Models  \\ \vspace{2 mm} {\small }}
\label{sec:models}
In the RAVEN code a crucial entity is represented by a Model. A model is an object that employs a mathematical representation of a phenomenology, either physical or of other nature (e.g. statistical operators, etc.). From a practical point of view, it can be seen, as a ``black box'' that, given an input, returns an output. 
\\ In the RAVEN code, a strict  classification of the different models is present. As obviously, each ``class'' of models is represented by the definition reported above, but it can be further classified based on the peculiar functionalities:
\begin{itemize}
\item \textbf{Code}. This ``class'' is the representation of an external system code that employs an high fidelity physical model;
\item \textbf{Dummy}. The ``Dummy'' object is a model that acts as ``transfer'' tool. The only action it performs is transferring the the information in the input space (inputs) into the output space (outputs). For example, it can be used to check the effect of a Sampling strategy, since its outputs are the sampled parameters' values (input space) and a counter that keeps track of the number of times an evaluation has been requested; 
\item \textbf{ROM}. A ROM is a mathematical model of fast solution trained to predict a response of interest of a physical system. The ``training'' process is performed by “sampling” the response of a physical model with respect variation of its parameters subject to probabilistic behavior. The results (outcomes of the physical model) of those sampling are fed into the algorithm representing the ROM that tunes itself to replicate those results;
\item \textbf{ExternalModel}. As the name suggests, an external model  is an entity that is embedded in the RAVEN code at run time. This object allows the user to create a python module that is going to be treated as a predefined internal model object;
%\item [Projector:] generic data manipulator
\item \textbf{PostProcessor}.  The post-processor ``class'' of objects  is the container of all the actions that can be performed to manipulate and process the data in order to extract key information, such as statistical quantities, etc. 
\end{itemize}
Before analyzing  each model in details, it is important to mention that each type needs to be contained in the main XML node $<Models>$, as reported below:

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <WhatEverModel name='whatever'>
      ... 
    </WhatEverModel>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
In the following sub-sections each \textbf{Model} type is fully analyzed and described.
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  Code  Model   %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%<Code name='MyRAVEN' subType='RAVEN'><executable>%FRAMEWORK_DIR%/../RAVEN-%METHOD%</executable></Code>
%<alias variable='internal_variable_name'>Material|Fuel|thermal_conductivity</alias>
\subsection{Code}
\label{subsec:models_code}
As already mentioned, the model \textbf{Code} is the representation of an external system software that employs an high fidelity physical model. The link between RAVEN and the driven code is performed at run time, through coded interfaces that are the responsible of transferring the information from the code to RAVEN and vice versa. In section \ref{sec:existingInterface} all the available interfaces are reported and, for advanced users, section \ref{sec:newCodeCoupling} explains how to couple a newer code.
\\ The specifications of this Model must be defined within the xml block $<Code>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, in this attribute the user selects the code that needs to be associated to this Model. NB. See section \ref{sec:existingInterface} to check which codes are currently supported.
\end{itemize}
\vspace{-5mm}

In the \textbf{Code} input block, the following XML sub-nodes are available:
\begin{itemize}
   \item $<executable>$ \textbf{\textit{, string, required field.}}. In this node, the user needs to specify the path of the executable to be used. NB. In this node, either the absolute or relative path can be inputted;
    \item $<alias>$ \textbf{\textit{, string, optional field.}}. In the $<alias>$ block the user can specify aliases for some variables of interest coming from the code this model refers to. These aliases can be used in the whole input to refer to the code variables. In the body of this node the user specifies the name of the variable that RAVEN will look for in the output files of the code. The actual alias, usable throughout the input, are instead defined in the attribute \textbf{variable}.
 NB. The user can specify as many aliases as needed. \textit{Default = None}. 
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <Code name='***' subType='RAVEN_Driven_code'>
      <executable>path_to_executable</executable>
      <alias variable='internal_variable_name1'>
         External_Code_Variable_Name_1
      </alias>
      <alias variable='internal_variable_name2'>
         External_Code_Variable_Name_2
      </alias>
    </Code>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Dummy Model  %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dummy.}
\label{subsec:models_dummy}
The model \textbf{Dummy} is an object that acts as ``transfer'' tool. The only action it performs is transferring the the information in the input space (inputs) into the output space (outputs). For example, it can be used to check the effect of a Sampling strategy, since its outputs are the sampled parameters' values (input space) and a counter that keeps track of the number of times an evaluation has been requested.
\\ The specifications of this Model must be defined within the xml block $<Dummy>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, this attribute must be kept empty.
\end{itemize}
\vspace{-5mm}
If this model, in a \textit{Step}, is associated to a \textit{Data} with the role of \textbf{Output}, it expects that one of the output parameters of such \textit{Data} is identified by the keyword ``OutputPlaceHolder'' (see section \ref{sec:steps}).

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <Dummy name='***' subType=''/>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%
%%%%% ROM Model  %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{ROM}
\label{subsec:models_ROM}
A Reduced Order Model (ROM) is a mathematical model of fast solution trained to predict a response of interest of a physical system. The ``training'' process is performed by “sampling” the response of a physical model with respect variation of its parameters subject, for example, to probabilistic behavior. The results (outcomes of the physical model) of those sampling are fed into the algorithm representing the ROM that tunes itself to replicate those results.
RAVEN supports several different types of ROMs, both internally developed and imported through an external library called ``SciKitLearn'' ~\cite{SciKitLearn}. Currently in RAVEN the Reduced Order Models are classified in 4 main ``classes'' that, once chosen, provide access to several different algorithms:
\begin{itemize}
   \item \textbf{NDspline;}
   \item \textbf{NDinvDistWeigth;}
   \item \textbf{microSphere;}
   \item \textbf{SciKitLearn.}
\end{itemize}
The specifications of this Model must be defined within the XML block $<ROM>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, in this attribute the user defines which of the main ``classes'' needs to be used, choosing among the previously reported types. Obviously, this choice conditions the subsequent the required and/or optional $<ROM>$ sub nodes.
\end{itemize}
\vspace{-5mm}

In the \textbf{ROM} input block, the following XML sub-nodes are required, independently on the ``main'' class inputted in the attribute \textit{subType}:
\begin{itemize}
   \item $<Features>$ \textbf{\textit{, comma separated string, required field.}}. In this node, the user needs to specify the names of the features of this ROM. NB. These parameters are going to be requested for the training of this object (see section \ref{subsec:stepTraining};
    \item $<Target>$ \textbf{\textit{, comma separated string, required field.}}. This XML node contains a comma separated list of the targets of this ROM. By Instance, these parameters are the Figure of Merits this ROM is supposed to predict. NB. These parameters are going to be requested for the training of this object (see section \ref{subsec:stepTraining}.
\end{itemize}
As already mentioned, all the types and meaning of the remaining sub-nodes depend on the main ``class'' type specified in the attribute \textit{subType}. In the following sections the specifications of each type are reported.
%%%%% ROM Model - NDspline  %%%%%%%
\subsubsection{NDspline.}
\label{subsubsec:NDspline}
The main ``class'' NDspline contains a single ROM type, based on a N-Dimensional spline interpolation/extrapolation. The spline interpolation is a form of interpolation where the interpolant is a special type of piecewise polynomial called a spline. The interpolation error can be made small even when using low degree polynomials for the spline. Spline interpolation avoids the problem of Runge's phenomenon, in which oscillation can occur between points when interpolating using high degree polynomials.
\\In order to use this Reduced Order Model, the $<ROM>$ attribute \textit{subType} needs to be ``NDspline'' (i.e. \textit{subType = ``NDspline''}). No further XML sub-nodes are required.
\\NB. This ROM type must be trained from a Regular Cartesian Grid. By instance, it can only be trained from the outcomes of a Grid Sampling strategy. 

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <ROM name='***' subType='NDspline'>
       <Features>***,***,***</Features> 
       <Target>***,***</Target>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
%%%%% ROM Model - NDinvDistWeigth  %%%%%%%
\subsubsection{NDinvDistWeigth.}
\label{subsubsec:NDinvDistWeigth}
The main ``class'' NDinvDistWeigth contains a single ROM type, based on a N-Dimensional Inverse Distance Weighting formulation. Inverse Distance Weighting (IDW) is a type of deterministic method for multivariate interpolation with a known scattered set of points. The assigned values to unknown points are calculated with a weighted average of the values available at the known points. 
\\In order to use this Reduced Order Model, the $<ROM>$ attribute \textit{subType} needs to be ``NDinvDistWeigth'' (i.e. \textit{subType = ``NDinvDistWeigth''}). The specification of the ROM \textit{``NDinvDistWeigth''} needs to be completed inputting, within the main XML node $<ROM>$, of the following sub-node:
\begin{itemize}
\item $<p>$ \textbf{\textit{, integer, required field.}}. This node contains an $integer > 0$ that represents the ``power parameter'. For the choice of value for $<p>$,it is necessary to consider the degree of smoothing desired in the interpolation/extrapolation, the density and distribution of samples being interpolated, and the maximum distance over which an individual sample is allowed to influence the surrounding ones (lower p means greater importance for points faraway).
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <ROM name='***' subType='NDinvDistWeigth'>
       <Features>***,***,***</Features> 
       <Target>***</Target>
       <p>3</p>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
%%%%% ROM Model - MicroSphere  %%%%%%%
\subsubsection{MicroSphere.}
\label{subsubsec:microSphere}
Not yet functional. Its validity for prediction purposes  needs to be still assessed.
%%%%% ROM Model - SciKitLearn  %%%%%%%
\subsubsection{SciKitLearn.}
\label{subsubsec:SciKitLearn}
The main ``class'' SciKitLearn represents the container of several Reduced Order Models that are available in RAVEN through the external library SciKitLearn~\cite{SciKitLearn}.
\\In order to use this Reduced Order Model, the $<ROM>$ attribute \textit{subType} needs to be ``SciKitLearn'' (i.e. \textit{subType = ``SciKitLearn''}). The specifications of the ROM \textit{``SciKitLearn''} depends on value assumed by the following sub-node within the main XML node $<ROM>$:
\begin{itemize}
\item $<SKLtype>$ \textbf{\textit{, vertical bar ($\vert$) separated string , required field.}}. This nodes contains a string that represents the ROM type that needs to be used. As mentioned, its format is, for example, $<SKLtype>$\textit{mainSKLclass}~$\vert$~\textit{algorithm} $</SKLtype>$: the first word (before symbol $\vert$) represents the main class of algorithms; the second word (after symbol $\vert$) represents the specific algorithm.
\end{itemize}
Based on the $<SKLtype>$ several different algorithms are available. In the following paragraphs a brief explanation and the input requirements are reported for each of them.
%%%%% ROM Model - SciKitLearn: Linear Models %%%%%%%
\paragraph{Linear Models.}
\label{LinearModels}
The LinearModels' type of algorithms implement generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.
In the following, all the linear models available in RAVEN are reported.
\subparagraph{Linear Model: Automatic Relevance Determination regression}
\mbox{}
\\The \textit{Automatic Relevance Determination} regressor is a hierarchical Bayesian approach where there are hyperparameters which explicitly represent the relevance of different input features. These relevance hyperparameters determine the range of variation for the parameters relating to a particular input, usually by modelling the width of a zero-mean Gaussian prior on those parameters. If the width of that Gaussian is zero, then those parameters are constrained to be zero, and the corresponding input cannot have any effect on the predictions, therefore making it irrelevant. ARD optimizes these hyperparameters to discover which inputs are relevant.
In order to use the  \textit{Automatic Relevance Determination} regressor, the user needs to set the sub-node $<SKLtype>ARDRegression</SKLtype>$. In addition to this XML node, several other need (or not) be inputted:
\begin{itemize}
\item $<n_iter>$ \textbf{\textit{, integer, optional field.}}.  Maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  Stop the algorithm if w has converged. \textit{Default = 1.e-3};
\item $<alpha_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<alpha_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<lambda_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<lambda_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<compute_score>$ \textbf{\textit{, boolean, optional field.}}.  If True, compute the objective function at each step of the model. \textit{Default =  False};
\item $<threshold_lambda>$ \textbf{\textit{, float, optional field.}}.  Threshold for removing (pruning) weights with high precision from the computation. \textit{Default =  1.e+4};
\item $<fit_intercept>$ \textbf{\textit{, float, optional field.}}.  whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). \textit{Default =  True};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\end{itemize}
%%%%%%%%
\subparagraph{Linear Model: Bayesian ridge regression}
\mbox{}
\\The \textit{Bayesian ridge regression}  estimates a probabilistic model of the regression problem as described above. The prior for the parameter w is given by a spherical Gaussian:
\begin{equation}
p(w|\lambda) =\mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})
\end{equation}
The priors over $\alpha$ and $\lambda$ are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian.
The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge. The parameters $w, \alpha$ and $\lambda$ are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over $\alpha$ and $\lambda$. These are usually chosen to be non-informative. The parameters are estimated by maximizing the marginal log likelihood.
In order to use the  \textit{Bayesian ridge regression} regressor, the user needs to set the sub-node $<SKLtype>BayesianRidge</SKLtype>$.  In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_iter>$ \textbf{\textit{, integer, optional field.}}.  Maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  Stop the algorithm if w has converged. \textit{Default = 1.e-3};
\item $<alpha_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<alpha_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<lambda_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<lambda_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<compute_score>$ \textbf{\textit{, boolean, optional field.}}.  If True, compute the objective function at each step of the model. \textit{Default =  False};
\item $<threshold_lambda>$ \textbf{\textit{, float, optional field.}}.  Threshold for removing (pruning) weights with high precision from the computation. \textit{Default =  1.e+4};
\item $<fit_intercept>$ \textbf{\textit{, float, optional field.}}.  whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). \textit{Default =  True};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\end{itemize}
%%%%%%%
\subparagraph{Linear Model: Elastic Net}
\mbox{}
\\The \textit{Elastic Net} is a linear regression with combined L1 and L2 priors as regularizer.
It minimizes the objective function:
\begin{equation}
1/(2*n_samples) *||y - Xw||^2_2+alpha*l1_ratio*||w||_1 + 0.5 *alpha*(1 - l1_ratio)*||w||^2_2
\end{equation}
In order to use the  \textit{Elastic Net} regressor, the user needs to set the sub-node $<SKLtype>ElasticNet</SKLtype>$.  In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<alpha>$ \textbf{\textit{, float, optional field.}}.  Constant that multiplies the penalty terms. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. \textit{Default = 1.0};
\item $<l1_ratio>$ \textbf{\textit{, float, optional field.}}.  The ElasticNet mixing parameter, with $0 <= l1_ratio <= 1$. For $l1_ratio = 0$ the penalty is an L2 penalty. For $l1_ratio = 1$ it is an L1 penalty. For $0 < l1_ratio < 1$, the penalty is a combination of L1 and L2. \textit{Default = 0.5};
\item $<fit_intercept>$ \textbf{\textit{, boolean, optional field.}}.  Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. \textit{Default =  True};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<max_iter>$ \textbf{\textit{, integer, optional field.}}.  The maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.. \textit{Default = 1.0e-4};
\item $<warm_start >$ \textbf{\textit{, boolean, optional field.}}.  When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.. \textit{Default =  False};
\item $<positive>$ \textbf{\textit{, float, optional field.}}.  When set to True, forces the coefficients to be positive. \textit{Default =  False}.
\end{itemize}
%%%%%%%%
\subparagraph{Linear Model: Elastic Net CV}
\mbox{}
\\The \textit{Elastic Net CV} is a linear regression similar to Elastic Net model but with an iterative fitting along a regularization path. The best model is selected by cross-validation.
\\In order to use the  \textit{Elastic Net CV} regressor, the user needs to set the sub-node $<SKLtype>ElasticNetCV</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<l1_ratio>$ \textbf{\textit{, float, optional field.}}. Float flag between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For $l1_ratio = 0$ the penalty is an L2 penalty. For $l1_ratio = 1$ it is an L1 penalty. For $0 < l1_ratio < 1$, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for $l1_ratio$ is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1, .5, .7, .9, .95, .99, 1]. \textit{Default = 0.5};
\item $<eps>$ \textbf{\textit{, float, optional field.}}.  Length of the path. eps=1e-3 means that $alpha_min / alpha_max = 1e-3$. \textit{Default = 0.001};
\item $<n_alphas>$ \textbf{\textit{, integer, optional field.}}.  Number of alphas along the regularization path, used for each $l1_ratio$. \textit{Default = 100};
\item $<precompute>$ \textbf{\textit{, boolean or string, optional field.}}.  Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Available are [True | False | ‘auto’ | array-like]. \textit{Default = 1.0};
\item $<max_iter>$ \textbf{\textit{, integer, optional field.}}.  The maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.. \textit{Default = 1.0e-4};
\item $<positive>$ \textbf{\textit{, float, optional field.}}.  When set to True, forces the coefficients to be positive. \textit{Default =  False}.
\end{itemize}
%%%%%%
\subparagraph{Linear Model: Least Angle Regression model}
\mbox{}
\\The \textit{Least Angle Regression model} (LARS)  is a regression algorithm for high-dimensional data. LARS algorithm provides a means of producing an estimate of which variables to include, as well as their coefficients, when a  response variable is determined by a linear combination of a subset of potential covariate.
\\In order to use the  \textit{Least Angle Regression model} regressor, the user needs to set the sub-node $<SKLtype>Lars</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_nonzero_coef>$ \textbf{\textit{, integer, optional field.}}. Target number of non-zero coefficients. \textit{Default = 500};
\item $<fit_intercept>$ \textbf{\textit{, boolean, optional field.}}.  Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. \textit{Default =  True};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\item $<precompute>$ \textbf{\textit{, boolean or string, optional field.}}.  Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Available are [True | False | ‘auto’ | array-like]. \textit{Default = 1.0};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<eps>$ \textbf{\textit{, float, optional field.}}.  The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. \textit{Default =2.22e-16};
\item $<fit_path>$ \textbf{\textit{, boolean, optional field.}}.  f True the full path is stored in the coef\_path\_ attribute. If you compute the solution for a large problem or many targets, setting fit\_path to False will lead to a speedup, especially with a small alpha. \textit{Default =  True}.
\end{itemize}
%%%%%%
\subparagraph{Linear Model: Cross-validated Least Angle Regression model}
\mbox{}
\\The \textit{Cross-validated Least Angle Regression model} is a regression algorithm for high-dimensional data. It is similar to LARS method, but the best model is selected by cross-validation.
\\In order to use the  \textit{Cross-validated Least Angle Regression model} regressor, the user needs to set the sub-node $<SKLtype>LarsCV</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<fit_intercept>$ \textbf{\textit{, boolean, optional field.}}.  Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. \textit{Default =  True};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<precompute>$ \textbf{\textit{, boolean or string, optional field.}}.  Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Available are [True | False | ‘auto’ | array-like]. \textit{Default = 1.0};
\item $<max\textunderscore~iter>$ \textbf{\textit{, integer, optional field.}}.  The maximum number of iterations. \textit{Default = 300};
\item $<max\textunderscore~n\textunderscore~alphas>$ \textbf{\textit{, integer, optional field.}}. The maximum number of points on the path used to compute the residuals in the cross-validation. \textit{Default = 1000};
\item $<eps>$ \textbf{\textit{, float, optional field.}}.  The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. \textit{Default =2.22e-16};
\end{itemize}
\subparagraph{Linear Model trained with L1 prior as regularizer (aka the Lasso)}
\mbox{}
\\The \textit{Linear Model trained with L1 prior as regularizer (Lasso)} is a shrinkage and selection method for linear regression. It minimizes the usual sum of squared errors, with a bound on the sum of the absolute values of the coefficients. 
\\In order to use the \textit{Linear Model trained with L1 prior as regularizer (Lasso)} regressor, the user needs to set the sub-node $<SKLtype>Lasso</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alpha>$ \textbf{\textit{, float, optional field.}}. Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 is with the Lasso object is not advised and you should prefer the LinearRegression object.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional, default False.}}. If True, the regressors X will be normalized before regression.
  \item $<precompute>$ \textbf{\textit{, True | False | ‘auto’ | array-like.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always True to preserve sparsity.
  \item $<max \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The maximum number of iterations
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.
  \item $<warm \textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
  \item $<positive>$ \textbf{\textit{, boolean, optional field.}}. When set to True, forces the coefficients to be positive.
\end{itemize}

\subparagraph{Lasso linear model with iterative fitting along a regularization path}
\mbox{}
\\The \textit{Lasso linear model with iterative fitting along a regularization path} is Lasso linear model with iterative fitting along a regularization path
\\In order to use the \textit{Lasso linear model with iterative fitting along a regularization path} regressor, the user needs to set the sub-node $<SKLtype>LassoCV</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. Length of the path. eps=1e-3 means that alpha \textunderscore min / alpha \textunderscore max = 1e-3.
  \item $<n \textunderscore alphas>$ \textbf{\textit{, int, optional field.}}. Number of alphas along the regularization path
  \item $<alphas>$ \textbf{\textit{, numpy array, optional field.}}. List of alphas where to compute the models. If None alphas are set automatically
  \item $<precompute>$ \textbf{\textit{, true | False | ‘auto’ | array-like.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.
  \item $<max \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The maximum number of iterations
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.
  \item $<verbose>$ \textbf{\textit{, boolean ot integer.}}. Amount of verbosity.
  \item $<positive>$ \textbf{\textit{, boolean, optional field.}}. If positive, restrict regression coefficients to be positive
\end{itemize}

\subparagraph{Lasso model fit with Least Angle Regression }
\mbox{}
\\The \textit{Lasso model fit with Least Angle Regression} is a Cross-validated Least Angle Regression model
\\In order to use the \textit{Cross-validated Least Angle Regression model} regressor, the user needs to set the sub-node $<SKLtype>LarsCV</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<verbose>$ \textbf{\textit{, boolean or integer, optional field.}}. Sets the verbosity amount
  \item $<normalize>$ \textbf{\textit{, boolean, optional, default False.}}. If True, the regressors X will be normalized before regression.
  \item $<precompute>$ \textbf{\textit{, True | False | ‘auto’ | array-like.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.
  \item $<max \textunderscore iter>$ \textbf{\textit{, integer, optional field.}}. Maximum number of iterations to perform.
  \item $<max \textunderscore n \textunderscore alphas>$ \textbf{\textit{, integer, optional field.}}. The maximum number of points on the path used to compute the residuals in the cross-validation
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.
\end{itemize}

\subparagraph{Cross-validated Lasso, using the LARS algorithm}
p\mbox{}
\\The \textit{Cross-validated Lasso, using the LARS algorithm} is a Cross-validated Lasso, using the LARS algorithm
\\In order to use the \textit{Cross-validated Lasso, using the LARS algorithm} regressor, the user needs to set the sub-node $<SKLtype>LassoLarsCV</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<verbose>$ \textbf{\textit{, boolean or integer, optional field.}}. Sets the verbosity amount
  \item $<normalize>$ \textbf{\textit{, boolean, optional, default False.}}. If True, the regressors X will be normalized before regression.
  \item $<precompute>$ \textbf{\textit{, True | False | ‘auto’ | array-like.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.
  \item $<max \textunderscore iter>$ \textbf{\textit{, integer, optional field.}}. Maximum number of iterations to perform.
  \item $<max \textunderscore n \textunderscore alphas>$ \textbf{\textit{, integer, optional field.}}. The maximum number of points on the path used to compute the residuals in the cross-validation
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.
\end{itemize}


\subparagraph{Lasso model fit with Lars using BIC or AIC for model selection}
\mbox{}
\\The \textit{Lasso model fit with Lars using BIC or AIC for model selection} is Lasso model fit with Lars using BIC or AIC for model selection
The optimization objective for Lasso is:
$(1 / (2 * n \textunderscore samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.
\\In order to use the \textit{Lasso model fit with Lars using BIC or AIC for model selection} regressor, the user needs to set the sub-node $<SKLtype>LassoLarsIC</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<criterion>$ \textbf{\textit{, ‘bic’ | ‘aic’ .}}. The type of criterion to use.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<verbose>$ \textbf{\textit{, boolean or integer, optional field.}}. Sets the verbosity amount
  \item $<normalize>$ \textbf{\textit{, boolean, optional, default False.}}. If True, the regressors X will be normalized before regression.
  \item $<precompute>$ \textbf{\textit{, True | False | ‘auto’ | array-like.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.
  \item $<max \textunderscore iter>$ \textbf{\textit{, integer, optional field.}}. Maximum number of iterations to perform. Can be used for early stopping.
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.
\end{itemize}

\subparagraph{Ordinary least squares Linear Regression}
\mbox{}
\\The \textit{Ordinary least squares Linear Regression} is a method for estimating the unknown parameters in a linear regression model, with the goal of minimizing the differences between the observed responses in some arbitrary dataset and the responses predicted by the linear approximation of the data
\\In order to use the \textit{Ordinary least squares Linear Regression} regressor, the user needs to set the sub-node $<SKLtype>LinearRegression</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean, optional field.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional field, default False.}}. If True, the regressors X will be normalized before regression.
\end{itemize}

\subparagraph{Logistic Regression }
\mbox{}
\\The \textit{Logistic Regression} implements L1 and L2 regularized logistic regression using the liblinear library. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).
\\In order to use the \textit{Logistic Regression} regressor, the user needs to set the sub-node $<SKLtype>LogisticRegression</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<penalty>$ \textbf{\textit{, string, ‘l1’ or ‘l2’.}}. Used to specify the norm used in the penalization.
  \item $<dual>$ \textbf{\textit{, boolean.}}. Dual or primal formulation. Dual formulation is only implemented for l2 penalty. Prefer dual=False when n \textunderscore samples > n \textunderscore features.
  \item $<C>$ \textbf{\textit{, float, optional field (default: 1.0).}}. Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean (default: True).}}. Specifies if a constant (a.k.a. bias or intercept) should be added the decision function.
  \item $<intercept \textunderscore scaling>$ \textbf{\textit{, float, optional field (default: 1.0).}}. when self.fit \textunderscore intercept is True, instance vector x becomes [x, self.intercept \textunderscore scaling], i.e. a “synthetic” feature with constant value equals to intercept \textunderscore scaling is appended to the instance vector. The intercept becomes intercept \textunderscore scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept \textunderscore scaling has to be increased
  \item $<class \textunderscore weight>$ \textbf{\textit{, {dict, ‘auto’}, optional.}}. Over-/undersamples the samples of each class according to the given weights. If not given, all classes are supposed to have weight one. The ‘auto’ mode selects weights inversely proportional to class frequencies in the training set.
  \item $<random \textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. Tolerance for stopping criteria.
\end{itemize}

\subparagraph{Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer }
\mbox{}
\\The \textit{Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer} is a regressor where the optimization objective for Lasso is:
$(1 / (2 * n \textunderscore samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21$
Where:
$||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of earch row.
\\In order to use the \textit{Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer} regressor, the user needs to set the sub-node $<SKLtype>MultiTaskLasso</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alpha>$ \textbf{\textit{, float, optional field.}}. Constant that multiplies the L1/L2 term. Defaults to 1.0
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normlaize>$ \textbf{\textit{, boolean, optional field (default: False).}}. If True, the regressors X will be normalized before regression.
  \item $<max \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The maximum number of iterations
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.
  \item $<warm \textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer}
\mbox{}
\\The \textit{Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer} is a regressor where the optimization objective for MultiTaskElasticNet is:
$(1 / (2 * n \textunderscore samples)) * ||Y - XW||^Fro_2
+ alpha * l1 \textunderscore ratio * ||W||_21
+ 0.5 * alpha * (1 - l1 \textunderscore ratio) * ||W||_Fro^2$
Where:
$||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
\\In order to use the \textit{Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer} regressor, the user needs to set the sub-node $<SKLtype>MultiTaskElasticNet</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alpha>$ \textbf{\textit{, float, optional field.}}. Constant that multiplies the L1/L2 term. Defaults to 1.0
  \item $<l1 \textunderscore ratio>$ \textbf{\textit{, float.}}. The ElasticNet mixing parameter, with $0 < l1 \textunderscore ratio <= 1$. For $l1 \textunderscore ratio = 0$ the penalty is an L1/L2 penalty. For $l1 \textunderscore ratio = 1$ it is an L1 penalty. For $0 < l1 \textunderscore ratio < 1$, the penalty is a combination of L1/L2 and L2.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default False).}}. If True, the regressors X will be normalized before regression.
  \item $<max \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The maximum number of iterations
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.
  \item $<warm \textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Orthogonal Mathching Pursuit model (OMP)}
\mbox{}
\\The \textit{Orthogonal Mathching Pursuit model (OMP)} is a type of sparse approximation which involves finding the "best matching" projections of multidimensional data onto an over-complete dictionary D. 
\\In order to use the \textit{Orthogonal Mathching Pursuit model (OMP)} regressor, the user needs to set the sub-node $<SKLtype>OrthogonalMatchingPursuit</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<n \textunderscore nonzero \textunderscore coefs>$ \textbf{\textit{, int, optional field.}}. Desired number of non-zero entries in the solution. If None (by default) this value is set to 10\% of n \textunderscore features.
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. Maximum norm of the residual. If not None, overrides n \textunderscore nonzero \textunderscore coefs.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean, optional field.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional field.}}. If False, the regressors X are assumed to be already normalized.
  \item $<precompute>$ \textbf{\textit{, {True, False, ‘auto’}, default ‘auto’.}}. Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when n \textunderscore targets or n \textunderscore samples is very large. Note that if you already have such matrices, you can pass them directly to the fit method.
\end{itemize}

\subparagraph{Cross-validated Orthogonal Mathching Pursuit model (OMP)}
\mbox{}
\\The \textit{Cross-validated Orthogonal Mathching Pursuit model (OMP)} is a regressor similar to OMP which has good performance in sparse recovery.
\\In order to use the \textit{Cross-validated Orthogonal Mathching Pursuit model (OMP)} regressor, the user needs to set the sub-node $<SKLtype>OrthogonalMatchingPursuitCV</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean, optional field.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional field.}}. If False, the regressors X are assumed to be already normalized.
  \item $<max \textunderscore iter>$ \textbf{\textit{, integer, optional field.}}. Maximum numbers of iterations to perform, therefore maximum features to include 10\% of n \textunderscore features but at least 5 if available.
  \item $<cv>$ \textbf{\textit{, cross-validation generator, optional.}}. see sklearn.cross \textunderscore validation. If None is passed, default to a 5-fold strategy
  \item $<verbose>$ \textbf{\textit{,  boolean or integer, optional.}}. Sets the verbosity amount
\end{itemize}

\subparagraph{Passive Aggressive Classifier}
\mbox{}
\\The \textit{Passive Aggressive Classifier} is a principled approach to linear classification that advocates minimal weight updates i.e., the least required so that the current training instance is correctly classified.
\\In order to use the \textit{Passive Aggressive Classifier} classifier, the user needs to set the sub-node $<SKLtype>PassiveAggressiveClassifier</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<C>$ \textbf{\textit{, float.}}. Maximum step size (regularization). Defaults to 1.0.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.
  \item $<n \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The number of passes over the training data (aka epochs). Defaults to 5.
  \item $<shuffle>$ \textbf{\textit{, boolean, optional field.}}. Whether or not the training data should be shuffled after each epoch. Defaults to False.
  \item $<random \textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
  \item $<verbose>$ \textbf{\textit{, integer or boolean, optional field.}}. The verbosity level
  \item $<loss>$ \textbf{\textit{, string, optional field.}}. The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared \textunderscore hinge: equivalent to PA-II in the reference paper.
  \item $<warm \textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Passive Aggressive Regressor}
\mbox{}
\\The \textit{Passive Aggressive Regressor} is similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C
\\In order to use the \textit{Passive Aggressive Regressor} regressor, the user needs to set the sub-node $<SKLtype>PassiveAggressiveRegressor</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<C>$ \textbf{\textit{, float.}}. Maximum step size (regularization). Defaults to 1.0.
  \item $<epsilon>$ \textbf{\textit{, float.}}. If the difference between the current prediction and the correct label is below this threshold, the model is not updated.
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.
  \item $<n \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The number of passes over the training data (aka epochs). Defaults to 5.
  \item $<shuffle>$ \textbf{\textit{, boolean, optional field.}}. Whether or not the training data should be shuffled after each epoch. Defaults to False.
  \item $<random \textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
  \item $<verbose>$ \textbf{\textit{, int, optional field.}}. The verbosity level
  \item $<loss>$ \textbf{\textit{, string, optional field.}}. The loss function to be used: epsilon \textunderscore insensitive: equivalent to PA-I in the reference paper. squared \textunderscore epsilon \textunderscore insensitive: equivalent to PA-II in the reference paper.
  \item $<warm \textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Perceptron}
\mbox{}
\\The \textit{Perceptron} is an algorithm for supervised classification of an input into one of several possible non-binary outputs. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.
\\In order to use the \textit{Perceptron} classifier, the user needs to set the sub-node $<SKLtype>Perceptron</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<penalty>$ \textbf{\textit{, None, ‘l2’ or ‘l1’ or ‘elasticnet’.}}. The penalty (aka regularization term) to be used. Defaults to None.
  \item $<alpha>$ \textbf{\textit{, float.}}. Constant that multiplies the regularization term if regularization is used. Defaults to 0.0001
  \item $<fit \textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.
  \item $<n \textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The number of passes over the training data (aka epochs). Defaults to 5.
  \item $<shuffle>$ \textbf{\textit{, boolean, optional field.}}. Whether or not the training data should be shuffled after each epoch. Defaults to False.
  \item $<random \textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
  \item $<verbose>$ \textbf{\textit{, int, optional field.}}. The verbosity level
  \item $<eta0>$ \textbf{\textit{, double, optional field.}}. Constant by which the updates are multiplied. Defaults to 1.
  \item $<class \textunderscore weight>$ \textbf{\textit{, dict, {class \textunderscore label}.}}. Preset for the class \textunderscore weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The “auto” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.
  \item $<warm \textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Randomized Lasso}
\mbox{}
\\The \textit{Randomized Lasso} works by resampling the train data and computing a Lasso on each resampling. In short, the features selected more often are good features. It is also known as stability selection. 
\\In order to use the \textit{Randomized Lasso} regressor, the user needs to set the sub-node $<SKLtype>RandomizedLasso</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alpha>$ \textbf{\textit{, float, ‘aic’, or ‘bic’, optional.}}. The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.
  \item $<scaling>$ \textbf{\textit{, float, optional field.}}. The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1. 
  \item $<sample\textunderscore fraction>$ \textbf{\textit{, float, optional field.}}. The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.
  \item $<n\textunderscore resampling>$ \textbf{\textit{, int, optional field.}}. Number of randomized models.
  \item $<selection\textunderscore threshold>$ \textbf{\textit{, float, optional field.}}. The score above which features should be selected.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean, optional field.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<verbose>$ \textbf{\textit{, boolean or integer, optional field.}}. Sets the verbosity amount
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default True).}}. If True, the regressors X will be normalized before regression.
  \item $<precompute>$ \textbf{\textit{, True | False | ‘auto’.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to ‘auto’ let us decide. The Gram matrix can also be passed as argument.
  \item $<max\textunderscore iter>$ \textbf{\textit{, int, optional field.}}. Maximum number of iterations to perform in the Lars algorithm.
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ‘tol’ parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.
  \item $<random\textunderscore state>$ \textbf{\textit{, int, RandomState instance or None, optional (default=None).}}. If int, random\textunderscore state is the seed used by the random number generator; If RandomState instance, random\textunderscore state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
\end{itemize}

\subparagraph{Randomized Logistic Regression}
\mbox{}
\\The \textit{Randomized Logistic Regression} works by resampling the train data and computing a LogisticRegression on each resampling. In short, the features selected more often are good features. It is also known as stability selection.
\\In order to use the \textit{Randomized Logistic Regression} regressor, the user needs to set the sub-node $<SKLtype>RandomizedLogisticRegression</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<C>$ \textbf{\textit{, float, optional field (default=1).}}. The regularization parameter C in the LogisticRegression.
  \item $<scaling>$ \textbf{\textit{, float, optional field (default=0.5).}}. The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.
  \item $<sample\textunderscore fraction>$ \textbf{\textit{, float, optional field (default=0.75).}}. The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.
  \item $<n\textunderscore resampling>$ \textbf{\textit{, int, optional field (default=200).}}. Number of randomized models.
  \item $<selection\textunderscore threshold>$ \textbf{\textit{, float, optional field (default=0.25).}}. The score above which features should be selected.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean, optional field.}}. whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<verbose>$ \textbf{\textit{, boolean or integer, optional field (default=True).}} Sets the verbosity amount
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default=True).}}. If True, the regressors X will be normalized before regression.
  \item $<tol>$ \textbf{\textit{, float, optional field (default=1e-3).}}. tolerance for stopping criteria of LogisticRegression
  \item $<random\textunderscore state>$ \textbf{\textit{, int, RandomState instance or None, optional (default=None).}}. If int, random\textunderscore state is the seed used by the random number generator; If RandomState instance, random\textunderscore state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
\end{itemize}

\subparagraph{Linear least squares with l2 regularization}
\mbox{}
\\The \textit{Linear least squares with l2 regularization} solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n\textunderscore samples, n\textunderscore targets]).
\\In order to use the \textit{Linear least squares with l2 regularization} regressor, the user needs to set the sub-node $<SKLtype>Ridge</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alpha>$ \textbf{\textit{, float, array-like.}}. shape = [n\textunderscore targets] Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates. Alpha corresponds to $(2*C)^-1$ in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<max\textunderscore iter>$ \textbf{\textit{, int, optional field.}}. Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default False).}}. If True, the regressors X will be normalized before regression.
  \item $<solver>$ \textbf{\textit{, {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse\textunderscore cg’}.}}. Solver to use in the computational routines:
				\begin{itemize}
				\item‘auto’ chooses the solver automatically based on the type of data.
				\item‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.
				\item‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.
				\item‘sparse\textunderscore cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max\textunderscore iter).
				\item‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fatest but may not be available in old scipy versions. It also uses an iterative procedure.
				\end{itemize}
				All three solvers support both dense and sparse data.
\end{itemize}

\subparagraph{Classifier using Ridge regression}
\mbox{}
\\The \textit{Classifier using Ridge regression} is a classifier based on Linear least squares with l2 regularization
\\In order to use the \textit{Classifier using Ridge regression} classifier, the user needs to set the sub-node $<SKLtype>RidgeClassifier</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alpha>$ \textbf{\textit{, float.}}. Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates. Alpha corresponds to $(2*C)^-1$ in other linear models such as LogisticRegression or LinearSVC.
  \item $<class\textunderscore weight>$ \textbf{\textit{, dic, optional field.}}. Weights associated with classes in the form {class\textunderscore label : weight}. If not given, all classes are supposed to have weight one.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<max\textunderscore iter>$ \textbf{\textit{, int, optional field.}}. Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default False).}}. If True, the regressors X will be normalized before regression.
  \item $<solver>$ \textbf{\textit{, {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse\textunderscore cg’}.}}. Solver to use in the computational routines. ‘svd’ will use a Singular value decomposition to obtain the solution, ‘cholesky’ will use the standard scipy.linalg.solve function, ‘sparse\textunderscore cg’ will use the conjugate gradient solver as found in scipy.sparse.linalg.cg while ‘auto’ will chose the most appropriate depending on the matrix X. ‘lsqr’ uses a direct regularized least-squares routine provided by scipy.
  \item $<tol>$ \textbf{\textit{, float.}}. Precision of the solution.
\end{itemize}

\subparagraph{Ridge classifier with built-in cross-validation}
\mbox{}
\\The \textit{Ridge classifier with built-in cross-validation} performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n\textunderscore features > n\textunderscore samples case is handled efficiently.
\\In order to use the \textit{Ridge classifier with built-in cross-validation} classifier, the user needs to set the sub-node $<SKLtype>RidgeClassifierCV</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alphas>$ \textbf{\textit{, numpy array of shape [n\textunderscore alphas].}}. Array of alpha values to try. Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates. Alpha corresponds to $(2*C)^-1$ in other linear models such as LogisticRegression or LinearSVC.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default False).}}. If True, the regressors X will be normalized before regression.
  \item $<scoring>$ \textbf{\textit{, string, callable or None, optional, default: None.}}. A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y).
  \item $<cv>$ \textbf{\textit{, cross-validation generator, optional.}}. If None, Generalized Cross-Validation (efficient Leave-One-Out) will be used.
  \item $<class\textunderscore weight>$ \textbf{\textit{, dic, optional field.}}. Weights associated with classes in the form {class\textunderscore label : weight}. If not given, all classes are supposed to have weight one.
\end{itemize}

\subparagraph{Ridge regression with built-in cross-validation}
\mbox{}
\\The \textit{Ridge regression with built-in cross-validation} performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.
\\In order to use the \textit{Ridge regression with built-in cross-validation} regressor, the user needs to set the sub-node $<SKLtype>RidgeCV</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<alphas>$ \textbf{\textit{, numpy array of shape [n\textunderscore alphas] .}}. Array of alpha values to try. Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates. Alpha corresponds to $(2*C)^-1$ in other linear models such as LogisticRegression or LinearSVC.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<normalize>$ \textbf{\textit{, boolean, optional field (default False).}}. If True, the regressors X will be normalized before regression.
  \item $<scoring>$ \textbf{\textit{, string, callable or None, optional, default: None.}}. A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y).
  \item $<cv>$ \textbf{\textit{, cross-validation generator, optional field.}}. If None, Generalized Cross-Validation (efficient Leave-One-Out) will be used.
  \item $<gcv\textunderscore mode>$ \textbf{\textit{, {None, ‘auto’, ‘svd’, eigen’}, optional field.}}. Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:
				\begin{itemize}
				  \item 'auto' : use svd if n\textunderscore samples > n\textunderscore features or when X is a sparse matrix, otherwise use eigen
				  \item 'svd' : force computation via singular value decomposition of $X$ (does not work for sparse matrices)
				  \item 'eigen' : force computation via eigendecomposition of $X^T X$
				 \end{itemize}
				The ‘auto’ mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data.
  \item $<store\textunderscore cv\textunderscore values>$ \textbf{\textit{, boolean (default=False).}}. Flag indicating if the cross-validation values corresponding to each alpha should be stored in the cv\textunderscore values\textunderscore  attribute (see below). This flag is only compatible with cv=None (i.e. using Generalized Cross-Validation).
\end{itemize}

\subparagraph{Linear classifiers (SVM, logistic regression, a.o.) with SGD training}
\mbox{}
\\The \textit{Linear classifiers (SVM, logistic regression, a.o.) with SGD training} implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial\textunderscore fit method.
This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).
The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.
\\In order to use the \textit{Linear classifiers (SVM, logistic regression, a.o.) with SGD training} WHAT, the user needs to set the sub-node $<SKLtype>SGDClassifier</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<loss>$ \textbf{\textit{, str, ‘hinge’, ‘log’, ‘modified\textunderscore huber’, ‘squared\textunderscore hinge’, ‘perceptron’, or a regression loss: ‘squared\textunderscore loss’, ‘huber’, ‘epsilon\textunderscore insensitive’, or ‘squared\textunderscore epsilon\textunderscore insensitive’.}}. The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM. The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified\textunderscore huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared\textunderscore hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.
  \item $<penalty>$ \textbf{\textit{, str, ‘l2’ or ‘l1’ or ‘elasticnet’.}}. The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.
  \item $<alpha>$ \textbf{\textit{, float.}}. Constant that multiplies the regularization term. Defaults to 0.0001
  \item $<l1\textunderscore ratio>$ \textbf{\textit{, float.}}. The Elastic Net mixing parameter, with 0 <= l1\textunderscore ratio <= 1. l1\textunderscore ratio=0 corresponds to L2 penalty, l1\textunderscore ratio=1 to L1. Defaults to 0.15.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.
  \item $<n\textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The number of passes over the training data (aka epochs). Defaults to 5.
  \item $<shuffle>$ \textbf{\textit{, boolean, optional field.}}. Whether or not the training data should be shuffled after each epoch. Defaults to False.
  \item $<random\textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
  \item $<verbose>$ \textbf{\textit{, int, optional field.}}. The verbosity level
  \item $<epsilon>$ \textbf{\textit{, int, optional field.}}. The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means ‘all CPUs’. Defaults to 1.
  \item $<learning\textunderscore rate>$ \textbf{\textit{, string, optional field.}}. The learning rate: constant: eta = eta0 optimal: eta = 1.0 / (t + t0) [default] invscaling: eta = eta0 / pow(t, power\textunderscore t)
  \item $<eta0>$ \textbf{\textit{, double.}}. The initial learning rate for the ‘constant’ or ‘invscaling’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.
  \item $<power\textunderscore t>$ \textbf{\textit{, double.}}. The exponent for inverse scaling learning rate [default 0.5].
  \item $<class\textunderscore weight>$ \textbf{\textit{, dict, {class\textunderscore label}.}}. Preset for the class\textunderscore weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The “auto” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.
  \item $<warm\textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Linear model fitted by minimizing a regularized empirical loss with SGD}
\mbox{}
\\The \textit{Linear model fitted by minimizing a regularized empirical loss
with SGD} is a model where SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).
The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.
This implementation works with data represented as dense numpy arrays of floating point values for the features.
 \\In order to use the \textit{Linear model fitted by
minimizing a regularized empirical loss with SGD} model, the user needs to set the sub-node
$<SKLtype>FW\textunderscore NAME</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<loss>$ \textbf{\textit{,str, ‘squared\textunderscore loss’, ‘huber’, ‘epsilon\textunderscore insensitive’, or ‘squared\textunderscore epsilon\textunderscore insensitive’.}}. The loss function to be used. Defaults to ‘squared\textunderscore loss’ which refers to the ordinary least squares fit. ‘huber’ modifies ‘squared\textunderscore loss’ to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. ‘epsilon\textunderscore insensitive’ ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. ‘squared\textunderscore epsilon\textunderscore insensitive’ is the same but becomes squared loss past a tolerance of epsilon.
  \item $<penalty>$ \textbf{\textit{, str, ‘l2’ or ‘l1’ or ‘elasticnet’.}}. The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.
  \item $<alpha>$ \textbf{\textit{, float.}}. Constant that multiplies the regularization term. Defaults to 0.0001
  \item $<l1\textunderscore ratio>$ \textbf{\textit{, float.}}. The Elastic Net mixing parameter, with 0 <= l1\textunderscore ratio <= 1. l1\textunderscore ratio=0 corresponds to L2 penalty, l1\textunderscore ratio=1 to L1. Defaults to 0.15.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean.}}. Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.
  \item $<n\textunderscore iter>$ \textbf{\textit{, int, optional field.}}. The number of passes over the training data (aka epochs). Defaults to 5.
  \item $<shuffle>$ \textbf{\textit{, boolean, optional field.}}. Whether or not the training data should be shuffled after each epoch. Defaults to False.
  \item $<random\textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
  \item $<verbose>$ \textbf{\textit{, int, optional field.}}. The verbosity level.
  \item $<epsilon>$ \textbf{\textit{, float.}}. Epsilon in the epsilon-insensitive loss functions; only if loss is ‘huber’, ‘epsilon\textunderscore insensitive’, or ‘squared\textunderscore epsilon\textunderscore insensitive’. For ‘huber’, determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold.
  \item $<learning\textunderscore rate>$ \textbf{\textit{, string, optional field.}}.Learning rate: constant: eta = eta0 optimal: eta = 1.0/(t+t0) invscaling: eta= eta0 / pow(t, power\textunderscore t) [default]
  \item $<eta0>$ \textbf{\textit{, double.}}. The initial learning rate [default 0.01].
  \item $<power\textunderscore t>$ \textbf{\textit{, double, optional field.}}. The exponent for inverse scaling learning rate [default 0.25].
  \item $<warm\textunderscore start>$ \textbf{\textit{, boolean, optional field.}}. When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
\end{itemize}

\subparagraph{Compute Least Angle Regression or Lasso path using LARS algorithm}
\mbox{}
\\The \textit{Compute Least Angle Regression or Lasso path using LARS algorithm} is a regressor where the optimization objective for the case method=’lasso’ is:
$(1 / (2 * n\textunderscore samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
in the case of method=’lars’, the objective function is only known in the form of an implicit equation
\\In order to use the \textit{Compute Least Angle Regression or Lasso path using LARS algorithm} regressor, the user needs to set the sub-node $<SKLtype>lars\textunderscore path</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<X>$ \textbf{\textit{, array, shape: (n\textunderscore samples, n\textunderscore features).}}. Input data.
  \item $<y>$ \textbf{\textit{, array, shape: (n\textunderscore samples).}}. Input targets.
  \item $<max\textunderscore iter>$ \textbf{\textit{, integer, optional (default=500).}}. Maximum number of iterations to perform, set to infinity for no limit.
  \item $<gram>$ \textbf{\textit{, None, ‘auto’, array, shape: (n\textunderscore features, n\textunderscore features), optional.}}. Precomputed Gram matrix (X’ * X), if 'auto', the Gram matrix is precomputed from the given X, if there are more samples than features.
  \item $<alpha\textunderscore min>$ \textbf{\textit{, float, optional field (default=0).}}. Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.
  \item $<method>$ \textbf{\textit{, {‘lar’, ‘lasso’}, optional (default=’lar’).}}. Specifies the returned model. Select 'lar' for Least Angle Regression, 'lasso' for the Lasso.
  \item $<eps>$ \textbf{\textit{, float, optional (default=``np.finfo(np.float).eps``).}}. The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.
  \item $<verbose>$ \textbf{\textit{, int (default=0).}}. Controls output verbosity.
\end{itemize}

\subparagraph{Compute Lasso path with coordinate descent}
\mbox{}
\\The \textit{Compute Lasso path with coordinate descent} is a regressor where the Lasso optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
$(1 / (2 * n\textunderscore samples)) * ||y - Xw||^2_2 + alpha * ||w||_1$
For multi-output tasks it is:
$(1 / (2 * n\textunderscore samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21$
Where:
$||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}$
i.e. the sum of norm of each row.
\\In order to use the \textit{Compute Lasso path with coordinate descent} regressor, the user needs to set the sub-node $<SKLtype>lasso\textunderscore path</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<X>$ \textbf{\textit{, {array-like, sparse matrix}, shape (n\textunderscore samples, n\textunderscore features).}}. Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output then X can be sparse.
  \item $<y>$ \textbf{\textit{, ndarray, shape = (n\textunderscore samples,), or (n\textunderscore samples, n\textunderscore outputs).}}. Target values
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. Length of the path. eps=1e-3 means that alpha\textunderscore min / alpha\textunderscore max = 1e-3
  \item $<n\textunderscore alphas>$ \textbf{\textit{, int, optional field.}}. Number of alphas along the regularization path
  \item $<alphas>$ \textbf{\textit{, ndarray, optional field.}}. List of alphas where to compute the models. If None alphas are set automatically
  \item $<precompute>$ \textbf{\textit{, True | False | ‘auto’ | array-like.}}. Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.
  \item $<Xy>$ \textbf{\textit{, array-like, optional field.}}. Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.
  \item $<copy\textunderscore X>$ \textbf{\textit{, boolean, optional field (default True).}}. If True, X will be copied; else, it may be overwritten.
  \item $<coef\textunderscore init>$ \textbf{\textit{, array, shape (n\textunderscore features, ) | None.}}. The initial values of the coefficients.
  \item $<verbose>$ \textbf{\textit{, bool or integer.}}. Amount of verbosity.
\end{itemize}

\subparagraph{Stability path based on randomized Lasso estimates}
\mbox{}
\\In order to use the \textit{Stability path based on randomized Lasso estimates} regressor, the user needs to set the sub-node $<SKLtype>lasso\textunderscore stability\textunderscore path</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<X>$ \textbf{\textit{, array-like, shape = [n\textunderscore samples, n\textunderscore features].}}. training data.
  \item $<y>$ \textbf{\textit{, array-like, shape = [n\textunderscore samples].}}. target values.
  \item $<scaling>$ \textbf{\textit{, float, optional, default=0.5.}}. The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.
  \item $<random\textunderscore state>$ \textbf{\textit{, integer or numpy.random.RandomState, optional.}}. The generator used to randomize the design.
  \item $<n\textunderscore resampling>$ \textbf{\textit{, int, optional, default=200.}}. Number of randomized models.
  \item $<n\textunderscore grid>$ \textbf{\textit{, int, optional, default=100.}}. Number of grid points. The path is linearly reinterpolated on a grid between 0 and 1 before computing the scores.
  \item $<sample\textunderscore fraction>$ \textbf{\textit{, float, optional, default=0.75.}}. The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.
  \item $<eps>$ \textbf{\textit{, float, optional field.}}. Smallest value of alpha / alpha\textunderscore max considered
  \item $<verbose>$ \textbf{\textit{, boolean or integer, optional field.}}. Sets the verbosity amount
\end{itemize}

\subparagraph{Gram Orthogonal Matching Pursuit (OMP)}
\mbox{}
\\The \textit{NAME} Solves n\textunderscore targets Orthogonal Matching Pursuit problems using only the Gram matrix X.T * X and the product X.T * y.
\\In order to use the \textit{NAME} regressor, the user needs to set the sub-node $<SKLtype>orthogonal\textunderscore mp\textunderscore gram</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<Gram>$ \textbf{\textit{, array, shape (n\textunderscore features, n\textunderscore features).}}. Gram matrix of the input data: X.T * X
  \item $<Xy>$ \textbf{\textit{, array, shape (n\textunderscore features,) or (n\textunderscore features, n\textunderscore targets) .}}. Input targets multiplied by X: X.T * y
  \item $<n\textunderscore nonzero\textunderscore coefs>$ \textbf{\textit{, int.}}. Desired number of non-zero entries in the solution. If None (by default) this value is set to 10\% of n\textunderscore features.
  \item $<tol>$ \textbf{\textit{, float.}}. Maximum norm of the residual. If not None, overrides n\textunderscore nonzero\textunderscore coefs.
  \item $<norms\textunderscore squared>$ \textbf{\textit{, array-like, shape (n\textunderscore targets).}}. DESCRIPTION
\end{itemize}

%%%%% ROM Model - SciKitLearn: Support Vector Machines %%%%%%%
\paragraph{Support Vector Machines.}
\label{SVM}
In machine learning, \textbf{Support Vector Machines} (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
In the following, all the SVM models available in RAVEN are reported.

\subparagraph{Linear Support Vector Classifier}
\mbox{}
\\The \textit{Linear Support Vector Classifier} is similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better (to large numbers of samples).
This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.
\\In order to use the \textit{Linear Support Vector Classifier} classifier, the user needs to set the sub-node $<SKLtype>LinearSVC</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<C>$ \textbf{\textit{, boolean, optional field (default=1.0).}}. Penalty parameter C of the error term.
  \item $<loss>$ \textbf{\textit{, string, ‘l1’ or ‘l2’ (default=’l2’).}}. Specifies the loss function. ‘l1’ is the hinge loss (standard SVM) while ‘l2’ is the squared hinge loss.
  \item $<penalty>$ \textbf{\textit{, string, ‘l1’ or ‘l2’ (default=’l2’).}}. Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef\textunderscore  vectors that are sparse.
  \item $<dual>$ \textbf{\textit{, boolean (default=True).}}. Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n\textunderscore samples > n\textunderscore features.
  \item $<tol>$ \textbf{\textit{, float, optional field.}}. Tolerance for stopping criteria
  \item $<multi\textunderscore class>$ \textbf{\textit{, string, ‘ovr’ or ‘crammer\textunderscore singer’ (default=’ovr’).}}. Determines the multi-class strategy if y contains more than two classes. ovr trains n\textunderscore classes one-vs-rest classifiers, while crammer\textunderscore singer optimizes a joint objective over all classes. While crammer\textunderscore singer is interesting from an theoretical perspective as it is consistent it is seldom used in practice and rarely leads to better accuracy and is more expensive to compute. If crammer\textunderscore singer is chosen, the options loss, penalty and dual will be ignored.
  \item $<fit\textunderscore intercept>$ \textbf{\textit{, boolean, optional field (default=True).}}. Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
  \item $<intercept\textunderscore scaling>$ \textbf{\textit{, float, optional field (default=1).}}. when self.fit\textunderscore intercept is True, instance vector x becomes [x, self.intercept\textunderscore scaling], i.e. a “synthetic” feature with constant value equals to intercept\textunderscore scaling is appended to the instance vector. The intercept becomes intercept\textunderscore scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept\textunderscore scaling has to be increased
  \item $<class\textunderscore weight>$ \textbf{\textit{, {dict, ‘auto’}, optional field.}}. Set the parameter C of class i to class\textunderscore weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ‘auto’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.
  \item $<verbose>$ \textbf{\textit{, int, optional field (default=0).}}. Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context.
  \item $<random\textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data.
\end{itemize}

\subparagraph{C-Support Vector Classification}
\mbox{}
\\The \textit{C-Support Vector Classification} is a based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.
The multiclass support is handled according to a one-vs-one scheme.
\\In order to use the \textit{C-Support Vector Classification} classifier, the user needs to set the sub-node $<SKLtype>SVC</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<C>$ \textbf{\textit{, float, optional field (default=1.0).}}. Penalty parameter C of the error term.
  \item $<kernel>$ \textbf{\textit{, string, optional (default=’rbf’).}}. Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.
  \item $<degree>$ \textbf{\textit{, int, optional field (default=3).}}. Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
  \item $<gamma>$ \textbf{\textit{, float, optional field (default=0.0).}}. Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is 0.0 then 1/n\textunderscore features will be used instead.
  \item $<coef0>$ \textbf{\textit{, float, optional field (default=0.0).}}. Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.
  \item $<probability>$ \textbf{\textit{, boolean, optional field (default=False).}}. Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method.
  \item $<shrinking>$ \textbf{\textit{, boolean, optional field (default=True).}}. Whether to use the shrinking heuristic.
  \item $<tol>$ \textbf{\textit{, float, optional field (default=1e-3).}}. Tolerance for stopping criterion.
  \item $<cache\textunderscore size>$ \textbf{\textit{, float, optional field.}}. Specify the size of the kernel cache (in MB)
  \item $<class\textunderscore weight>$ \textbf{\textit{, {dict, ‘auto’}, optional.}}. Set the parameter C of class i to class\textunderscore weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ‘auto’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.
  \item $<verbose>$ \textbf{\textit{, boolean (default: False).}}. Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.
  \item $<max\textunderscore iter>$ \textbf{\textit{, int, optional field (default=-1).}}. Hard limit on iterations within solver, or -1 for no limit.
  \item $<random\textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data for probability estimation.
\end{itemize}

\subparagraph{Nu-Support Vector Classification}
\mbox{}
\\The \textit{Nu-Support Vector Classification} is similar to SVC but uses a parameter to control the number of support vectors.
The implementation is based on libsvm.
\\In order to use the \textit{Nu-Support Vector Classification} classifier, the user needs to set the sub-node $<SKLtype>NuSVC</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<nu>$ \textbf{\textit{, float, optional field (default=0.5).}}. An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].
  \item $<kernel>$ \textbf{\textit{, string, optional field (default=’rbf’).}}. Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.
  \item $<degree>$ \textbf{\textit{, int, optional field (default=3).}}. degree of kernel function is significant only in poly, rbf, sigmoid
  \item $<gamma>$ \textbf{\textit{, float, optional field (default=0.0).}}. kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n\textunderscore features will be taken.
  \item $<coef0>$ \textbf{\textit{, float, optional field (default=0.0).}}. independent term in kernel function. It is only significant in poly/sigmoid.
  \item $<probability>$ \textbf{\textit{, boolean, optional field (default=False).}}. Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method.
  \item $<shrinking>$ \textbf{\textit{, boolean, optional field (default=True).}}. Whether to use the shrinking heuristic.
  \item $<tol>$ \textbf{\textit{, float, optional field (default=1e-3).}}. Tolerance for stopping criterion.
  \item $<cache\textunderscore size>$ \textbf{\textit{, float, optional field.}}. Specify the size of the kernel cache (in MB)
  \item $<verbose>$ \textbf{\textit{, boolean, optional field (default=False).}}. Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.
  \item $<max\textunderscore iter>$ \textbf{\textit{, int, optional field (default=-1).}}. Hard limit on iterations within solver, or -1 for no limit.
  \item $<random\textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data for probability estimation.
\end{itemize}

\subparagraph{Support Vector Regression}
\mbox{}
\\The \textit{Support Vector Regression} is an epsilon-Support Vector Regression.
The free parameters in the model are C and epsilon.
The implementations is a based on libsvm.
\\In order to use the \textit{Support Vector Regression} regressor, the user needs to set the sub-node $<SKLtype>SVR</SKLtype>$.
In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
  \item $<C>$ \textbf{\textit{, float, optional field (default=1.0).}}. penalty parameter C of the error term.
  \item $<epsilon>$ \textbf{\textit{, float, optional field (default=0.1).}}. epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.
  \item $<kernel>$ \textbf{\textit{, string, optional field (default=’rbf’).}}. Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.
  \item $<degree>$ \textbf{\textit{, float, optional field (default=3.0).}}. degree of kernel function is significant only in poly, rbf, sigmoid
  \item $<gamma>$ \textbf{\textit{, float, optional field (default=0.0).}}. kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n\textunderscore features will be taken.
  \item $<coef0>$ \textbf{\textit{, float, optional field (default=0.0).}}. independent term in kernel function. It is only significant in poly/sigmoid.
  \item $<probability>$ \textbf{\textit{, boolean, optional field (default=False).}}. Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method.
  \item $<shrinking>$ \textbf{\textit{, boolean, optional field (default=True).}}. Whether to use the shrinking heuristic.
  \item $<tol>$ \textbf{\textit{, float, optional field (default=1e-3).}}. Tolerance for stopping criterion.
  \item $<cache\textunderscore size>$ \textbf{\textit{, float, optional field.}}. Specify the size of the kernel cache (in MB)
  \item $<verbose>$ \textbf{\textit{, boolean (default: False).}}. Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.
  \item $<max\textunderscore iter>$ \textbf{\textit{, int, optional field (default=-1).}}. Hard limit on iterations within solver, or -1 for no limit.
  \item $<random\textunderscore state>$ \textbf{\textit{, int seed, RandomState instance, or None (default).}}. The seed of the pseudo random number generator to use when shuffling the data for probability estimaton.
\end{itemize}
 %%%%% ROM Model - SciKitLearn: MultiClass %%%%%%%
\paragraph{Multi Class.}
\label{Multiclass}
Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.
In the following, all the multi-class models available in RAVEN are reported.
%%%%%%%%%
\subparagraph{One-vs-the-rest (OvR) multiclass/multilabel strategy}
\mbox{}
\\The \textit{One-vs-the-rest (OvR) multiclass/multilabel strategy}, also known as one-vs-all, consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n\_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.
\\In order to use the  \textit{One-vs-the-rest (OvR) multiclass/multilabel} classifier, the user needs to set the sub-node $<SKLtype>multiClass~\vert~OneVsRestClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<estimator>$ \textbf{\textit{, boolean, required field.}}.  An estimator object implementing fit and one of decision\_function or predict\_proba. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{estimatorType}, \textit{required string attribute}, this attribute an other reduced order mode type that needs to be used for the construction of the multi-class algorithms; each sub-sequential node depends on the chosen ROM;
\end{itemize}
\end{itemize}
%%%%%%%%%%%%
\subparagraph{One-vs-one multiclass strategy}
\mbox{}
\\The \textit{One-vs-one multiclass strategy} consists in fitting one classifier per class pair. 
At prediction time, the class which received the most votes is selected. Since it requires to fit n\_classes * (n\_classes - 1) / 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n\_classes\^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which do not scale well with n\_samples. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n\_classes times.
\\In order to use the   \textit{One-vs-one multiclass} classifier, the user needs to set the sub-node $<SKLtype>multiClass~\vert~OneVsOneClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<estimator>$ \textbf{\textit{, boolean, required field.}}.  An estimator object implementing fit and one of decision\_function or predict\_proba. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{estimatorType}, \textit{required string attribute}, this attribute an other reduced order mode type that needs to be used for the construction of the multi-class algorithms; each sub-sequential node depends on the chosen ROM;
\end{itemize}
\end{itemize}
%%%%%%%%%%%%%
\subparagraph{Error-Correcting Output-Code multiclass strategy}
\mbox{}
\\The \textit{Error-Correcting Output-Code multiclass strategy} consists in representing each class with a binary code (an array of 0s and 1s). At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. The main advantage of these strategies is that the number of classifiers used can be controlled by the user, either for compressing the model ($0 < code_size < 1$) or for making the model more robust to errors ($code_size > 1$).
\\In order to use the   \textit{Error-Correcting Output-Code multiclass} classifier, the user needs to set the sub-node $<SKLtype>multiClass~\vert~OutputCodeClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<estimator>$ \textbf{\textit{, xml node, required field.}}.  An estimator object implementing fit and one of decision\_function or predict\_proba.  This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{estimatorType}, \textit{required string attribute}, this attribute an other reduced order mode type that needs to be used for the construction of the multi-class algorithms; each sub-sequential node depends on the chosen ROM;
\end{itemize}
\item $<code_size>$ \textbf{\textit{, float, required field.}}.  ercentage of the number of classes to be used to create the code book. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. A number greater than 1 will require more classifiers than one-vs-the-rest.
\end{itemize}
%%%%%%%%%%%%%
%\subparagraph{fit a one-vs-the-rest strategy}
%pass
%\subparagraph{Make predictions using the one-vs-the-rest strategy}
%pass
%\subparagraph{ Fit a one-vs-one strategy}
%pass
%\subparagraph{Make predictions using the one-vs-one strategy}
%pass
%\subparagraph{Fit an error-correcting output-code strategy}
%pass
%\subparagraph{Make predictions using the error-correcting output-code strategy}
%pass



 %%%%% ROM Model - SciKitLearn: naiveBayes %%%%%%%
\paragraph{Naive Bayes.}
\label{naiveBayes}
Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features. Given a class variable y and a dependent feature vector x\_1 through x\_n, Bayes’ theorem states the following relationship:
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}
                                 {P(x_1, \dots, x_n)}
\end{equation}
Using the naive independence assumption that
\begin{equation}
P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),
\end{equation}
for all i, this relationship is simplified to
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
                                 {P(x_1, \dots, x_n)}
\end{equation}
Since $P(x_1, \dots, x_n)$ is constant given the input, we can use the following classification rule:
\begin{equation}
P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)
\Downarrow
\end{equation}
\begin{equation}
\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),
\end{equation}
and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \mid y)$; the former is then the relative frequency of class $y$ in the training set.
The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \mid y)$.
In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)
Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.
On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict\_proba are not to be taken too seriously.
In the following, all the Naive Bayes available in RAVEN are reported.
%%%%%%%
\subparagraph{Gaussian Naive Bayes}
\mbox{}
\\The \textit{Gaussian Naive Bayes strategy} implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:
\begin{equation}
P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}
The parameters $\sigma_y$ and $\mu_y$ are estimated using maximum likelihood.
\\In order to use the   \textit{Gaussian Naive Bayes strategy}, the user needs to set the sub-node $<SKLtype>naiveBayes~\vert~GaussianNB</SKLtype>$. No additional XML nodes are needed to be inputted.
%%%%%%%%%%%%
\subparagraph{Multinomial Naive Bayes}
\mbox{}
\\The \textit{Multinomial Naive Bayes} implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\theta_y = (\theta_{y1},\ldots,\theta_{yn})$ for each class $y$, where n is the number of features (in text classification, the size of the vocabulary) and $\theta_{yi}$ is the probability $P(x_i \mid y)$ of feature i appearing in a sample belonging to class y.
The parameters $\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:
\begin{equation}
\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}
\end{equation}
where $N_{yi} = \sum_{x \in T} x_i$ is the number of times feature i appears in a sample of class y in the training set T, and $N_{y} = \sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class y.
The smoothing priors $\alpha \ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\alpha = 1$ is called Laplace smoothing, while $\alpha < 1$ is called Lidstone smoothing.
\\In order to use the   \textit{Multinomial Naive Bayes} strategy, the user needs to set the sub-node $<SKLtype>naiveBayes~\vert~MultinomialNB</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<alpha>$ \textbf{\textit{, float, optional field.}}.  Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). \textit{Default = 1.0};
\item $<fit\textunderscore~prior>$ \textbf{\textit{, boolean, required field.}}.  Whether to learn class prior probabilities or not. If false, a uniform prior will be used. \textit{Default = False};
\item $<class\textunderscore~prior>$ \textbf{\textit{, array-like float (n\_classes), optional field.}}.  Prior probabilities of the classes. If specified the priors are not adjusted according to the data. \textit{Default = None}.
\end{itemize}

%%%%%%%%%%%% 
\subparagraph{Bernoulli Naive Bayes}
\mbox{}
\\The \textit{Bernoulli Naive Bayes} implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a  \textit{Bernoulli Naive Bayes} instance may binarize its input (depending on the binarize parameter).
The decision rule for Bernoulli naive Bayes is based on
\begin{equation}
P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)
\end{equation}
which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature i that is an indicator for class y, where the multinomial variant would simply ignore a non-occurring feature.
In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier.  \textit{Bernoulli Naive Bayes} might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.
\\In order to use the   \textit{Bernoulli Naive Bayes} strategy, the user needs to set the sub-node $<SKLtype>naiveBayes~\vert~BernoulliNB</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<alpha>$ \textbf{\textit{, float, optional field.}}.  Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). \textit{Default = 1.0};
\item $<binarize>$ \textbf{\textit{, float, optional field.}}.  Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.. \textit{Default = None};
\item $<fit_prior>$ \textbf{\textit{, boolean, optional field.}}.  Whether to learn class prior probabilities or not. If false, a uniform prior will be used. \textit{Default = False};
\item $<class_prior>$ \textbf{\textit{, array-like float (n\_classes), optional field.}}.  Prior probabilities of the classes. If specified the priors are not adjusted according to the data. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Neighbors %%%%%%%
\paragraph{Neighbors.}
\label{Neighbors}
The \textit{Neighbors} class provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: classification for data with discrete labels, and regression for data with continuous labels.
The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree.).
\\In the following, all the Neighbors' models available in RAVEN are reported.
%%%%%%%%%%%%%%%
\subparagraph{Nearest Neighbors}
\mbox{}
\\The \textit{Nearest Neighbors} implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm. 
%The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute']. When the default value 'auto' is passed, the algorithm attempts to determine the best approach from the training data. 
\\In order to use the   \textit{Nearest Neighbors} strategy, the user needs to set the sub-node $<SKLtype>neighbors~\vert~NearestNeighbors</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\textunderscore~tree} will use BallTree;
\item \textit{kd\textunderscore~tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Classifier }
\mbox{}
\\The \textit{K Neighbors Classifier} is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. It implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.
\\In order to use the   \textit{K Neighbors Classifier}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~KNeighborsClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};

\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Classifier}
\mbox{}
\\The \textit{Radius Neighbors Classifier} is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. It implements learning based on the number of neighbors within a fixed radius r of each training point, where r is a floating-point value specified by the user.
\\In order to use the   \textit{Radius Neighbors Classifier}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~RadiusNeighbors</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};
\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2};
\item $<outlier\textunderscore~label>$ \textbf{\textit{, integer, optional field.}}.  Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Regressor}
\mbox{}
\\The \textit{K Neighbors Regressor}  can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based the mean of the labels of its nearest neighbors. It implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.
\\In order to use the   \textit{K Neighbors Regressor}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~KNeighborsRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};

\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Regressor}
\mbox{}
\\The \textit{Radius Neighbors Regressor}  can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based the mean of the labels of its nearest neighbors. It implements learning based on the neighbors within a fixed radius r of the query point, where r is a floating-point value specified by the user.
\\In order to use the   \textit{Radius Neighbors Regressor}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~RadiusNeighborsRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};
\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2};
\item $<outlier\textunderscore~label>$ \textbf{\textit{, integer, optional field.}}.  Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Nearest Centroid Classifier}
\mbox{}
\\The \textit{Nearest Centroid classifier} is a simple algorithm that represents each class by the centroid of its members. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed.
\\In order to use the   \textit{Nearest Centroid Classifier}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~NearestCentroid</SKLtype>$. In addition to this XML node, another might be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, float, optional field.}}.  Threshold for shrinking centroids to remove features. \textit{Default = None}.
\end{itemize}
%\subparagraph{Ball Tree}
%pass
%\subparagraph{K-D Tree}
%pass

 %%%%% ROM Model - SciKitLearn: Quadratic Discriminant Analysis %%%%%%%
\paragraph{Quadratic Discriminant Analysis.}
\label{QDA}
The \textit{Quadratic Discriminant Analysis} is a classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.The model fits a Gaussian density to each class.
\\In order to use the   \textit{Quadratic Discriminant Analysis Classifier}, the user needs to set the sub-node $<SKLtype>qda~\vert~QDA</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<priors>$ \textbf{\textit{, array-like (n\_classes), optional field.}}.  Priors on classes. \textit{Default = None};
\item $<reg\textunderscore~param>$ \textbf{\textit{, float, optional field.}}. Regularizes the covariance estimate as (1-reg\_param)*Sigma + reg\_param*Identity(n\_features). \textit{Default = 0.0}.
\end{itemize}

 %%%%% ROM Model - SciKitLearn: Tree %%%%%%%
\paragraph{Tree.}
\label{tree}
Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
\begin{itemize}
\item Some advantages of decision trees are:
\item  Simple to understand and to interpret. Trees can be visualized.
\item Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.
\item The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
\item  Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. 
\item Able to handle multi-output problems.
\item  Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.
\item Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
\item Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.
\end{itemize}
The disadvantages of decision trees include:
\begin{itemize}
\item Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.
\item Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.
\item The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.
\item There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.
\item Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.
\end{itemize}
In the following, all the linear models available in RAVEN are reported.
%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Classifier}
\mbox{}
\\The \textit{Decision Tree Classifier} is a classifier that is based on the decision tree logic.
\\In order to use the \textit{Decision Tree Classifier}, the user needs to set the sub-node $<SKLtype>tree~\vert~DecisionTreeClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Regressor}
\mbox{}
\\The \textit{Decision Tree Regressor} is a Regressor that is based on the decision tree logic.
\\In order to use the \textit{Decision Tree Regressor}, the user needs to set the sub-node $<SKLtype>tree~\vert~DecisionTreeRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%
\subparagraph{Extra Tree Classifier}
\mbox{}
\\The \textit{Extra Tree Classifier} is an extremely randomized tree classifier.
Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max\_features randomly selected features and the best split among those is chosen. When max\_features is set 1, this amounts to building a totally random decision tree.
\\In order to use the \textit{Extra Tree Classifier}, the user needs to set the sub-node $<SKLtype>tree~\vert~ExtraTreeClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:

\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%
\subparagraph{Extra Tree Regressor}
\mbox{}
\\The \textit{Extra Tree Regressor} is an extremely randomized tree regressor.
Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max\_features randomly selected features and the best split among those is chosen. When max\_features is set 1, this amounts to building a totally random decision tree.
\\In order to use the \textit{Extra Tree Regressor}, the user needs to set the sub-node $<SKLtype>tree~\vert~ExtraTreeRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Gaussian Process %%%%%%%
\paragraph{Gaussian Process.}
\label{GP}
Gaussian Processes for Machine Learning (GPML) is a generic supervised learning method primarily designed to solve regression problems.
The advantages of Gaussian Processes for Machine Learning are:
\begin{itemize}
\item The prediction interpolates the observations (at least for regular correlation models);
\item The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and exceedance probabilities that might be used to refit (online fitting, adaptive fitting) the prediction in some region of interest;
\item Versatile: different linear regression models and correlation models can be specified. Common models are provided, but it is also possible to specify custom models provided they are stationary.
\end{itemize}
The disadvantages of Gaussian Processes for Machine Learning include:
\begin{itemize}
\item It is not sparse. It uses the whole samples/features information to perform the prediction;
\item It loses efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens. It might indeed give poor performance and it loses computational efficiency;
\item Classification is only a post-processing, meaning that one first need to solve a regression problem by providing the complete scalar float precision output y of the experiment one attempt to model.
\end{itemize}
In order to use the \textit{Gaussian Process Regressor}, the user needs to set the sub-node $<SKLtype>GaussianProcess~\vert~GaussianProcess</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<regr>$ \textbf{\textit{, string, optional field.}}.  A regression function returning an array of outputs of the linear regression functional basis. The number of observations n\_samples should be greater than the size p of this basis. Available built-in regression models are:
'constant', 'linear', 'quadratic'. \textit{Default = constant};
\item $<corr>$ \textbf{\textit{, string, optional field.}}. A stationary autocorrelation function returning the autocorrelation between two points x and x’. Default assumes a squared-exponential autocorrelation model. Built-in correlation models are:
'absolute\_exponential', 'squared\_exponential','generalized\_exponential', 'cubic', 'linear'. \textit{Default = squared\_exponential};
\item $<beta0>$ \textbf{\textit{, float, array-like, optional field.}}. The regression weight vector to perform Ordinary Kriging (OK). \textit{Default = Universal Kriging};
\item $<storage\textunderscore~mode>$ \textbf{\textit{, string, optional field.}}. A string specifying whether the Cholesky decomposition of the correlation matrix should be stored in the class (storage\_mode = ‘full’) or not (storage\_mode = ‘light’). \textit{Default = full};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  boolean specifying the verbose level. \textit{Default = False};
\item $<theta0>$ \textbf{\textit{, float, array-like, optional field.}}.  An array with shape (n\_features, ) or (1, ). The parameters in the autocorrelation model. If thetaL and thetaU are also specified, theta0 is considered as the starting point for the maximum likelihood estimation of the best set of parameters. \textit{Default = [1e-1]};
\item $<thetaL>$ \textbf{\textit{, float, array-like, optional field.}}. An array with shape matching theta0’s. Lower bound on the autocorrelation parameters for maximum likelihood estimation. \textit{Default = None};
\item $<thetaU>$ \textbf{\textit{, float, array-like, optional field.}}. An array with shape matching theta0’s. Upper bound on the autocorrelation parameters for maximum likelihood estimation. \textit{Default = None};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  Input X and observations y are centered and reduced wrt means and standard deviations estimated from the n\_samples observations provided. \textit{Default = True};
\item $<nugget>$ \textbf{\textit{, float, optional field.}}.  Introduce a nugget effect to allow smooth predictions from noisy data.The nugget is added to the diagonal of the assumed training covariance; in this way it acts as a Tikhonov regularization in the problem. In the special case of the squared exponential correlation function, the nugget mathematically represents the variance of the input values. \textit{Default = 10. * MACHINE\_EPSILON};
\item $<optimizer>$ \textbf{\textit{, string, optional field.}}.  A string specifying the optimization algorithm to be used. Available optimizers are: 'fmin\_cobyla', 'Welch'. \textit{Default = fmin\_cobyla};
\item $<random\textunderscore~start>$ \textbf{\textit{, integer, optional field.}}. The number of times the Maximum Likelihood Estimation should be performed from a random starting point. The first MLE always uses the specified starting point (theta0), the next starting points are picked at random according to an exponential distribution (log-uniform on [thetaL, thetaU]). \textit{Default = 1};
\item $<random\textunderscore~state>$ \textbf{\textit{, integer, optional field.}}. Seed of the internal random number generator. \textit{Default = random}.
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
   <ROM name='***' subType='SciKitLearn'>
     <Features>***,***</Features>
     <SKLtype>linear_model|LinearRegression</SKLtype>
     <Target>***</Target>
     <fit_intercept>***</fit_intercept>
     <normalize>***</normalize>
   </ROM>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  External  Model   %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{External Model}
\label{subsec:models_externalModel}
As the name suggests, an external model  is an entity that is embedded in the RAVEN code at run time. This object allows the user to create a python module that is going to be treated as a predefined internal model object. 
In other words, the \textbf{External Model} is going to be treated by RAVEN as a normal external Code (e.g. it is going to be called in order to compute a whatever quantity, based on a whatever input).
\\The specifications of an External Model must be defined within the XML block $<ExternalModel>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this External Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (XML);
\item \textbf{subType}, \textit{required string attribute}, This string must be kept empty;
\item \textbf{ModuleToLoad}, \textit{required string attribute}, file name with its absolute or relative path. NB. If a relative path is specified, it must be noticed it is relative with respect to where the user runs the code.
\end{itemize}
\vspace{-5mm}
In order to make the RAVEN code aware of the variables the user is going to manipulate/use in its own python Module, the variables need to be specified in the \textbf{$<ExternalModel>$} input block. The user needs to input, within this block, only the variables that RAVEN needs to be aware of (i.e. the variables are going to directly be used by the code) and not the local variables that the user does not want to, for example, store in a RAVEN internal object. These variables are inputted within consecutive XML blocks called $<variable>$:
\begin{itemize}
\item $<variable>$, string, required parameter. In the body of this XML node, the user needs to specify the name of the variable. This variable needs to match a variable used/defined in the external python model.
\end{itemize}
When the external function variables are defined, at run time, RAVEN initialize those and take track of their values during the simulation. Each variable defined in the $<ExternalModel>$ block is available in the module (each method implemented) as a python ``self''. 
\\ In the External Python module, the user can implement all the methods that are needed for the functionality of the model, but only the following methods are going, if present, called by the framework:
\begin{itemize}
\item \textit{\textbf{def \textunderscore~readMoreXML}, OPTIONAL METHOD}, This method can be implemented by the user if the XML input that belongs to this External Model needs to be extended to contain other information. The information read needs to be stored in ``self'' in order to be available to all the other methods (e.g. if the user needs to add a couple of newer XML nodes with information needed by the algorithm implemented in the ``run'' method);
\item \textit{\textbf{def initialize}, OPTIONAL METHOD}, Initialization method. In this function the user can implement all the actions need to be performed at the initialization stage;
\item \textit{\textbf{def createNewInput}, OPTIONAL METHOD}, Method to create a new input with the information coming from the RAVEN framework. In this function the user can retrieve the information coming from the RAVEN framework, during the employment of a calculation flow, and use them to construct a new input that is going to be transferred to the ``run'' method;
\item \textit{\textbf{def run}, REQUIRED METHOD}, This is the actual location where the user needs to implement the model action (e.g. resolution of a set of equations, etc.). This function is going to receive the Input(or Inputs) generated either by  the External Model ``createNewInput'' method or the internal RAVEN one;
\end{itemize}
In the following sub-sections, all the methods are going to be analyzed in detail.
\subsubsection{Method: def \textunderscore~readMoreXML.}
\label{subsubsec:externalReadMoreXML}
As already mentioned, the \textbf{readMoreXML} method can be implemented by the user if the XML input that belongs to this External Model needs to be extended to contain other information. The information read needs to be stored in ``self'' in order to be available to all the other methods (e.g. if the user needs to add a couple of newer XML nodes with information needed by the algorithm implemented in the ``run'' method).
If this method is implemented in the \textbf{External Model}, RAVEN is going to call it when the node $<ExternalModel>$ is found parsing XML input file. The method receives from RAVEN an attribute of type ``xml.etree.ElementTree'', containing all the sub-nodes and attribute of the XML block $<ExternalModel>$.
\\In the following an example is reported:
\begin{lstlisting}[style=XML]
------------------------------------
<Simulation> 
  ...
  <Models>
     ...
    <ExternalModel name='AnExtModule' 
                              subType='' 
                              ModuleToLoad='path_to_external_module'>  
       <variable>sigma</variable>
       <variable>rho</variable>
       <variable>outcome</variable>
       <!-- 
          here we define other nodes RAVEN does not read automatically 
          => We need to implement, in the external module ``AnExtModule''
          the readMoreXML method
        -->
        <newNodeWeNeedToRead>
            whatNeedsToBeRead
        </newNodeWeNeedToRead>
    </ExternalModel>
     ...
  </Models> 
  ...
</Simulation> 
\end{lstlisting}
\begin{lstlisting}[language=python]
------------------------------------
def _readMoreXML(self,xmlNode):
  # the xmlNode is passed in by RAVEN framework
  # <newNodeWeNeedToRead> is unknown (in the RAVEN framework)
  # we have to read it on our own
  # get the node
  ourNode = xmlNode.find(``newNodeWeNeedToRead'')
  # get the information in the node
  self.ourNewVariable = ourNode.text
  # end function
------------------------------------
\end{lstlisting}
\subsubsection{Method: def initialize.}
\label{subsubsec:externalInitialize}
The \textbf{initialize} method can be implemented in the  \textbf{External Model} in order to initialize some variables needed by it. For example, it can be used to compute a quantity needed by the ``run'' method before performing the actual calculation).
If this method is implemented in the \textbf{External Model}, RAVEN is going to call it at the initialization stage of each ``Step'' (see section \ref{sec:steps}. RAVEN will communicate, thorough a set of method attributes, all the information that are generally needed to perform a initialization:
\begin{itemize}
\item runInfo, a dictionary containing information regarding how the calculation is set up (e.g. number of processors, etc.). It contains the following attributes:
    \begin{itemize}
       \item \textit{DefaultInputFile},  Default input file to use
       \item \textit{SimulationFiles}, the xml input file
       \item \textit{ScriptDir}, the location of the pbs script interfaces
       \item \textit{FrameworkDir}, the directory where the framework is located
       \item \textit{WorkingDir}, the directory where the framework should be running
       \item \textit{TempWorkingDir}, the temporary directory where a simulation step is run
       \item \textit{NumMPI}, the number of mpi process by run
       \item \textit{NumThreads}, Number of Threads by run
       \item \textit{numProcByRun}, Total number of core used by one run (number of threads by number of mpi)
       \item \textit{batchSize}, number of contemporaneous runs
       \item \textit{ParallelCommand}, the command that should be used to submit jobs in parallel (mpi)
       \item \textit{numNode},  number of nodes
       \item \textit{procByNode},  number of processors by node
       \item \textit{totalNumCoresUsed}, total number of cores used by driver
       \item \textit{queueingSoftware}, queueing software name
       \item \textit{stepName}, the name of the step currently running
       \item \textit{precommand}, Add to the front of the command that is run
       \item \textit{postcommand}, Added after the command that is run
       \item \textit{delSucLogFiles}, If a simulation (code run) has not failed, delete the relative log file (if True) 
       \item \textit{deleteOutExtension}, If a simulation (code run) has not failed, delete the relative output files with the listed extension (comma separated list, for example: 'e,r,txt')
       \item \textit{mode},  Running mode.  Curently the only modes supported are pbsdsh and mpi
       \item \textit{expectedTime}, How long the complete input is expected to run
       \item \textit{logfileBuffer}, logfile buffer size in bytes
    \end{itemize}
\item inputs, a list of all the inputs that have been specified in the ``Step'' is using this model.
\end{itemize}
In the following an example is reported:
\begin{lstlisting}[language=python]
------------------------------------
def initialize(self,runInfo,inputs):
 # Let's suppose we just need to initialize some variables
  self.sigma = 10.0
  self.rho   = 28.0
  # end function
------------------------------------
\end{lstlisting}
\subsubsection{Method: def createNewInput.}
\label{subsubsec:externalcreateNewInput}
The \textbf{createNewInput} method can be implemented by the user to create a new input with the information coming from the RAVEN framework. In this function the user can retrieve the information coming from the RAVEN framework, during the employment of a calculation flow, and use them to construct a new input that is going to be transferred to the ``run'' method.
The new input created needs to be returned to RAVEN (i.e. ``return NewInput'').
RAVEN communicates, thorough a set of method attributes, all the information that are generally needed to create a new input:
myInput,samplerType,**Kwargs
\begin{itemize}
\item inputs, python list, a list of all the inputs that have been defined in the ``Step'' is using this model;
\item samplerType, string, the type of Sampler, if a Sampling strategy is employed; None otherwise;
\item Kwargs, dictionary, a dictionary containing several informations (that can change based on the ``Step'' type). If a Sampling strategy is employed, this dictionary contains another one identified by the keyword ``SampledVars'', in which the variables perturbed by the sampler are reported;
\end{itemize}
NB. If the ``Step'' that is using this Model has as input(s) an object of main class type ``Datas'' (see section \ref{datas}), the internal ``createNewInput'' method is going to convert it in a dictionary of values.
\\In the following an example is reported:
\begin{lstlisting}[language=python]
------------------------------------
def createNewInput(self,inputs,samplerType,**Kwargs):
  # in here the actual createNewInput of the 
  # model is implemented
  if samplerType == ``MonteCarlo'':
    avariable = inputs[``something'']*inputs[``something2'']
  else:
    avariable = inputs[``something'']/inputs[``something2'']
  return avariable*Kwargs[``SampledVars''][``aSampledVar'']
------------------------------------
\end{lstlisting} 
\subsubsection{Method: def run.}
\label{subsubsec:externalRun}
As stated previously, the only method the MUST be present in an External Module is the \textbf{run} function. In this function , the user needs to implement the algorithm that RAVEN has to execute. The  \textbf{run} method is generally called after having inquired the ``createNewInput'' method (either the internal  or the user-implemented one). The only attribute this method is going to receive by is a Python list of inputs (the inputs coming from the ``createNewInput'' method).
If the user wants RAVEN to collect the results of this method, the outcomes of interest need to be stored in ``self''. NB. RAVEN is trying to collect the values of the variables listed in the XML  block $<ExternalModel>$ only.
\\In the following an example is reported:

\begin{lstlisting}[language=python]
------------------------------------
def run(self,Input):
  # in here the actual run of the 
  # model is implemented
  input = Input[0]
  self.outcome = self.sigma*self.rho*input[``whatEver'']
------------------------------------
\end{lstlisting} 

%\subsection{Projector}
%\label{sec:models_projector}
%
%Description

%Summary

%Example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  External  PostProcessor   %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PostProcessor}
\label{sec:models_postProcessor}
A Post-Processor (PP) can be considered as an Action performed on a set of data or other type of objects. Most of the post-processors contained in RAVEN, employ a mathematical operation on the data given as ``input''/
RAVEN supports several different types of PPs. Currently in RAVEN the following Post-Processors are available:
\begin{itemize}
   \item \textbf{BasicStatistics;}
   \item \textbf{ComparisonStatistics;}
   \item \textbf{SafestPoint;}
   \item \textbf{LimitSurface;}
   \item \textbf{PrintCSV;}
   \item \textbf{LoadCsvIntoInternalObject.}
\end{itemize}
The specifications of this Model must be defined within the XML block $<PostProcessor>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, in this attribute the user defines which of the post-processors needs to be used, choosing among the previously reported types. Obviously, this choice conditions the subsequent the required and/or optional $<PostProcessor>$ sub nodes.
\end{itemize}
\vspace{-5mm}
As already mentioned, all the types and meaning of the remaining sub-nodes depend on the post-processor type specified in the attribute \textit{subType}. In the following sections the specifications of each type are reported.
%%%%% PP BasicStatistics %%%%%%%
\paragraph{BasicStatistics.}
\label{BasicStatistics}
The \textbf{BasicStatistics} post-processor is the container of the algorithms to compute all the most important statistical quantities. 
\\In order to use the   \textit{BasicStatistics} post-processor, the user needs to set the sub-node $<subType>BasicStatistics</subType>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<what>$ \textbf{\textit{, comma separated string, required field.}}.  List of quantities need to be computed. Currently the quantities available are:
\begin{itemize}
   \item \textbf{covariance matrix,} Covariance matrix;
   \item \textbf{NormalizedSensitivity,} Matrix of normalized sensitivity coefficients;
   \item \textbf{sensitivity,} Matrix of sensitivity coefficients;
   \item \textbf{pearson,} Matrix of sensitivity coefficients;
   \item \textbf{expectedValue}, Expected value;
   \item \textbf{sigma,} Standard deviation;
   \item \textbf{variationCoefficient,} Coefficient of variation (sigma/expected value);
   \item \textbf{variance,} Variance;
   \item \textbf{skewness,} Skewness;
   \item \textbf{kurtois,} Kurtois;
   \item \textbf{median,} Data median;
   \item \textbf{percentile,} 95 percentile.
\end{itemize}
If all the quantities need to be computed, the user can input in the body of $<what>$ the string ``all''.
\item $<parameters>$ \textbf{\textit{, comma separated string, required field.}}. List of the parameters on which the previous operations need to be applied on (e.g., massFlow, Temperature);
\item $<methodsToRun>$ \textbf{\textit{, comma separated string, optional field.}}.  The method names of an external Function that need to be run before computing any of the predefined quantities If this XML node is inputted, the $<Assembler>$ node must be present. \textit{Default = None};
\item $<Assembler>$\textbf{\textit{, xml node, required field.}} This xml node contains a ``list'' of objects that are  optional for the functionality of the  \textbf{BasicStatistics} post-processor. The objects must be listed with a rigorous syntax that, except for the xml node tag, is common among all the objects.  
Each of these sub-nodes  must contain 2 attributes that are used to map those within the simulation framework:
   \begin{itemize}
     \item \textbf{class}, \textit{required string attribute}, it is the main ``class'' the listed object is from;
     \item \textbf{type},  \textit{required string attribute}, it is the object identifier or sub-type.
    \end{itemize}
The  \textbf{BasicStatistics} post-processor approach  optionally accepts the following object type:
   \begin{itemize}
       \item $<Function>$\textbf{\textit{, string, required field.}}.The body of this xml block needs to contain the name of an External Function defined within the $<Functions>$ main block (see section \ref{sec:functions}). This object needs to containt the methods listed in the node $<methodsToRun>$.;
    \end{itemize}
\end{itemize}
\textbf{Example:}
----------------------------------------------
\begin{lstlisting}[style=XML]
<Simulation>
   ...
   <Models>
      ...
      <PostProcessor name="***" subType='BasicStatistics'  debug='true'>
          <!-- => you can here specify what type of figure of merit
              you need to compute
             <what>expectedValue,sigma,variance,kurtois,pearson,covariance</what> 
           -->
          <what>expectedValue</what>
          <parameters>x01,x02</parameters>
          <methodsToRun>failureProbability</methodsToRun>
      </PostProcessor>
      ...
   <Models>
   ...
<Simulation>
----------------------------------------------
\end{lstlisting}

%%%%% PP ComparisonStatistics %%%%%%%
\paragraph{ComparisonStatistics.}
\label{ComparisonStatistics}
To be finalized.

%%%%% PP SafestPoint %%%%%%%
\paragraph{SafestPoint.}
\label{SafestPoint}
The \textbf{SafestPoint} post-processor provides the coordinates of the farthest point from the limit surface that is given as an input.
The safest point coordinates are expected values of the coordinates of the farthest points from the limit surface in the space of the ''controllable'' variables based on the probability distributions of the ''non-controllable'' variables.
\\The term ''controllable'' identifies those variables that are under control during the system operation, while the ''non-controllable'' variables are stochastic parameters affecting the system behaviour randomly. 
\\The ''SafestPoint'' post-processor requires the set of points belonging to the limit surface, which must be given as an input. The ''Assembler'' subsection requires the probability distributions of both ''controllable'' and ''non-controllable'' variables.
\\The sampling method used by the ''SafestPoint'' is a ''value'' or ''CDF'' grid. At present only the ''equal'' grid type is available.
\begin{itemize}
	\item $<Assembler>$: the probability distributions of the ''controllable'' and ''non-controllable'' variables are required.
	\item $<controllable>$: the list of the controllable variables is given here. Each variable is associated with its name and the two items below:
		\begin{itemize}
			\item $<distribution>$: the name of the probability distribution associated with the controllable variable is specified here.
			\item $<grid>$: type, number of steps and tolerance of the sampling grid are defined here.
		\end{itemize}
	\item $<non-controllable>$: the list of the non-controllable variables is given here. Each variable is associated with its name and the two items below:
		\begin{itemize}
			\item $<distribution>$: the name of the probability distribution associated with the non-controllable variable is specified here.
			\item $<grid>$: type, number of steps and tolerance of the sampling grid are defined here.
		\end{itemize}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
----------------------------------------------
<Simulation>
   ...
   <Models>
      ...
  <PostProcessor name='SP' subType='SafestPoint'>
      <Assembler>
        <Distribution  class = 'Distributions'  type = 'Normal'>x1_dst</Distribution>
        <Distribution  class = 'Distributions'  type = 'Normal'>x2_dst</Distribution>
        <Distribution  class = 'Distributions'  type = 'Normal'>gammay_dst</Distribution>
      </Assembler>
      <controllable>
        <variable name = 'x1'>
          <distribution>x1_dst</distribution>
          <grid type = 'value' steps = '20'>1</grid>   		
        </variable>
        <variable name = 'x2'>
          <distribution>x2_dst</distribution>
          <grid type = 'value' steps = '20'>1</grid>   		
        </variable>
        </controllable>
        <non-controllable>
          <variable name = 'gammay'>
            <distribution>gammay_dst</distribution>
            <grid type = 'value' steps = '20'>2</grid>
          </variable> 	
        </non-controllable>
    </PostProcessor>
      ...
   </Models>
   ...
</Simulation>
----------------------------------------------
\end{lstlisting}
%%%%% PP LimitSurface %%%%%%%
\paragraph{LimitSurface.}
\label{LimitSurface}
The \textbf{LimitSurface} post-processor is aimed to identify the transition zones that determine a change in the status of the system (Limit Surface). 
\\In order to use the   \textit{LimitSurface}, the user needs to set the sub-node $<subType>LimitSurface</subType>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<parameters>$ \textbf{\textit{, comma separated string, required field.}}. List of the parameters that define the uncertain domain and from which the LS needs to be computed;
\item $<tollerance>$ \textbf{\textit{, float, optional field.}}. Absolute value converge tollerance. This value defines the coarsens  of the evaluation grid. \textit{Default= 1.e-4};
 \item $<Assembler>$\textbf{\textit{, xml node, required field.}} This xml node contains a ``list'' of objects that are required (or optional) for the functionality of the Adaptive Sampler. The objects must be listed with a rigorous syntax that, except for the xml node tag, is common among all the objects.  
Each of these sub-nodes  must contain 2 attributes that are used to map those within the simulation framework:
   \begin{itemize}
     \item \textbf{class}, \textit{required string attribute}, it is the main ``class'' the listed object is from. For example, it can be ``Models'', ``Functions'', etc;
     \item \textbf{type},  \textit{required string attribute}, it is the object identifier or sub-type. For example, it can be ``ROM'', ``External'', etc.
    \end{itemize}
   The \textbf{LimitSurface} post-processor requires or optionally accepts the following objects' types:
   \begin{itemize}
     \item $<ROM>$\textbf{\textit{, string, optional  field.}}. If inputted, the body of this xml node must contain the name of a ROM defined in the $<Models>$ block (see section \ref{subsec:models_ROM});
       \item $<Function>$\textbf{\textit{, string, required field.}}.The body of this xml block needs to contain the name of an External Function defined within the $<Functions>$ main block (see section \ref{sec:functions}). This object represents the boolean function that defines the transition boundaries. This function must implement a method called \textit{\_\_residuumSign(self)}, that returns either -1 or 1, depending on the system conditions (see section \ref{sec:functions}.
    \end{itemize}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
----------------------------------------------
<Simulation>
   ...
   <Models>
      ...
    <PostProcessor name="computeLimitSurface" subType='LimitSurface' debug='True'>
        <what>all</what>
        <parameters>x0,y0</parameters>
        <parameters>x0,y0</parameters>
        <Assembler>
            <ROM      class='Models'           type='ROM'             >Acc</ROM> 
            <!--You can add here a ROM defined in Models block.If not Present, 
                    a nearest algorithm is going to be used
              -->
            <Function class='Functions'        type='External'        >goalFunctionForLimitSurface</Function>
        </Assembler>
    </PostProcessor>
      ...
   </Models>
   ...
</Simulation>
----------------------------------------------
\end{lstlisting}
%%%%% PP PrintCSV %%%%%%%
\paragraph{PrintCSV.}
\label{PrintCSV}
TO BE MOVED TO STEP ``IOSTEP''
%%%%% PP LoadCsvIntoInternalObject %%%%%%%
\paragraph{LoadCsvIntoInternalObject.}
\label{LoadCsvIntoInternalObject}
TO BE MOVED TO STEP ``IOSTEP''