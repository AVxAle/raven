\section{Models  \\ \vspace{2 mm} {\small }}
\label{sec:models}
In the RAVEN code a crucial entity is represented by a Model. A model is an object that employs a mathematical representation of a phenomenology, either physical or of other nature (e.g. statistical operators, etc.). From a practical point of view, it can be seen, as a ``black box'' that, given an input, returns an output. 
\\ In the RAVEN code, a strict  classification of the different models is present. As obviously, each ``class'' of models is represented by the definition reported above, but it can be further classified based on the peculiar functionalities:
\begin{itemize}
\item \textbf{Code}. This ``class'' is the representation of an external system code that employs an high fidelity physical model;
\item \textbf{Dummy}. The ``Dummy'' object is a model that acts as ``transfer'' tool. The only action it performs is transferring the the information in the input space (inputs) into the output space (outputs). For example, it can be used to check the effect of a Sampling strategy, since its outputs are the sampled parameters' values (input space) and a counter that keeps track of the number of times an evaluation has been requested; 
\item \textbf{ROM}. A ROM is a mathematical model of fast solution trained to predict a response of interest of a physical system. The ``training'' process is performed by “sampling” the response of a physical model with respect variation of its parameters subject to probabilistic behavior. The results (outcomes of the physical model) of those sampling are fed into the algorithm representing the ROM that tunes itself to replicate those results;
\item \textbf{ExternalModel}. As the name suggests, an external model  is an entity that is embedded in the RAVEN code at run time. This object allows the user to create a python module that is going to be treated as a predefined internal model object;
%\item [Projector:] generic data manipulator
\item \textbf{PostProcessor}.  The post-processor ``class'' of objects  is the container of all the actions that can be performed to manipulate and process the data in order to extract key information, such as statistical quantities, etc. 
\end{itemize}
Before analyzing  each model in details, it is important to mention that each type needs to be contained in the main XML node $<Models>$, as reported below:

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <WhatEverModel name='whatever'>
      ... 
    </WhatEverModel>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
In the following sub-sections each \textbf{Model} type is fully analyzed and described.
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  Code  Model   %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%<Code name='MyRAVEN' subType='RAVEN'><executable>%FRAMEWORK_DIR%/../RAVEN-%METHOD%</executable></Code>
%<alias variable='internal_variable_name'>Material|Fuel|thermal_conductivity</alias>
\subsection{Code}
\label{subsec:models_code}
As already mentioned, the model \textbf{Code} is the representation of an external system software that employs an high fidelity physical model. The link between RAVEN and the driven code is performed at run time, through coded interfaces that are the responsible of transferring the information from the code to RAVEN and vice versa. In section \ref{sec:existingInterface} all the available interfaces are reported and, for advanced users, section \ref{sec:newCodeCoupling} explains how to couple a newer code.
\\ The specifications of this Model must be defined within the xml block $<Code>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, in this attribute the user selects the code that needs to be associated to this Model. NB. See section \ref{sec:existingInterface} to check which codes are currently supported.
\end{itemize}
\vspace{-5mm}

In the \textbf{Code} input block, the following XML sub-nodes are available:
\begin{itemize}
   \item $<executable>$ \textbf{\textit{, string, required field.}}. In this node, the user needs to specify the path of the executable to be used. NB. In this node, either the absolute or relative path can be inputted;
    \item $<alias>$ \textbf{\textit{, string, optional field.}}. In the $<alias>$ block the user can specify aliases for some variables of interest coming from the code this model refers to. These aliases can be used in the whole input to refer to the code variables. In the body of this node the user specifies the name of the variable that RAVEN will look for in the output files of the code. The actual alias, usable throughout the input, are instead defined in the attribute \textbf{variable}.
 NB. The user can specify as many aliases as needed. \textit{Default = None}. 
\end{itemize}
\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <Code name='***' subType='RAVEN_Driven_code'>
      <executable>path_to_executable</executable>
      <alias variable='internal_variable_name1'>
         External_Code_Variable_Name_1
      </alias>
      <alias variable='internal_variable_name2'>
         External_Code_Variable_Name_2
      </alias>
    </Code>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Dummy Model  %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dummy.}
\label{subsec:models_dummy}
The model \textbf{Dummy} is an object that acts as ``transfer'' tool. The only action it performs is transferring the the information in the input space (inputs) into the output space (outputs). For example, it can be used to check the effect of a Sampling strategy, since its outputs are the sampled parameters' values (input space) and a counter that keeps track of the number of times an evaluation has been requested.
\\ The specifications of this Model must be defined within the xml block $<Dummy>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, this attribute must be kept empty.
\end{itemize}
\vspace{-5mm}
If this model, in a \textit{Step}, is associated to a \textit{Data} with the role of \textbf{Output}, it expects that one of the output parameters of such \textit{Data} is identified by the keyword ``OutputPlaceHolder'' (see section \ref{sec:steps}).

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <Dummy name='***' subType=''/>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%
%%%%% ROM Model  %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{ROM}
\label{subsec:models_ROM}
A Reduced Order Model (ROM) is a mathematical model of fast solution trained to predict a response of interest of a physical system. The ``training'' process is performed by “sampling” the response of a physical model with respect variation of its parameters subject, for example, to probabilistic behavior. The results (outcomes of the physical model) of those sampling are fed into the algorithm representing the ROM that tunes itself to replicate those results.
RAVEN supports several different types of ROMs, both internally developed and imported through an external library called ``SciKitLearn'' ~\cite{SciKitLearn}. Currently in RAVEN the Reduced Order Models are classified in 4 main ``classes'' that, once chosen, provide access to several different algorithms:
\begin{itemize}
   \item \textbf{NDspline;}
   \item \textbf{NDinvDistWeigth;}
   \item \textbf{microSphere;}
   \item \textbf{SciKitLearn.}
\end{itemize}
The specifications of this Model must be defined within the XML block $<ROM>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, in this attribute the user defines which of the main ``classes'' needs to be used, choosing among the previously reported types. Obviously, this choice conditions the subsequent the required and/or optional $<ROM>$ sub nodes.
\end{itemize}
\vspace{-5mm}

In the \textbf{ROM} input block, the following XML sub-nodes are required, independently on the ``main'' class inputted in the attribute \textit{subType}:
\begin{itemize}
   \item $<Features>$ \textbf{\textit{, comma separated string, required field.}}. In this node, the user needs to specify the names of the features of this ROM. NB. These parameters are going to be requested for the training of this object (see section \ref{subsec:stepTraining};
    \item $<Target>$ \textbf{\textit{, comma separated string, required field.}}. This XML node contains a comma separated list of the targets of this ROM. By Instance, these parameters are the Figure of Merits this ROM is supposed to predict. NB. These parameters are going to be requested for the training of this object (see section \ref{subsec:stepTraining}.
\end{itemize}
As already mentioned, all the types and meaning of the remaining sub-nodes depend on the main ``class'' type specified in the attribute \textit{subType}. In the following sections the specifications of each type are reported.
%%%%% ROM Model - NDspline  %%%%%%%
\subsubsection{NDspline.}
\label{subsubsec:NDspline}
The main ``class'' NDspline contains a single ROM type, based on a N-Dimensional spline interpolation/extrapolation. The spline interpolation is a form of interpolation where the interpolant is a special type of piecewise polynomial called a spline. The interpolation error can be made small even when using low degree polynomials for the spline. Spline interpolation avoids the problem of Runge's phenomenon, in which oscillation can occur between points when interpolating using high degree polynomials.
\\In order to use this Reduced Order Model, the $<ROM>$ attribute \textit{subType} needs to be ``NDspline'' (i.e. \textit{subType = ``NDspline''}). No further XML sub-nodes are required.
\\NB. This ROM type must be trained from a Regular Cartesian Grid. By instance, it can only be trained from the outcomes of a Grid Sampling strategy. 

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <ROM name='***' subType='NDspline'>
       <Features>***,***,***</Features> 
       <Target>***,***</Target>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
%%%%% ROM Model - NDinvDistWeigth  %%%%%%%
\subsubsection{NDinvDistWeigth.}
\label{subsubsec:NDinvDistWeigth}
The main ``class'' NDinvDistWeigth contains a single ROM type, based on a N-Dimensional Inverse Distance Weighting formulation. Inverse Distance Weighting (IDW) is a type of deterministic method for multivariate interpolation with a known scattered set of points. The assigned values to unknown points are calculated with a weighted average of the values available at the known points. 
\\In order to use this Reduced Order Model, the $<ROM>$ attribute \textit{subType} needs to be ``NDinvDistWeigth'' (i.e. \textit{subType = ``NDinvDistWeigth''}). The specification of the ROM \textit{``NDinvDistWeigth''} needs to be completed inputting, within the main XML node $<ROM>$, of the following sub-node:
\begin{itemize}
\item $<p>$ \textbf{\textit{, integer, required field.}}. This node contains an $integer > 0$ that represents the ``power parameter'. For the choice of value for $<p>$,it is necessary to consider the degree of smoothing desired in the interpolation/extrapolation, the density and distribution of samples being interpolated, and the maximum distance over which an individual sample is allowed to influence the surrounding ones (lower p means greater importance for points faraway).
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
    <ROM name='***' subType='NDinvDistWeigth'>
       <Features>***,***,***</Features> 
       <Target>***</Target>
       <p>3</p>
     </ROM>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}
%%%%% ROM Model - MicroSphere  %%%%%%%
\subsubsection{MicroSphere.}
\label{subsubsec:microSphere}
Not yet functional. Its validity for prediction purposes  needs to be still assessed.
%%%%% ROM Model - SciKitLearn  %%%%%%%
\subsubsection{SciKitLearn.}
\label{subsubsec:SciKitLearn}
The main ``class'' SciKitLearn represents the container of several Reduced Order Models that are available in RAVEN through the external library SciKitLearn~\cite{SciKitLearn}.
\\In order to use this Reduced Order Model, the $<ROM>$ attribute \textit{subType} needs to be ``SciKitLearn'' (i.e. \textit{subType = ``SciKitLearn''}). The specifications of the ROM \textit{``SciKitLearn''} depends on value assumed by the following sub-node within the main XML node $<ROM>$:
\begin{itemize}
\item $<SKLtype>$ \textbf{\textit{, vertical bar ($\vert$) separated string , required field.}}. This nodes contains a string that represents the ROM type that needs to be used. As mentioned, its format is, for example, $<SKLtype>$\textit{mainSKLclass}~$\vert$~\textit{algorithm} $</SKLtype>$: the first word (before symbol $\vert$) represents the main class of algorithms; the second word (after symbol $\vert$) represents the specific algorithm.
\end{itemize}
Based on the $<SKLtype>$ several different algorithms are available. In the following paragraphs a brief explanation and the input requirements are reported for each of them.
%%%%% ROM Model - SciKitLearn: Linear Models %%%%%%%
\paragraph{Linear Models.}
\label{LinearModels}
The LinearModels' type of algorithms implement generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.
In the following, all the linear models available in RAVEN are reported.
\subparagraph{Linear Model: Automatic Relevance Determination regression}
\mbox{}
\\The \textit{Automatic Relevance Determination} regressor is a hierarchical Bayesian approach where there are hyperparameters which explicitly represent the relevance of different input features. These relevance hyperparameters determine the range of variation for the parameters relating to a particular input, usually by modelling the width of a zero-mean Gaussian prior on those parameters. If the width of that Gaussian is zero, then those parameters are constrained to be zero, and the corresponding input cannot have any effect on the predictions, therefore making it irrelevant. ARD optimizes these hyperparameters to discover which inputs are relevant.
In order to use the  \textit{Automatic Relevance Determination} regressor, the user needs to set the sub-node $<SKLtype>ARDRegression</SKLtype>$. In addition to this XML node, several other need (or not) be inputted:
\begin{itemize}
\item $<n_iter>$ \textbf{\textit{, integer, optional field.}}.  Maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  Stop the algorithm if w has converged. \textit{Default = 1.e-3};
\item $<alpha_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<alpha_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<lambda_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<lambda_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<compute_score>$ \textbf{\textit{, boolean, optional field.}}.  If True, compute the objective function at each step of the model. \textit{Default =  False};
\item $<threshold_lambda>$ \textbf{\textit{, float, optional field.}}.  Threshold for removing (pruning) weights with high precision from the computation. \textit{Default =  1.e+4};
\item $<fit_intercept>$ \textbf{\textit{, float, optional field.}}.  whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). \textit{Default =  True};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\end{itemize}
%%%%%%%%
\subparagraph{Linear Model: Bayesian ridge regression}
\mbox{}
\\The \textit{Bayesian ridge regression}  estimates a probabilistic model of the regression problem as described above. The prior for the parameter w is given by a spherical Gaussian:
\begin{equation}
p(w|\lambda) =\mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})
\end{equation}
The priors over $\alpha$ and $\lambda$ are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian.
The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge. The parameters $w, \alpha$ and $\lambda$ are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over $\alpha$ and $\lambda$. These are usually chosen to be non-informative. The parameters are estimated by maximizing the marginal log likelihood.
In order to use the  \textit{Bayesian ridge regression} regressor, the user needs to set the sub-node $<SKLtype>BayesianRidge</SKLtype>$.  In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_iter>$ \textbf{\textit{, integer, optional field.}}.  Maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  Stop the algorithm if w has converged. \textit{Default = 1.e-3};
\item $<alpha_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<alpha_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. \textit{Default =  1.e-6};
\item $<lambda_1>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<lambda_2>$ \textbf{\textit{, float, optional field.}}.  Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. \textit{Default =  1.e-6};
\item $<compute_score>$ \textbf{\textit{, boolean, optional field.}}.  If True, compute the objective function at each step of the model. \textit{Default =  False};
\item $<threshold_lambda>$ \textbf{\textit{, float, optional field.}}.  Threshold for removing (pruning) weights with high precision from the computation. \textit{Default =  1.e+4};
\item $<fit_intercept>$ \textbf{\textit{, float, optional field.}}.  whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). \textit{Default =  True};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\end{itemize}
%%%%%%%
\subparagraph{Linear Model: Elastic Net}
\mbox{}
\\The \textit{Elastic Net} is a linear regression with combined L1 and L2 priors as regularizer.
It minimizes the objective function:
\begin{equation}
1/(2*n_samples) *||y - Xw||^2_2+alpha*l1_ratio*||w||_1 + 0.5 *alpha*(1 - l1_ratio)*||w||^2_2
\end{equation}
In order to use the  \textit{Elastic Net} regressor, the user needs to set the sub-node $<SKLtype>ElasticNet</SKLtype>$.  In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<alpha>$ \textbf{\textit{, float, optional field.}}.  Constant that multiplies the penalty terms. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. \textit{Default = 1.0};
\item $<l1_ratio>$ \textbf{\textit{, float, optional field.}}.  The ElasticNet mixing parameter, with $0 <= l1_ratio <= 1$. For $l1_ratio = 0$ the penalty is an L2 penalty. For $l1_ratio = 1$ it is an L1 penalty. For $0 < l1_ratio < 1$, the penalty is a combination of L1 and L2. \textit{Default = 0.5};
\item $<fit_intercept>$ \textbf{\textit{, boolean, optional field.}}.  Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. \textit{Default =  True};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<max_iter>$ \textbf{\textit{, integer, optional field.}}.  The maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.. \textit{Default = 1.0e-4};
\item $<warm_start >$ \textbf{\textit{, boolean, optional field.}}.  When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.. \textit{Default =  False};
\item $<positive>$ \textbf{\textit{, float, optional field.}}.  When set to True, forces the coefficients to be positive. \textit{Default =  False}.
\end{itemize}
%%%%%%%%
\subparagraph{Linear Model: Elastic Net CV}
\mbox{}
\\The \textit{Elastic Net CV} is a linear regression similar to Elastic Net model but with an iterative fitting along a regularization path. The best model is selected by cross-validation.
\\In order to use the  \textit{Elastic Net CV} regressor, the user needs to set the sub-node $<SKLtype>ElasticNetCV</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<l1_ratio>$ \textbf{\textit{, float, optional field.}}. Float flag between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For $l1_ratio = 0$ the penalty is an L2 penalty. For $l1_ratio = 1$ it is an L1 penalty. For $0 < l1_ratio < 1$, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for $l1_ratio$ is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1, .5, .7, .9, .95, .99, 1]. \textit{Default = 0.5};
\item $<eps>$ \textbf{\textit{, float, optional field.}}.  Length of the path. eps=1e-3 means that $alpha_min / alpha_max = 1e-3$. \textit{Default = 0.001};
\item $<n_alphas>$ \textbf{\textit{, integer, optional field.}}.  Number of alphas along the regularization path, used for each $l1_ratio$. \textit{Default = 100};
\item $<precompute>$ \textbf{\textit{, boolean or string, optional field.}}.  Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Available are [True | False | ‘auto’ | array-like]. \textit{Default = 1.0};
\item $<max_iter>$ \textbf{\textit{, integer, optional field.}}.  The maximum number of iterations. \textit{Default = 300};
\item $<tol>$ \textbf{\textit{, float, optional field.}}.  The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.. \textit{Default = 1.0e-4};
\item $<positive>$ \textbf{\textit{, float, optional field.}}.  When set to True, forces the coefficients to be positive. \textit{Default =  False}.
\end{itemize}
%%%%%%
\subparagraph{Linear Model: Least Angle Regression model}
\mbox{}
\\The \textit{Least Angle Regression model} (LARS)  is a regression algorithm for high-dimensional data. LARS algorithm provides a means of producing an estimate of which variables to include, as well as their coefficients, when a  response variable is determined by a linear combination of a subset of potential covariate.
\\In order to use the  \textit{Least Angle Regression model} regressor, the user needs to set the sub-node $<SKLtype>Lars</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_nonzero_coef>$ \textbf{\textit{, integer, optional field.}}. Target number of non-zero coefficients. \textit{Default = 500};
\item $<fit_intercept>$ \textbf{\textit{, boolean, optional field.}}.  Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. \textit{Default =  True};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\item $<precompute>$ \textbf{\textit{, boolean or string, optional field.}}.  Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Available are [True | False | ‘auto’ | array-like]. \textit{Default = 1.0};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<eps>$ \textbf{\textit{, float, optional field.}}.  The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. \textit{Default =2.22e-16};
\item $<fit_path>$ \textbf{\textit{, boolean, optional field.}}.  f True the full path is stored in the coef\_path\_ attribute. If you compute the solution for a large problem or many targets, setting fit\_path to False will lead to a speedup, especially with a small alpha. \textit{Default =  True}.
\end{itemize}
%%%%%%
\subparagraph{Linear Model: Cross-validated Least Angle Regression model}
\mbox{}
\\The \textit{Cross-validated Least Angle Regression model} is a regression algorithm for high-dimensional data. It is similar to LARS method, but the best model is selected by cross-validation.
\\In order to use the  \textit{Cross-validated Least Angle Regression model} regressor, the user needs to set the sub-node $<SKLtype>LarsCV</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<fit_intercept>$ \textbf{\textit{, boolean, optional field.}}.  Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. \textit{Default =  True};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  Verbose mode when fitting the model. \textit{Default =  False};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  If True, the regressors X will be normalized before regression. \textit{Default =  False};
\item $<precompute>$ \textbf{\textit{, boolean or string, optional field.}}.  Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Available are [True | False | ‘auto’ | array-like]. \textit{Default = 1.0};
\item $<max\textunderscore~iter>$ \textbf{\textit{, integer, optional field.}}.  The maximum number of iterations. \textit{Default = 300};
\item $<max\textunderscore~n\textunderscore~alphas>$ \textbf{\textit{, integer, optional field.}}. The maximum number of points on the path used to compute the residuals in the cross-validation. \textit{Default = 1000};
\item $<eps>$ \textbf{\textit{, float, optional field.}}.  The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. \textit{Default =2.22e-16};
\end{itemize}

\subparagraph{Linear Model trained with L1 prior as regularizer (aka the Lasso)}
pass
\subparagraph{Lasso linear model with iterative fitting along a regularization path}
pass
\subparagraph{Lasso model fit with Least Angle Regression }
pass
\subparagraph{Cross-validated Lasso, using the LARS algorithm}
pass
\subparagraph{Lasso model fit with Lars using BIC or AIC for model selection}
pass
\subparagraph{Ordinary least squares Linear Regression}
pass
\subparagraph{Logistic Regression }
pass
\subparagraph{Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer }
pass
\subparagraph{Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer}
pass
\subparagraph{Orthogonal Mathching Pursuit model (OMP)}
pass
\subparagraph{Cross-validated Orthogonal Mathching Pursuit model (OMP)}
pass
\subparagraph{Passive Aggressive Classifier}
pass
\subparagraph{Passive Aggressive Regressor}
pass
\subparagraph{Perceptron}
pass
\subparagraph{Randomized Lasso}
pass
\subparagraph{Randomized Logistic Regression}
pass
\subparagraph{Linear least squares with l2 regularization}
pass
\subparagraph{Classifier using Ridge regression}
pass
\subparagraph{Ridge classifier with built-in cross-validation}
pass
\subparagraph{Ridge regression with built-in cross-validation}
pass
\subparagraph{Linear classifiers (SVM, logistic regression, a.o.) with SGD training}
pass
\subparagraph{Linear model fitted by minimizing a regularized empirical loss with SGD}
pass
\subparagraph{Compute Least Angle Regression or Lasso path using LARS algorithm}
pass
\subparagraph{Compute Lasso path with coordinate descent}
pass
\subparagraph{Stabiliy path based on randomized Lasso estimates}
pass
\subparagraph{Gram Orthogonal Matching Pursuit (OMP)}
pass
%%%%% ROM Model - SciKitLearn: Support Vector Machineas %%%%%%%
\paragraph{Support Vector Machines.}
\label{SVM}
The LinearModels' type of algorithms implement generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.
In the following, all the linear models available in RAVEN are reported.
\subparagraph{Linear Support Vector Classifier}
pass
\subparagraph{C-Support Vector Classification}
pass
\subparagraph{Nu-Support Vector Classification}
pass
\subparagraph{Support Vector Regression}
pass




 %%%%% ROM Model - SciKitLearn: MultiClass %%%%%%%
\paragraph{Multi Class.}
\label{Multiclass}
Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.
In the following, all the multi-class models available in RAVEN are reported.
%%%%%%%%%
\subparagraph{One-vs-the-rest (OvR) multiclass/multilabel strategy}
\mbox{}
\\The \textit{One-vs-the-rest (OvR) multiclass/multilabel strategy}, also known as one-vs-all, consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n\_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.
\\In order to use the  \textit{One-vs-the-rest (OvR) multiclass/multilabel} classifier, the user needs to set the sub-node $<SKLtype>multiClass~\vert~OneVsRestClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<estimator>$ \textbf{\textit{, boolean, required field.}}.  An estimator object implementing fit and one of decision\_function or predict\_proba. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{estimatorType}, \textit{required string attribute}, this attribute an other reduced order mode type that needs to be used for the construction of the multi-class algorithms; each sub-sequential node depends on the chosen ROM;
\end{itemize}
\end{itemize}
%%%%%%%%%%%%
\subparagraph{One-vs-one multiclass strategy}
\mbox{}
\\The \textit{One-vs-one multiclass strategy} consists in fitting one classifier per class pair. 
At prediction time, the class which received the most votes is selected. Since it requires to fit n\_classes * (n\_classes - 1) / 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n\_classes\^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which do not scale well with n\_samples. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n\_classes times.
\\In order to use the   \textit{One-vs-one multiclass} classifier, the user needs to set the sub-node $<SKLtype>multiClass~\vert~OneVsOneClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<estimator>$ \textbf{\textit{, boolean, required field.}}.  An estimator object implementing fit and one of decision\_function or predict\_proba. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{estimatorType}, \textit{required string attribute}, this attribute an other reduced order mode type that needs to be used for the construction of the multi-class algorithms; each sub-sequential node depends on the chosen ROM;
\end{itemize}
\end{itemize}
%%%%%%%%%%%%%
\subparagraph{Error-Correcting Output-Code multiclass strategy}
\mbox{}
\\The \textit{Error-Correcting Output-Code multiclass strategy} consists in representing each class with a binary code (an array of 0s and 1s). At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. The main advantage of these strategies is that the number of classifiers used can be controlled by the user, either for compressing the model ($0 < code_size < 1$) or for making the model more robust to errors ($code_size > 1$).
\\In order to use the   \textit{Error-Correcting Output-Code multiclass} classifier, the user needs to set the sub-node $<SKLtype>multiClass~\vert~OutputCodeClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<estimator>$ \textbf{\textit{, xml node, required field.}}.  An estimator object implementing fit and one of decision\_function or predict\_proba.  This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{estimatorType}, \textit{required string attribute}, this attribute an other reduced order mode type that needs to be used for the construction of the multi-class algorithms; each sub-sequential node depends on the chosen ROM;
\end{itemize}
\item $<code_size>$ \textbf{\textit{, float, required field.}}.  ercentage of the number of classes to be used to create the code book. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. A number greater than 1 will require more classifiers than one-vs-the-rest.
\end{itemize}
%%%%%%%%%%%%%
%\subparagraph{fit a one-vs-the-rest strategy}
%pass
%\subparagraph{Make predictions using the one-vs-the-rest strategy}
%pass
%\subparagraph{ Fit a one-vs-one strategy}
%pass
%\subparagraph{Make predictions using the one-vs-one strategy}
%pass
%\subparagraph{Fit an error-correcting output-code strategy}
%pass
%\subparagraph{Make predictions using the error-correcting output-code strategy}
%pass



 %%%%% ROM Model - SciKitLearn: naiveBayes %%%%%%%
\paragraph{Naive Bayes.}
\label{naiveBayes}
Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features. Given a class variable y and a dependent feature vector x\_1 through x\_n, Bayes’ theorem states the following relationship:
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}
                                 {P(x_1, \dots, x_n)}
\end{equation}
Using the naive independence assumption that
\begin{equation}
P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),
\end{equation}
for all i, this relationship is simplified to
\begin{equation}
P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
                                 {P(x_1, \dots, x_n)}
\end{equation}
Since $P(x_1, \dots, x_n)$ is constant given the input, we can use the following classification rule:
\begin{equation}
P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)
\Downarrow
\end{equation}
\begin{equation}
\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),
\end{equation}
and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \mid y)$; the former is then the relative frequency of class $y$ in the training set.
The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \mid y)$.
In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)
Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.
On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict\_proba are not to be taken too seriously.
In the following, all the Naive Bayes available in RAVEN are reported.
%%%%%%%
\subparagraph{Gaussian Naive Bayes}
\mbox{}
\\The \textit{Gaussian Naive Bayes strategy} implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:
\begin{equation}
P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)
\end{equation}
The parameters $\sigma_y$ and $\mu_y$ are estimated using maximum likelihood.
\\In order to use the   \textit{Gaussian Naive Bayes strategy}, the user needs to set the sub-node $<SKLtype>naiveBayes~\vert~GaussianNB</SKLtype>$. No additional XML nodes are needed to be inputted.
%%%%%%%%%%%%
\subparagraph{Multinomial Naive Bayes}
\mbox{}
\\The \textit{Multinomial Naive Bayes} implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\theta_y = (\theta_{y1},\ldots,\theta_{yn})$ for each class $y$, where n is the number of features (in text classification, the size of the vocabulary) and $\theta_{yi}$ is the probability $P(x_i \mid y)$ of feature i appearing in a sample belonging to class y.
The parameters $\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:
\begin{equation}
\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}
\end{equation}
where $N_{yi} = \sum_{x \in T} x_i$ is the number of times feature i appears in a sample of class y in the training set T, and $N_{y} = \sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class y.
The smoothing priors $\alpha \ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\alpha = 1$ is called Laplace smoothing, while $\alpha < 1$ is called Lidstone smoothing.
\\In order to use the   \textit{Multinomial Naive Bayes} strategy, the user needs to set the sub-node $<SKLtype>naiveBayes~\vert~MultinomialNB</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<alpha>$ \textbf{\textit{, float, optional field.}}.  Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). \textit{Default = 1.0};
\item $<fit\textunderscore~prior>$ \textbf{\textit{, boolean, required field.}}.  Whether to learn class prior probabilities or not. If false, a uniform prior will be used. \textit{Default = False};
\item $<class\textunderscore~prior>$ \textbf{\textit{, array-like float (n\_classes), optional field.}}.  Prior probabilities of the classes. If specified the priors are not adjusted according to the data. \textit{Default = None}.
\end{itemize}

%%%%%%%%%%%% 
\subparagraph{Bernoulli Naive Bayes}
\mbox{}
\\The \textit{Bernoulli Naive Bayes} implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a  \textit{Bernoulli Naive Bayes} instance may binarize its input (depending on the binarize parameter).
The decision rule for Bernoulli naive Bayes is based on
\begin{equation}
P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)
\end{equation}
which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature i that is an indicator for class y, where the multinomial variant would simply ignore a non-occurring feature.
In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier.  \textit{Bernoulli Naive Bayes} might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.
\\In order to use the   \textit{Bernoulli Naive Bayes} strategy, the user needs to set the sub-node $<SKLtype>naiveBayes~\vert~BernoulliNB</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<alpha>$ \textbf{\textit{, float, optional field.}}.  Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). \textit{Default = 1.0};
\item $<binarize>$ \textbf{\textit{, float, optional field.}}.  Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.. \textit{Default = None};
\item $<fit_prior>$ \textbf{\textit{, boolean, optional field.}}.  Whether to learn class prior probabilities or not. If false, a uniform prior will be used. \textit{Default = False};
\item $<class_prior>$ \textbf{\textit{, array-like float (n\_classes), optional field.}}.  Prior probabilities of the classes. If specified the priors are not adjusted according to the data. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Neighbors %%%%%%%
\paragraph{Neighbors.}
\label{Neighbors}
The \textit{Neighbors} class provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: classification for data with discrete labels, and regression for data with continuous labels.
The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree.).
\\In the following, all the Neighbors' models available in RAVEN are reported.
%%%%%%%%%%%%%%%
\subparagraph{Nearest Neighbors}
\mbox{}
\\The \textit{Nearest Neighbors} implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm. 
%The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute']. When the default value 'auto' is passed, the algorithm attempts to determine the best approach from the training data. 
\\In order to use the   \textit{Nearest Neighbors} strategy, the user needs to set the sub-node $<SKLtype>neighbors~\vert~NearestNeighbors</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\textunderscore~tree} will use BallTree;
\item \textit{kd\textunderscore~tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Classifier }
\mbox{}
\\The \textit{K Neighbors Classifier} is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. It implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.
\\In order to use the   \textit{K Neighbors Classifier}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~KNeighborsClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};

\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Classifier}
\mbox{}
\\The \textit{Radius Neighbors Classifier} is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. It implements learning based on the number of neighbors within a fixed radius r of each training point, where r is a floating-point value specified by the user.
\\In order to use the   \textit{Radius Neighbors Classifier}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~RadiusNeighbors</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};
\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2};
\item $<outlier\textunderscore~label>$ \textbf{\textit{, integer, optional field.}}.  Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{K Neighbors Regressor}
\mbox{}
\\The \textit{K Neighbors Regressor}  can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based the mean of the labels of its nearest neighbors. It implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.
\\In order to use the   \textit{K Neighbors Regressor}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~KNeighborsRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n_neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};

\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Radius Neighbors Regressor}
\mbox{}
\\The \textit{Radius Neighbors Regressor}  can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based the mean of the labels of its nearest neighbors. It implements learning based on the neighbors within a fixed radius r of the query point, where r is a floating-point value specified by the user.
\\In order to use the   \textit{Radius Neighbors Regressor}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~RadiusNeighborsRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, integer, optional field.}}.  Number of neighbors to use by default for k\_neighbors queries.. \textit{Default = 5};
\item $<weights>$ \textbf{\textit{, string, optional field.}}. Weight function used in prediction. Possible values:
\begin{itemize}
\item \textit{uniform} : uniform weights. All points in each neighborhood are weighted equally;
\item \textit{distance} : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
\end{itemize}
\textit{Default = uniform};
\item $<radius>$ \textbf{\textit{, float, optional field.}}.  Range of parameter space to use by default for :meth`radius\_neighbors` queries. \textit{Default = 1.0};
\item $<algorithm>$ \textbf{\textit{, string, optional field.}}.  Algorithm used to compute the nearest neighbors:
\begin{itemize}
\item \textit{ball\_tree} will use BallTree;
\item \textit{kd\_tree} will use KDtree;
\item \textit{brute} will use a brute-force search;
\item \textit{auto} will attempt to decide the most appropriate algorithm based on the values passed to fit method.
\end{itemize}
NB. fitting on sparse input will override the setting of this parameter, using brute force. \textit{Default = auto};
\item $<metric>$ \textbf{\textit{, string, optional field.}}.  the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \textit{Default = minkowski};
\item $<leaf\textunderscore~size>$ \textbf{\textit{, integer, optional field.}}.  Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. \textit{Default = 30};
\item $<p>$ \textbf{\textit{, integer, optional field.}}.  Parameter for the Minkowski metric. When $p = 1$, this is equivalent to using manhattan distance (L1), and euclidean distance (L2) for $p = 2$. For arbitrary p, minkowski distance (L\_p) is used. \textit{Default = 2};
\item $<outlier\textunderscore~label>$ \textbf{\textit{, integer, optional field.}}.  Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%
\subparagraph{Nearest Centroid Classifier}
\mbox{}
\\The \textit{Nearest Centroid classifier} is a simple algorithm that represents each class by the centroid of its members. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed.
\\In order to use the   \textit{Nearest Centroid Classifier}, the user needs to set the sub-node $<SKLtype>neighbors~\vert~NearestCentroid</SKLtype>$. In addition to this XML node, another might be inputted:
\begin{itemize}
\item $<n\textunderscore~neighbors>$ \textbf{\textit{, float, optional field.}}.  Threshold for shrinking centroids to remove features. \textit{Default = None}.
\end{itemize}
%\subparagraph{Ball Tree}
%pass
%\subparagraph{K-D Tree}
%pass

 %%%%% ROM Model - SciKitLearn: Quadratic Discriminant Analysis %%%%%%%
\paragraph{Quadratic Discriminant Analysis.}
\label{QDA}
The \textit{Quadratic Discriminant Analysis} is a classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.The model fits a Gaussian density to each class.
\\In order to use the   \textit{Quadratic Discriminant Analysis Classifier}, the user needs to set the sub-node $<SKLtype>qda~\vert~QDA</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<priors>$ \textbf{\textit{, array-like (n\_classes), optional field.}}.  Priors on classes. \textit{Default = None};
\item $<reg\textunderscore~param>$ \textbf{\textit{, float, optional field.}}. Regularizes the covariance estimate as (1-reg\_param)*Sigma + reg\_param*Identity(n\_features). \textit{Default = 0.0}.
\end{itemize}

 %%%%% ROM Model - SciKitLearn: Tree %%%%%%%
\paragraph{Tree.}
\label{tree}
Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
\begin{itemize}
\item Some advantages of decision trees are:
\item  Simple to understand and to interpret. Trees can be visualized.
\item Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.
\item The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
\item  Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. 
\item Able to handle multi-output problems.
\item  Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.
\item Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
\item Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.
\end{itemize}
The disadvantages of decision trees include:
\begin{itemize}
\item Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.
\item Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.
\item The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.
\item There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.
\item Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.
\end{itemize}
In the following, all the linear models available in RAVEN are reported.
%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Classifier}
\mbox{}
\\The \textit{Decision Tree Classifier} is a classifier that is based on the decision tree logic.
\\In order to use the \textit{Decision Tree Classifier}, the user needs to set the sub-node $<SKLtype>tree~\vert~DecisionTreeClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%
\subparagraph{Decision Tree Regressor}
\mbox{}
\\The \textit{Decision Tree Regressor} is a Regressor that is based on the decision tree logic.
\\In order to use the \textit{Decision Tree Regressor}, the user needs to set the sub-node $<SKLtype>tree~\vert~DecisionTreeRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%
\subparagraph{Extra Tree Classifier}
\mbox{}
\\The \textit{Extra Tree Classifier} is an extremely randomized tree classifier.
Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max\_features randomly selected features and the best split among those is chosen. When max\_features is set 1, this amounts to building a totally random decision tree.
\\In order to use the \textit{Extra Tree Classifier}, the user needs to set the sub-node $<SKLtype>tree~\vert~ExtraTreeClassifier</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:

\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%
\subparagraph{Extra Tree Regressor}
\mbox{}
\\The \textit{Extra Tree Regressor} is an extremely randomized tree regressor.
Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max\_features randomly selected features and the best split among those is chosen. When max\_features is set 1, this amounts to building a totally random decision tree.
\\In order to use the \textit{Extra Tree Regressor}, the user needs to set the sub-node $<SKLtype>tree~\vert~ExtraTreeRegressor</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<criterion>$ \textbf{\textit{, string, optional field.}}.  The function to measure the quality of a split. Supported criteria are ``gini'' for the Gini impurity and ``entropy'' for the information gain. \textit{Default = gini};
\item $<splitter>$ \textbf{\textit{, string, optional field.}}. The strategy used to choose the split at each node. Supported strategies are ``best'' to choose the best split and ``random'' to choose the best random split. \textit{Default = best};
\item $<max\textunderscore~features>$ \textbf{\textit{, int, float or string, optional field.}}. The number of features to consider when looking for the best split:
\begin{itemize}
\item If int, then consider max\_features features at each split.
\item If float, then max\_features  is a percentage and int(max\_features * n\_features) features are considered at each split.
\item If “auto”, then max\_features=sqrt(n\_features);
\item If “sqrt”, then max\_features=sqrt(n\_features);
\item If “log2”, then max\_features=log2(n\_features);
\item If None, then max\_features=n\_features.
\end{itemize}
NB. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max\_features features. 
\\\textit{Default = None};
\item $<max\textunderscore~depth>$ \textbf{\textit{, integer, optional field.}}. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples. Ignored if max\_samples\_leaf is not None. \textit{Default = None};
\item $<min\textunderscore~samples\textunderscore~split>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to split an internal node. \textit{Default = 2};
\item $<min\textunderscore~samples\textunderscore~leaf>$ \textbf{\textit{, integer, optional field.}}. The minimum number of samples required to be at a leaf node. \textit{Default = 1};
\item $<max\textunderscore~leaf\textunderscore~nodes>$ \textbf{\textit{, integer, optional field.}}. Grow a tree with max\_leaf\_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. If not None then max\_depth will be ignored. \textit{Default = None}.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%% ROM Model - SciKitLearn: Gaussian Process %%%%%%%
\paragraph{Gaussian Process.}
\label{GP}
Gaussian Processes for Machine Learning (GPML) is a generic supervised learning method primarily designed to solve regression problems.
The advantages of Gaussian Processes for Machine Learning are:
\begin{itemize}
\item The prediction interpolates the observations (at least for regular correlation models);
\item The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and exceedance probabilities that might be used to refit (online fitting, adaptive fitting) the prediction in some region of interest;
\item Versatile: different linear regression models and correlation models can be specified. Common models are provided, but it is also possible to specify custom models provided they are stationary.
\end{itemize}
The disadvantages of Gaussian Processes for Machine Learning include:
\begin{itemize}
\item It is not sparse. It uses the whole samples/features information to perform the prediction;
\item It loses efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens. It might indeed give poor performance and it loses computational efficiency;
\item Classification is only a post-processing, meaning that one first need to solve a regression problem by providing the complete scalar float precision output y of the experiment one attempt to model.
\end{itemize}
In order to use the \textit{Gaussian Process Regressor}, the user needs to set the sub-node $<SKLtype>GaussianProcess~\vert~GaussianProcess</SKLtype>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<regr>$ \textbf{\textit{, string, optional field.}}.  A regression function returning an array of outputs of the linear regression functional basis. The number of observations n\_samples should be greater than the size p of this basis. Available built-in regression models are:
'constant', 'linear', 'quadratic'. \textit{Default = constant};
\item $<corr>$ \textbf{\textit{, string, optional field.}}. A stationary autocorrelation function returning the autocorrelation between two points x and x’. Default assumes a squared-exponential autocorrelation model. Built-in correlation models are:
'absolute\_exponential', 'squared\_exponential','generalized\_exponential', 'cubic', 'linear'. \textit{Default = squared\_exponential};
\item $<beta0>$ \textbf{\textit{, float, array-like, optional field.}}. The regression weight vector to perform Ordinary Kriging (OK). \textit{Default = Universal Kriging};
\item $<storage\textunderscore~mode>$ \textbf{\textit{, string, optional field.}}. A string specifying whether the Cholesky decomposition of the correlation matrix should be stored in the class (storage\_mode = ‘full’) or not (storage\_mode = ‘light’). \textit{Default = full};
\item $<verbose>$ \textbf{\textit{, boolean, optional field.}}.  boolean specifying the verbose level. \textit{Default = False};
\item $<theta0>$ \textbf{\textit{, float, array-like, optional field.}}.  An array with shape (n\_features, ) or (1, ). The parameters in the autocorrelation model. If thetaL and thetaU are also specified, theta0 is considered as the starting point for the maximum likelihood estimation of the best set of parameters. \textit{Default = [1e-1]};
\item $<thetaL>$ \textbf{\textit{, float, array-like, optional field.}}. An array with shape matching theta0’s. Lower bound on the autocorrelation parameters for maximum likelihood estimation. \textit{Default = None};
\item $<thetaU>$ \textbf{\textit{, float, array-like, optional field.}}. An array with shape matching theta0’s. Upper bound on the autocorrelation parameters for maximum likelihood estimation. \textit{Default = None};
\item $<normalize>$ \textbf{\textit{, boolean, optional field.}}.  Input X and observations y are centered and reduced wrt means and standard deviations estimated from the n\_samples observations provided. \textit{Default = True};
\item $<nugget>$ \textbf{\textit{, float, optional field.}}.  Introduce a nugget effect to allow smooth predictions from noisy data.The nugget is added to the diagonal of the assumed training covariance; in this way it acts as a Tikhonov regularization in the problem. In the special case of the squared exponential correlation function, the nugget mathematically represents the variance of the input values. \textit{Default = 10. * MACHINE\_EPSILON};
\item $<optimizer>$ \textbf{\textit{, string, optional field.}}.  A string specifying the optimization algorithm to be used. Available optimizers are: 'fmin\_cobyla', 'Welch'. \textit{Default = fmin\_cobyla};
\item $<random\textunderscore~start>$ \textbf{\textit{, integer, optional field.}}. The number of times the Maximum Likelihood Estimation should be performed from a random starting point. The first MLE always uses the specified starting point (theta0), the next starting points are picked at random according to an exponential distribution (log-uniform on [thetaL, thetaU]). \textit{Default = 1};
\item $<random\textunderscore~state>$ \textbf{\textit{, integer, optional field.}}. Seed of the internal random number generator. \textit{Default = random}.
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
------------------------------------------------------------
<Simulation>
  ...
  <Models>
    ...
   <ROM name='***' subType='SciKitLearn'>
     <Features>***,***</Features>
     <SKLtype>linear_model|LinearRegression</SKLtype>
     <Target>***</Target>
     <fit_intercept>***</fit_intercept>
     <normalize>***</normalize>
   </ROM>
    ...
  </Models>
  ...
</Simulation>
------------------------------------------------------------
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  External  Model   %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{External Model}
\label{subsec:models_externalModel}
As the name suggests, an external model  is an entity that is embedded in the RAVEN code at run time. This object allows the user to create a python module that is going to be treated as a predefined internal model object. 
In other words, the \textbf{External Model} is going to be treated by RAVEN as a normal external Code (e.g. it is going to be called in order to compute a whatever quantity, based on a whatever input).
\\The specifications of an External Model must be defined within the XML block $<ExternalModel>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this External Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (XML);
\item \textbf{subType}, \textit{required string attribute}, This string must be kept empty;
\item \textbf{ModuleToLoad}, \textit{required string attribute}, file name with its absolute or relative path. NB. If a relative path is specified, it must be noticed it is relative with respect to where the user runs the code.
\end{itemize}
\vspace{-5mm}
In order to make the RAVEN code aware of the variables the user is going to manipulate/use in its own python Module, the variables need to be specified in the \textbf{$<ExternalModel>$} input block. The user needs to input, within this block, only the variables that RAVEN needs to be aware of (i.e. the variables are going to directly be used by the code) and not the local variables that the user does not want to, for example, store in a RAVEN internal object. These variables are inputted within consecutive XML blocks called $<variable>$:
\begin{itemize}
\item $<variable>$, string, required parameter. In the body of this XML node, the user needs to specify the name of the variable. This variable needs to match a variable used/defined in the external python model.
\end{itemize}
When the external function variables are defined, at run time, RAVEN initialize those and take track of their values during the simulation. Each variable defined in the $<ExternalModel>$ block is available in the module (each method implemented) as a python ``self''. 
\\ In the External Python module, the user can implement all the methods that are needed for the functionality of the model, but only the following methods are going, if present, called by the framework:
\begin{itemize}
\item \textit{\textbf{def \textunderscore~readMoreXML}, OPTIONAL METHOD}, This method can be implemented by the user if the XML input that belongs to this External Model needs to be extended to contain other information. The information read needs to be stored in ``self'' in order to be available to all the other methods (e.g. if the user needs to add a couple of newer XML nodes with information needed by the algorithm implemented in the ``run'' method);
\item \textit{\textbf{def initialize}, OPTIONAL METHOD}, Initialization method. In this function the user can implement all the actions need to be performed at the initialization stage;
\item \textit{\textbf{def createNewInput}, OPTIONAL METHOD}, Method to create a new input with the information coming from the RAVEN framework. In this function the user can retrieve the information coming from the RAVEN framework, during the employment of a calculation flow, and use them to construct a new input that is going to be transferred to the ``run'' method;
\item \textit{\textbf{def run}, REQUIRED METHOD}, This is the actual location where the user needs to implement the model action (e.g. resolution of a set of equations, etc.). This function is going to receive the Input(or Inputs) generated either by  the External Model ``createNewInput'' method or the internal RAVEN one;
\end{itemize}
In the following sub-sections, all the methods are going to be analyzed in detail.
\subsubsection{Method: def \textunderscore~readMoreXML.}
\label{subsubsec:externalReadMoreXML}
As already mentioned, the \textbf{readMoreXML} method can be implemented by the user if the XML input that belongs to this External Model needs to be extended to contain other information. The information read needs to be stored in ``self'' in order to be available to all the other methods (e.g. if the user needs to add a couple of newer XML nodes with information needed by the algorithm implemented in the ``run'' method).
If this method is implemented in the \textbf{External Model}, RAVEN is going to call it when the node $<ExternalModel>$ is found parsing XML input file. The method receives from RAVEN an attribute of type ``xml.etree.ElementTree'', containing all the sub-nodes and attribute of the XML block $<ExternalModel>$.
\\In the following an example is reported:
\begin{lstlisting}[style=XML]
------------------------------------
<Simulation> 
  ...
  <Models>
     ...
    <ExternalModel name='AnExtModule' 
                              subType='' 
                              ModuleToLoad='path_to_external_module'>  
       <variable>sigma</variable>
       <variable>rho</variable>
       <variable>outcome</variable>
       <!-- 
          here we define other nodes RAVEN does not read automatically 
          => We need to implement, in the external module ``AnExtModule''
          the readMoreXML method
        -->
        <newNodeWeNeedToRead>
            whatNeedsToBeRead
        </newNodeWeNeedToRead>
    </ExternalModel>
     ...
  </Models> 
  ...
</Simulation> 
\end{lstlisting}
\begin{lstlisting}[language=python]
------------------------------------
def _readMoreXML(self,xmlNode):
  # the xmlNode is passed in by RAVEN framework
  # <newNodeWeNeedToRead> is unknown (in the RAVEN framework)
  # we have to read it on our own
  # get the node
  ourNode = xmlNode.find(``newNodeWeNeedToRead'')
  # get the information in the node
  self.ourNewVariable = ourNode.text
  # end function
------------------------------------
\end{lstlisting}
\subsubsection{Method: def initialize.}
\label{subsubsec:externalInitialize}
The \textbf{initialize} method can be implemented in the  \textbf{External Model} in order to initialize some variables needed by it. For example, it can be used to compute a quantity needed by the ``run'' method before performing the actual calculation).
If this method is implemented in the \textbf{External Model}, RAVEN is going to call it at the initialization stage of each ``Step'' (see section \ref{sec:steps}. RAVEN will communicate, thorough a set of method attributes, all the information that are generally needed to perform a initialization:
\begin{itemize}
\item runInfo, a dictionary containing information regarding how the calculation is set up (e.g. number of processors, etc.). It contains the following attributes:
    \begin{itemize}
       \item \textit{DefaultInputFile},  Default input file to use
       \item \textit{SimulationFiles}, the xml input file
       \item \textit{ScriptDir}, the location of the pbs script interfaces
       \item \textit{FrameworkDir}, the directory where the framework is located
       \item \textit{WorkingDir}, the directory where the framework should be running
       \item \textit{TempWorkingDir}, the temporary directory where a simulation step is run
       \item \textit{NumMPI}, the number of mpi process by run
       \item \textit{NumThreads}, Number of Threads by run
       \item \textit{numProcByRun}, Total number of core used by one run (number of threads by number of mpi)
       \item \textit{batchSize}, number of contemporaneous runs
       \item \textit{ParallelCommand}, the command that should be used to submit jobs in parallel (mpi)
       \item \textit{numNode},  number of nodes
       \item \textit{procByNode},  number of processors by node
       \item \textit{totalNumCoresUsed}, total number of cores used by driver
       \item \textit{quequingSoftware}, quequing software name
       \item \textit{stepName}, the name of the step currently running
       \item \textit{precommand}, Add to the front of the command that is run
       \item \textit{postcommand}, Added after the command that is run
       \item \textit{delSucLogFiles}, If a simulation (code run) has not failed, delete the relative log file (if True) 
       \item \textit{deleteOutExtension}, If a simulation (code run) has not failed, delete the relative output files with the listed extension (comma separated list, for example: 'e,r,txt')
       \item \textit{mode},  Running mode.  Curently the only modes supported are pbsdsh and mpi
       \item \textit{expectedTime}, How long the complete input is expected to run
       \item \textit{logfileBuffer}, logfile buffer size in bytes
    \end{itemize}
\item inputs, a list of all the inputs that have been specified in the ``Step'' is using this model.
\end{itemize}
In the following an example is reported:
\begin{lstlisting}[language=python]
------------------------------------
def initialize(self,runInfo,inputs):
 # Let's suppose we just need to initialize some variables
  self.sigma = 10.0
  self.rho   = 28.0
  # end function
------------------------------------
\end{lstlisting}
\subsubsection{Method: def createNewInput.}
\label{subsubsec:externalcreateNewInput}
The \textbf{createNewInput} method can be implemented by the user to create a new input with the information coming from the RAVEN framework. In this function the user can retrieve the information coming from the RAVEN framework, during the employment of a calculation flow, and use them to construct a new input that is going to be transferred to the ``run'' method.
The new input created needs to be returned to RAVEN (i.e. ``return NewInput'').
RAVEN communicates, thorough a set of method attributes, all the information that are generally needed to create a new input:
myInput,samplerType,**Kwargs
\begin{itemize}
\item inputs, python list, a list of all the inputs that have been defined in the ``Step'' is using this model;
\item samplerType, string, the type of Sampler, if a Sampling strategy is employed; None otherwise;
\item Kwargs, dictionary, a dictionary containing several informations (that can change based on the ``Step'' type). If a Sampling strategy is employed, this dictionary contains another one identified by the keyword ``SampledVars'', in which the variables perturbed by the sampler are reported;
\end{itemize}
NB. If the ``Step'' that is using this Model has as input(s) an object of main class type ``Datas'' (see section \ref{datas}), the internal ``createNewInput'' method is going to convert it in a dictionary of values.
\\In the following an example is reported:
\begin{lstlisting}[language=python]
------------------------------------
def createNewInput(self,inputs,samplerType,**Kwargs):
  # in here the actual createNewInput of the 
  # model is implemented
  if samplerType == ``MonteCarlo'':
    avariable = inputs[``something'']*inputs[``something2'']
  else:
    avariable = inputs[``something'']/inputs[``something2'']
  return avariable*Kwargs[``SampledVars''][``aSampledVar'']
------------------------------------
\end{lstlisting} 
\subsubsection{Method: def run.}
\label{subsubsec:externalRun}
As stated previously, the only method the MUST be present in an External Module is the \textbf{run} function. In this function , the user needs to implement the algorithm that RAVEN has to execute. The  \textbf{run} method is generally called after having inquired the ``createNewInput'' method (either the internal  or the user-implemented one). The only attribute this method is going to receive by is a Python list of inputs (the inputs coming from the ``createNewInput'' method).
If the user wants RAVEN to collect the results of this method, the outcomes of interest need to be stored in ``self''. NB. RAVEN is trying to collect the values of the variables listed in the XML  block $<ExternalModel>$ only.
\\In the following an example is reported:

\begin{lstlisting}[language=python]
------------------------------------
def run(self,Input):
  # in here the actual run of the 
  # model is implemented
  input = Input[0]
  self.outcome = self.sigma*self.rho*input[``whatEver'']
------------------------------------
\end{lstlisting} 

%\subsection{Projector}
%\label{sec:models_projector}
%
%Description

%Summary

%Example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  External  PostProcessor   %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PostProcessor}
\label{sec:models_postProcessor}
A Post-Processor (PP) can be considered as an Action performed on a set of data or other type of objects. Most of the post-processors contained in RAVEN, employ a mathematical operation on the data given as ``input''/
RAVEN supports several different types of PPs. Currently in RAVEN the following Post-Processors are available:
\begin{itemize}
   \item \textbf{BasicStatistics;}
   \item \textbf{ComparisonStatistics;}
   \item \textbf{SafestPoint;}
   \item \textbf{LimitSurface;}
   \item \textbf{PrintCSV;}
   \item \textbf{LoadCsvIntoInternalObject.}
\end{itemize}
The specifications of this Model must be defined within the XML block $<PostProcessor>$. This XML node needs to contain the attributes:
\vspace{-5mm}
\begin{itemize}
\itemsep0em
\item \textbf{name}, \textit{required string attribute}, user-defined name of this Model. N.B. As for the other objects, this is the name that can be used to refer to this specific entity from other input blocks (xml);
\item \textbf{subType}, \textit{required string attribute}, in this attribute the user defines which of the post-processors needs to be used, choosing among the previously reported types. Obviously, this choice conditions the subsequent the required and/or optional $<PostProcessor>$ sub nodes.
\end{itemize}
\vspace{-5mm}
As already mentioned, all the types and meaning of the remaining sub-nodes depend on the post-processor type specified in the attribute \textit{subType}. In the following sections the specifications of each type are reported.
%%%%% PP BasicStatistics %%%%%%%
\paragraph{BasicStatistics.}
\label{BasicStatistics}
The \textbf{BasicStatistics} post-processor is the container of the algorithms to compute all the most important statistical quantities. 
\\In order to use the   \textit{BasicStatistics} post-processor, the user needs to set the sub-node $<subType>BasicStatistics</subType>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<what>$ \textbf{\textit{, comma separated string, required field.}}.  List of quantities need to be computed. Currently the quantities available are:
\begin{itemize}
   \item \textbf{covariance matrix,} Covariance matrix;
   \item \textbf{NormalizedSensitivity,} Matrix of normalized sensitivity coefficients;
   \item \textbf{sensitivity,} Matrix of sensitivity coefficients;
   \item \textbf{pearson,} Matrix of sensitivity coefficients;
   \item \textbf{expectedValue}, Expected value;
   \item \textbf{sigma,} Standard deviation;
   \item \textbf{variationCoefficient,} Coefficient of variation (sigma/expected value);
   \item \textbf{variance,} Variance;
   \item \textbf{skewness,} Skewness;
   \item \textbf{kurtois,} Kurtois;
   \item \textbf{median,} Data median;
   \item \textbf{percentile,} 95 percentile.
\end{itemize}
If all the quantities need to be computed, the user can input in the body of $<what>$ the string ``all''.
\item $<parameters>$ \textbf{\textit{, comma separated string, required field.}}. List of the parameters on which the previous operations need to be applied on (e.g., massFlow, Temperature);
\item $<methodsToRun>$ \textbf{\textit{, comma separated string, optional field.}}.  The method names of an external Function that need to be run before computing any of the predefined quantities If this XML node is inputted, the $<Assembler>$ node must be present. \textit{Default = None};
\item $<Assembler>$\textbf{\textit{, xml node, required field.}} This xml node contains a ``list'' of objects that are  optional for the functionality of the  \textbf{BasicStatistics} post-processor. The objects must be listed with a rigorous syntax that, except for the xml node tag, is common among all the objects.  
Each of these sub-nodes  must contain 2 attributes that are used to map those within the simulation framework:
   \begin{itemize}
     \item \textbf{class}, \textit{required string attribute}, it is the main ``class'' the listed object is from;
     \item \textbf{type},  \textit{required string attribute}, it is the object identifier or sub-type.
    \end{itemize}
The  \textbf{BasicStatistics} post-processor approach  optionally accepts the following object type:
   \begin{itemize}
       \item $<Function>$\textbf{\textit{, string, required field.}}.The body of this xml block needs to contain the name of an External Function defined within the $<Functions>$ main block (see section \ref{sec:functions}). This object needs to containt the methods listed in the node $<methodsToRun>$.;
    \end{itemize}
\end{itemize}
\textbf{Example:}
----------------------------------------------
\begin{lstlisting}[style=XML]
<Simulation>
   ...
   <Models>
      ...
      <PostProcessor name="***" subType='BasicStatistics'  debug='true'>
          <!-- => you can here specify what type of figure of merit
              you need to compute
             <what>expectedValue,sigma,variance,kurtois,pearson,covariance</what> 
           -->
          <what>expectedValue</what>
          <parameters>x01,x02</parameters>
          <methodsToRun>failureProbability</methodsToRun>
      </PostProcessor>
      ...
   <Models>
   ...
<Simulation>
----------------------------------------------
\end{lstlisting}

%%%%% PP ComparisonStatistics %%%%%%%
\paragraph{ComparisonStatistics.}
\label{ComparisonStatistics}
To be finalized.

%%%%% PP SafestPoint %%%%%%%
\paragraph{SafestPoint.}
\label{SafestPoint}
The \textbf{SafestPoint} post-processor provides the coordinates of the farthest point from the limit surface that is given as an input.
The safest point coordinates are expected values of the coordinates of the farthest points from the limit surface in the space of the ''controllable'' variables based on the probability distributions of the ''non-controllable'' variables.
\\The term ''controllable'' identifies those variables that are under control during the system operation, while the ''non-controllable'' variables are stochastic parameters affecting the system behaviour randomly. 
\\The ''SafestPoint'' post-processor requires the set of points belonging to the limit surface, which must be given as an input. The ''Assembler'' subsection requires the probability distributions of both ''controllable'' and ''non-controllable'' variables.
\\The sampling method used by the ''SafestPoint'' is a ''value'' or ''CDF'' grid. At present only the ''equal'' grid type is available.
\begin{itemize}
	\item $<Assembler>$: the probability distributions of the ''controllable'' and ''non-controllable'' variables are required.
	\item $<controllable>$: the list of the controllable variables is given here. Each variable is associated with its name and the two items below:
		\begin{itemize}
			\item $<distribution>$: the name of the probability distribution associated with the controllable variable is specified here.
			\item $<grid>$: type, number of steps and tolerance of the sampling grid are defined here.
		\end{itemize}
	\item $<non-controllable>$: the list of the non-controllable variables is given here. Each variable is associated with its name and the two items below:
		\begin{itemize}
			\item $<distribution>$: the name of the probability distribution associated with the non-controllable variable is specified here.
			\item $<grid>$: type, number of steps and tolerance of the sampling grid are defined here.
		\end{itemize}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
----------------------------------------------
<Simulation>
   ...
   <Models>
      ...
  <PostProcessor name='SP' subType='SafestPoint'>
      <Assembler>
        <Distribution  class = 'Distributions'  type = 'Normal'>x1_dst</Distribution>
        <Distribution  class = 'Distributions'  type = 'Normal'>x2_dst</Distribution>
        <Distribution  class = 'Distributions'  type = 'Normal'>gammay_dst</Distribution>
      </Assembler>
      <controllable>
        <variable name = 'x1'>
          <distribution>x1_dst</distribution>
          <grid type = 'value' steps = '20'>1</grid>   		
        </variable>
        <variable name = 'x2'>
          <distribution>x2_dst</distribution>
          <grid type = 'value' steps = '20'>1</grid>   		
        </variable>
        </controllable>
        <non-controllable>
          <variable name = 'gammay'>
            <distribution>gammay_dst</distribution>
            <grid type = 'value' steps = '20'>2</grid>
          </variable> 	
        </non-controllable>
    </PostProcessor>
      ...
   </Models>
   ...
</Simulation>
----------------------------------------------
\end{lstlisting}
%%%%% PP LimitSurface %%%%%%%
\paragraph{LimitSurface.}
\label{LimitSurface}
The \textbf{LimitSurface} post-processor is aimed to identify the transition zones that determine a change in the status of the system (Limit Surface). 
\\In order to use the   \textit{LimitSurface}, the user needs to set the sub-node $<subType>LimitSurface</subType>$. In addition to this XML node, several others need (or not) to be inputted:
\begin{itemize}
\item $<parameters>$ \textbf{\textit{, comma separated string, required field.}}. List of the parameters that define the uncertain domain and from which the LS needs to be computed;
\item $<tollerance>$ \textbf{\textit{, float, optional field.}}. Absolute value converge tollerance. This value defines the coarsens  of the evaluation grid. \textit{Default= 1.e-4};
 \item $<Assembler>$\textbf{\textit{, xml node, required field.}} This xml node contains a ``list'' of objects that are required (or optional) for the functionality of the Adaptive Sampler. The objects must be listed with a rigorous syntax that, except for the xml node tag, is common among all the objects.  
Each of these sub-nodes  must contain 2 attributes that are used to map those within the simulation framework:
   \begin{itemize}
     \item \textbf{class}, \textit{required string attribute}, it is the main ``class'' the listed object is from. For example, it can be ``Models'', ``Functions'', etc;
     \item \textbf{type},  \textit{required string attribute}, it is the object identifier or sub-type. For example, it can be ``ROM'', ``External'', etc.
    \end{itemize}
   The \textbf{LimitSurface} post-processor requires or optionally accepts the following objects' types:
   \begin{itemize}
     \item $<ROM>$\textbf{\textit{, string, optional  field.}}. If inputted, the body of this xml node must contain the name of a ROM defined in the $<Models>$ block (see section \ref{subsec:models_ROM});
       \item $<Function>$\textbf{\textit{, string, required field.}}.The body of this xml block needs to contain the name of an External Function defined within the $<Functions>$ main block (see section \ref{sec:functions}). This object represents the boolean function that defines the transition boundaries. This function must implement a method called \textit{\_\_residuumSign(self)}, that returns either -1 or 1, depending on the system conditions (see section \ref{sec:functions}.
    \end{itemize}
\end{itemize}

\textbf{Example:}
\begin{lstlisting}[style=XML]
----------------------------------------------
<Simulation>
   ...
   <Models>
      ...
    <PostProcessor name="computeLimitSurface" subType='LimitSurface' debug='True'>
        <what>all</what>
        <parameters>x0,y0</parameters>
        <parameters>x0,y0</parameters>
        <Assembler>
            <ROM      class='Models'           type='ROM'             >Acc</ROM> 
            <!--You can add here a ROM defined in Models block.If not Present, 
                    a nearest algorithm is going to be used
              -->
            <Function class='Functions'        type='External'        >goalFunctionForLimitSurface</Function>
        </Assembler>
    </PostProcessor>
      ...
   </Models>
   ...
</Simulation>
----------------------------------------------
\end{lstlisting}
%%%%% PP PrintCSV %%%%%%%
\paragraph{PrintCSV.}
\label{PrintCSV}
TO BE MOVED TO STEP ``IOSTEP''
%%%%% PP LoadCsvIntoInternalObject %%%%%%%
\paragraph{LoadCsvIntoInternalObject.}
\label{LoadCsvIntoInternalObject}
TO BE MOVED TO STEP ``IOSTEP''