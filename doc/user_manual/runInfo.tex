\section{RunInfo  \\ \vspace{2 mm} {\small }}
The $RunInfo$ block is the place where the user specifiy how the calculation needs to be performed. In this input block, several settings can be inputted, in order to define how to drive the calculation and set up, when needed, particular settings for the machine the code needs to run on (queue system if not PBS, etc.).
In the following, all the keywords are explained in detail.
\begin{itemize}
\item $<WorkingDir>$, in this block the user needs to specify the absolute or relative (with respect to the location where RAVEN is run from) path to a directory that is going to be used to store all the results of the calculations and where RAVEN looks for the files specified in the block $<Files>$;
%\item $<ParallelCommand>$, in RAVEN the default command for running in parallel is ``mpiexec''. If the system does use a different protocol, the user can specify the command needs to be used in this section;
%\item $<ThreadingCommand>$, in RAVEN the default command for running codes is ``mpiexec''. If the system does use a different protocol, the user can specify the command needs to be used in this section;The command should be used to submit multi-threaded jobs?????
\item $<quequingSoftware>$, RAVEN has support for PBS quequing system. If the platform provides a different quequing system, the user can specify its name here (e.g., PBS PROFESSIONAL, etc.);
\item $<NumThreads>$, this section can be used to specify the number of threads RAVEN should associate when running the driven software. For example, if RAVEN is driving a code named "FOO", and this code has multi-threading support, in here the user specify how many threads each instance of FOO should use (e.g. FOO --n-threads=$NumThreads$);
\item $<NumNode>$, this xml node is used to specify the number of nodes RAVEN should request when running in High Performance Computing (HPC) systems;
\item $<procByNode>$, in this section the user specify the number of processor to be used in each HPC node;
\item $<NumCoresUsed>$, global number of cpus RAVEN is going to use for performing the calculation;
\item $<NumMPI>$, this section can be used to specify the number of MPI cpus RAVEN should associate when running the driven software. For example, if RAVEN is driving a code named "FOO", and this code has MPI support, in here the user specifies how many mpi cpus each instance of FOO should use (e.g. mpiexec FOO -np $NumMPI$);
\item $<MaxLogFileSize>$, Every time RAVEN drives a code/software, it creates a logfile of the code screen output. In this block, the user can input the maximum size of log file in bytes. NB. This flag is not implemtend yet; 
\item $<precommand>$: add before of the command that is used to run the external model
\item $<postcommand>$: add after of the command that is used to run the external model
\item $<deleteOutExtension>$:if a run of an external model has not failed delete the outut files with the listed extension
\item $<delSucLogFiles>$:if a run of an external model has not failed delete the log files with the listed extension
\item $<Files>$: these are the paths to the files required by the code, string from the working directory 
\item $<Sequence>$: ordered list of the step name that the simulation will run
\item $<mode>$: these refers to the way in which the parallel environment is set up (currently the only modes supported are pbsdsh and mpi). When the user is running the code on his own machine \textit{no mode should be set}. On the contrary, when the user wants to run the code on the cluster, he should specify the mode. Additional info can be found in the readme file. 
\item $<batchSize>$: number of contemporaneous processors used when the code is running on the cluster. It is good pratice to try to run the input a first time using a batchSize=1 to see if there are no faults inside the RAVEN input and/or any code that is coupled with RAVEN. If there are no faults then the user can input batchSize= to as many processors are needed to be used.
\item $<expectedTime>$*: how much time the simulation is expected to run. After this period of time the cluster will automatically stop the simulation (even if the simulation is not competed) so be sure that the time is overestimated.
\item $<DefaultInputFile>$: default input file to be read*******
\item $<CustomMode>$: **********
\end{itemize}
% source: Simulation.py

The example:
\begin{lstlisting}[style=XML]
<RunInfo>
    <WorkingDir>externalModel</WorkingDir>
    <Files>lorentzAttractor.py</Files>
    <Sequence>MonteCarlo</Sequence>
    <batchSize>100</batchSize>
    <NumThreads>4</NumThreads>    
    <mode>mpi</mode>
    <NumMPI>2</NumMPI>
</RunInfo>
\end{lstlisting}
Specifies the working directory (WorkingDir) where are located the files necessary (Files) to run a series of 100 (batchSize) Monte-Carlo calculations (Sequence).
MPI (mode) mode is used along with 4 threads (NumThreads) and 2 mpi process per run (NumMPI).
