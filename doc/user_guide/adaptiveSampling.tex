\section{RAVEN Theory by way of Examples: Adaptive Sampling Strategies}
In order to perform uncertainty quantification (UQ) and dynamic risk and
probabilistic assessment (D-PRA),
a sampling strategy needs to be employed. The sampling is aimed to
perturb the input space (domain of the uncertainties) in order to explore
the response of a complex system in relation to selected Figures of 
Merits (FOMs). 
\\The most widely used strategies to perform UQ and PRA are generally
collected into the category that, in RAVEN, is named \textit{\textbf{Forward}} samplers. The \textit{\textbf{Forward}} sampler category collects all the strategies that perform the sampling of the input space without exploiting, through dynamic learning approaches, the information made available from the outcomes of evaluation previously performed (adaptive sampling) and the common system evolution (patterns) that different sampled calculations can generate in the phase space (dynamic event tree). 
\\As mentioned in section \ref{par:ForwardSamplers}, RAVEN owns
different \textit{\textbf{Forward}} samplers, among which the most 
common are:
\begin{itemize}
  \item \textit{Monte-Carlo};
  \item \textit{Grid-based};
  \item \textit{Stratified} and its specialization, \textit{Latin Hyper Cube}.
\end{itemize}
In addition, RAVEN posses advanced \textit{\textbf{Forward}} sampling strategies that:
\begin{itemize}
  \item build a grid in the input space selecting evaluation points 
  based on characteristic quadratures as part of stochastic collocation 
  for generalized polynomial chaos method (\textit{Sparse 
  Grid Collocation} sampler);
  \item use high-density model reduction (HDMR) a.k.a. Sobol 
  decomposition to approximate a function as the sum of increasing-
  complexity interactions (\textit{Sobol} sampler).
\end{itemize} 
In the following subsections, the theory behind these sampling 
methodologies are going to be explained by way of applied RAVEN 
examples.






%%%%%%%%%%%%%%%%

\chapter{Adaptive Sampling: Limit Surface Search Method}
\label{chap:AdaptiveSamplingLS}
As already mentioned and easily inferable, the activities proposed and
developed during this research work have been many. Among the 
different methodologies, smart (and, consequentially, adaptive) 
sampling methods have been thought 
and deployed.
%As already mentioned, the key subject of this report is the development of convergence acceleration schemes for the LS search algorithm. LS search is one of the adaptive (or smart) sampling strategies developed within the RAVEN framework.
The motivation of adaptive sampling strategies is that physic 
simulations are often computationally expensive, time consuming, and 
with a large number of uncertain parameters. Thus, exploring the space 
of all possible simulation outcomes is almost unfeasible using finite 
computing resources. During simulation based PRA analysis, it is 
important to discover the relationship between a potentially large 
number of uncertain parameters and the response of a simulation using 
as few simulation trials as possible.
This is a typical context where ``goal'' oriented sampling could be 
beneficial. In this research work, effort has been invested in the 
determination of a goal oriented sampling strategy named ``Limit 
Surface Search method'', a scheme  where few observations, obtained 
from the model run, are used to build a simpler and faster evaluable 
mathematical representation of the model, Reduced Order Model, also 
known as Surrogate Model (ROM or SM). The ROM is then 
used to predict where further exploration of the input space could be 
most informative. This information is used to select new locations in the 
input space for which a code run is executed (see 
Figure~\ref{fig:ExampleLSschematic}). The new 
observations are used to update the ROM and this process iterates 
until, within a certain metric, it is converged.
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/ExampleLSschematic.png}
  \caption{Example of limit surface in the uncertain space.}
  \label{fig:ExampleLSschematic}
\end{figure}
\\To summarize, in the case of the ``Limit 
Surface (LS) Search method'', a ROM is used to 
determine which location in the input space further observations is most 
informative, to establish the location of the LS, then code runs are 
executed on those locations and the ROM updated. The process 
continues until the location of the LS is established within a certain tolerance.

%%%%%%%%%
%%%%%%%%% Limit Surface Concept and Properties
%%%%%%%%%
\section{Limit Surface Concept and Properties}
\label{sec:LSconcept}	
%As already mentioned, this report describes the acceleration schemes implemented for the research of the LS.

To properly explain the Limit Surface concept and relative properties, 
it is necessary to analyze the idea behind the LSs, firstly, from a 
mathematical and, secondly, from a practical point of view.
Consider a dynamic system that is represented in the phase space by the Eq.~\ref{eq:dThetaOverDT} in chapter~\ref{chap:mathBackground}. 
The equation, for simplicity, can be re-casted as follows:
\begin{equation}
\label{eq:ThetaXPT}
\frac{\partial \overline{\theta} }{\partial t}=\overline{H}\left (  \overline{x},\overline{p},t \right )
\end{equation}
where:
\begin{itemize}
  \item $\overline{\theta}$ represents the coordinate of the system in the 
  phase space;
  \item $\left (  \overline{x},\overline{p},t \right )$ independent variables 
  that are separated, respectively, in spatial, temporal, and parametric  
  space (distinction between $\left (  \overline{x},\overline{p},t \right )$ is purely based on engineering considerations).
\end{itemize}
Now it is possible to introduce the concept of ``goal'' function, $C$. $C$ 
is a binary function that, based on the response of the system, can 
assume the value $0$ (false) to indicate that the system is properly 
available (e.g., system success) and $1$ (true) to indicate that the 
system is not available (e.g., failure of the system):
\begin{equation}
C\left (\overline{\theta}, \overline{x},\overline{p},t \right ) = C\left (H\left(\overline{x},\overline{p},t \right), \overline{x},\overline{p},t \right ) = C\left ( \overline{x},\overline{p},t \right ) 
\end{equation}
To simplify the dissertation  and without loss of generality, assume 
that $C$ does not depend on time (e.g. $C\leftarrow 
\bigintssss_{t_{0}}^{t_{end}}dtC\left(  \overline{x},\overline{p},t \right )$):
\begin{equation}
  \label{eq:goalFunction}
  C = C\left (\overline{x},\overline{p}\right ) 
\end{equation}
To simplify the mathematical description of the LS concept, it is 
possible to hypothesize that the 
equation describing the PDF time evolution of the system in the phase 
space is of type Gauss 
Codazzi (in its Liouvilleâ€™s derivation), which allows ensuring that all the 
stochastic phenomena in 
the system are representable as PDFs in the uncertain domain 
(see~\ref{chap:mathBackground}),. This allows combining the 
parametric space with the initial condition space:
\begin{equation}
  \label{eq:goalFunctionCodazzi}
  \begin{matrix} 
  \left ( \overline{x} \right ) \leftarrow \left ( \overline{x},\overline{p} \right ) \\
  C\left ( \overline{x} \right ) \leftarrow C\left ( \overline{x},\overline{p} \right )
  \end{matrix}
\end{equation}

This assumption is rarely violated, for example for those systems that 
present an intrinsic 
stochastic behavior (e.g., the dynamic of a particle of dust in the air 
where it continuously and 
randomly interacts with the molecules of air that ``move'' with different velocities and in different 
and random directions). In most of the cases of interest in the safety analysis, the above 
mentioned assumption is correct. The full heuristic approach to the characterization of system stochastic behaviors is reported in chapter~\ref{chap:mathBackground}.
\\Under the above simplifications, it is possible to identify the region of the input space ($V$) 
leading to a specific outcome of the ``goal'' function. In particular, it can be defined, for example, 
the failure region $V_{F}$ as the region of the input space where $C=1$:
\begin{equation}
 \label{eq:failureRegion}
V_{F}=\left \{ \forall \overline{x} | C\left ( \overline{x} \right ) = 1 \right \}
\end{equation}
The definition of the complementary of the failure region is obviously:
\begin{equation}
V_{F}^{c}=\left \{ \forall \overline{x} | C\left ( \overline{x} \right ) = 0 \right \}
\end{equation}
Its boundary is the named Limit Surface:
\begin{equation}
L_{S}= \partial V_{F}^{c}= \partial      \left \{ \forall \overline{x} | C\left ( \overline{x} \right ) = 1 \right \}
\end{equation}

The identification of the LS location is necessary to identify boundary regions for which the system under consideration will or will not exceed certain FOMs (e.g., operative margins).
\\The LS location is extremely important for design optimization and issue mitigation and, in addition, its informative content can be used to analyze the system to characterize its behavior from a stochastic point of view. Consider $\overline{x} \in V$ and $\overline{x}\sim \overline{X}$,
where $\overline{x}$ is the random variate realization of the stochastic variable $\overline{X}$.
If $pdf_{\overline{X}}\left ( \overline{x} \right ) $ is the probability density function of $ \overline{X}$, the failure probability of the system $\left ( P_{F} \right )$ is:
\begin{equation}
\label{eq:probabilityFailure}
P_{F} = \bigintssss_{V}d\overline{x}C\left ( \overline{x} \right )pdf_{\overline{X}}\left ( \overline{x} \right ) = \bigintssss_{V_{F}+V_{F}^{c}}d\overline{x}C\left ( \overline{x} \right )pdf_{\overline{X}}\left ( \overline{x} \right )
\end{equation}
And, based on the definition given in Equations~\ref{eq:goalFunctionCodazzi} and \ref{eq:failureRegion}:
\begin{equation}
 \label{eq:failureProbIntegral}
 \bigintssss_{V_{F}}d\overline{x}pdf_{\overline{X}}\left ( \overline{x} \right ) 
 \end{equation}
Equations \ref{eq:probabilityFailure} and \ref{eq:failureProbIntegral} are summarized by stating that the system failure probability is 
equivalent to the probability of the system being in the uncertain 
subdomain (region of the input space) that leads to a failure pattern. 
This probability is equal to the probability-weighted hyper-volume that is surrounded by the LS (see Fig.~\ref{fig:ProbabilityFailureLSExample}).
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/ProbabilityFailureLSExample.png}
  \caption{Example of limit surface probability of failure region.}
  \label{fig:ProbabilityFailureLSExample}
\end{figure}
\\It is beneficial for better understanding to assess the LS concept through an example related to the safety of an Nuclear Power Plant (NPP).
As an example, consider a station black out (SBO) scenario in an NPP. Suppose that the only uncertain parameters are:
\begin{itemize}
  \item $t_{F}$: Temperature that would determine the failure of the fuel cladding;
  \item $rt_{DGs}$: Recovery time of the diesel generators (DGs) that 
  can guarantee, through the emergency core cooling system (ECCS), 
  the removal of the decay heat.
\end{itemize}
And, the corresponding CDF (uniform) is:

\begin{equation}
t_{F}\sim pdf_{T_{F}}\left ( T_{F} \right )=\left\{\begin{matrix}
0 & if \: t_{F}< t_{F_{min}} \\ 
\frac{1}{\left ( t_{F_{max}}- t_{F_{min}} \right )=\Delta t_{F}} & \\ 
0 & if \: t_{F}>  t_{F_{max}} 
\end{matrix}\right.
\end{equation}
%
%
\begin{equation}
rt_{DGs}\sim pdf_{RT_{DGs}}\left ( rt_{DGs} \right )=\left\{\begin{matrix}
0 & if \: rt_{DGs}<rt_{DGs_{min}}   \\ 
\frac{1}{\left ( rt_{DGs_{max}} - rt_{DGs_{min}} \right )=\Delta rt_{DGs}} & \\ 
0 & if \: rt_{DGs}>rt_{DGs_{max}} 
\end{matrix}\right.
\end{equation}
For simplicity, assume that the clad temperature is a quadratic function of the DG recovery time in an SBO scenario:
\begin{equation}
  t = t_{0}+\alpha \times rt_{DGs}^{2}
\end{equation}
and that the  $ t_{F_{min}} > t_{0}+\alpha \times rt_{DGs_{min}}^{2}$
and $t_{F_{max}} < t_{0}+\alpha \times rt_{DGs_{max}}^{2}$.
The LS, failure region, and active part of the failure region (failure region with non-zero probability) are illustrated, for example, in Figure~\ref{fig:ProbabilityFailureLSExample} (in agreement with the above assumptions).
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/ExampleLSwitRiskDirections.png}
  \caption{Example of limit surface highlighting the risk directions.}
  \label{fig:ExampleLSwitRiskDirections}
\end{figure}
In this case, the transition/failure probability is evaluated as follows:
\begin{equation}
\begin{matrix} 
P_{F} = \bigintssss_{V_{F}}d\overline{x}\: pdf_{\overline{X}}\left ( \overline{x} \right ) 
= \bigintssss_{0}^{+\infty }d\overline{t_{F}}\: pdf_{T_{F}}\left ( T_{F} \right )\: \: \bigintssss_{\sqrt{\frac{t_{F}-t_{0}}{\alpha}}}^{+\infty } d\, rt_{DGs} \: pdf_{RT_{DGs}}\left ( rt_{DGs} \right )  =
\\
= \bigintssss_{t_{F_{min}}}^{t_{F_{max}}} dt_{F}\frac{1}{t_{F_{max}}-t_{F_{min}}}\: \: \bigintssss_{\sqrt{\frac{t_{F}-t_{0}}{\alpha}}}^{ rt_{DGs_{max}}}\: d\, rt_{DGs} \frac{1}{rt_{DGs_{max}}-rt_{DGs_{min}}} =
\\
= \frac{rt_{DGs_{max}}}{\Delta rt_{DGs}} + \frac{2\alpha}{3\left ( \Delta rt_{DGs}\Delta t_{F} \right )}
\left ( \sqrt[3/2]{\frac{t_{F_{min}}-t_{0}}{\alpha}} - \sqrt[3/2]{\frac{t_{F_{max}}-t_{0}}{\alpha}} \right )
\end{matrix}
\end{equation}

This simple example is useful to understand how the LS is defined in a practical application (that is analyzed numerically in the results section) and how the hyper volume needs weighted with respect to the probability in the uncertain domain. An example of the LS computed is shown in Figure ~\ref{fig:ExampleLSwitRiskDirections}.
In this figure the neutral and high risk directions are highlighted. 
%%%%%%%%%
%%%%%%%%% Reduced Order Models
%%%%%%%%%
\section{Reduced Order Models}
\label{sec:ROMs}	
As briefly and previously mentioned, a ROM, also called a surrogate 
model, is a mathematical representation of a system, used to predict 
a selected FOM of a physical system.
\\The ``training'' is a process that, sampling the physical model 
represented by the high fidelity simulator (e.g., RELAP 7, RELAP5 
3D, PHISICS, etc.), is aimed to improve the prediction capability 
(ability to predict the status of the system given a realization of the 
uncertain domain) of the ROM. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/ROMexampleOfPhysicalSystem.png}
  \caption{Example of reduced order model representation of physical system (regression).}
  \label{fig:ROMexampleOfPhysicalSystem}
\end{figure}
\\Two characteristics of these models 
are generally assumed (even if exceptions are possible):
\begin{enumerate}
  \item The higher the number of realizations in the training sets, the 
higher the accuracy of the prediction performed by the ROM is. This 
statement is true for most of the cases although some ROMs might be 
subject to the over-fitting issues. The over-fitting phenomenon is not 
analyzed in this thesis, since its occurrence highly depends on the 
algorithm type, and, hence, the problem needs to be analyzed for all 
the large number of ROM types available;
  \item The smaller the size of the input (uncertain) domain with 
  respect to the variability of the system response, the more likely the 
  ROM is able to represent the system response space.
\end{enumerate}

To provide a very simple idea of a ROM, assume that the final 
response space of a physical system is governed by the transfer 
function $H \left (  \overline{x}\right)$ (see 
\ref{chap:mathBackground}), which, from a practical point of 
view, represents the outcome of the system, based on the initial 
conditions  $\overline{x}$. Now, sample the domain of variability of the 
initial conditions $\overline{x}$ to create a 
set of $N$ realizations of the input and response space $ \left ( \left ( 
\overline{x}_{i}, H \left (  \overline{x}_{i}\right) \right), i=1,N \right)$, 
named ``training'' set. Based on the data set generated, it is possible 
to construct a mathematical representation $G\left ( \overline{x}:
\overline{x}_{i}\right)$ of the 
real system $H \left (  \overline{x}\right)$, which will approximate its 
response (see Figure~\ref{fig:ROMexampleOfPhysicalSystem}):
\begin{equation}
\label{eq:regressor}
G\left ( \overline{x} \right ):\overline{x}_{i} \rightarrow G\left ( \overline{x}_{i} \right ) \cong H\left ( \overline{x}_{i} \right )
\end{equation}
The ROMs reported above are generally named ``regressors'', among 
which all the most common data fitting algorithms are found (e.g., 
least square for construction of linear models).
\\An important class of ROMs for the work presented here after is the 
one containing the so called ``classifiers''. A classifier is a ROM that is 
capable of representing the system behavior from a binary point of 
view (e.g., event happened/not happened or failure/success). It is a 
model (set of equations) that identifies to which category an object 
belongs in the feature (input) space. Referring to the example that 
brought to Eq.~\ref{eq:regressor}, a classifier can be formally represented as follows (see 
Figure~\ref{fig:ROMClassifierExampleOfPhysicalSystem}):
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/ROMClassifierExampleOfPhysicalSystem.png}
  \caption{Example of reduced order model representation of physical system (classifier).}
  \label{fig:ROMClassifierExampleOfPhysicalSystem}
\end{figure}

\begin{equation}
\label{eq:classifier}
G\left ( \overline{x} \right ):\overline{x}_{i} \rightarrow G\left ( \overline{x}_{i} \right ) \cong 
C \left ( H\left ( \overline{x}_{i} \right ) \right )
\end{equation}

The function $C\left (  H\left ( \overline{x}_{i}  \right ) = \overline{\theta} 
\right ) $ is the so called ``goal'' function that is able to recast the 
response of the system $H\left ( \overline{x}_{i}  \right )$ into a binary 
form (e.g., failure/success). As an example, referring to 
Figure~\ref{fig:ROMClassifierExampleOfPhysicalSystem}, the 
``goal'' function would be:
\begin{equation}
\label{eq:goalFunctionClassifier}
C\left (   \overline{\theta}  \right ) = \left\{\begin{matrix}
1 & if \: \overline{\theta}>1.0 \\ 
0 &  if \: \overline{\theta} \leq 1.0
\end{matrix}\right.
\end{equation}
Hence, the ROM of type classifier $G\left (  \overline{x} \right )$  will operate in the space transformed through the ``goal''  function $C\left (   \overline{\theta}  \right )$. 
\\The classifiers and regressors can be categorized into two main classes:
\begin{itemize}
  \item Model based algorithms;
  \item Data based algorithms.
\end{itemize}
In the first class, the created ROM aims to approximate the response 
of the system as a function of the input parameters. These algorithms 
construct a functional representation of the system. Examples of such ROM type are support vector machines (SVMs), Kriging-based interpolators, discriminant based models, and polynomial chaos.
\\On the other side, data based algorithms do not build a response-
function-based ROM but classify or predict the response of the 
system from the neighborhood graph constructed from the training 
data, without any dependencies on a particular prediction model.
\\These algorithms directly build a neighborhood structure as the 
ROM (e.g., a relaxed Gabriel graph) on the initial training data. Examples of such ROM type are nearest neighbors and decision trees.
\\ In the following subsections some of the ROM types that have been
the most used in this research are reported:
%%%%%%%%%
%%%%%%%%% KNN Classifier and KNR Regressor
%%%%%%%%%
\subsection{KNN Classifier and KNR Regressor}
\label{subsec:KNNKNR}	
The K Nearest Neighbor algorithm~\cite{KNNrom} is a non-parametric method 
used for both regression and classification. The only input parameter 
is the variable $K$ which indicates the number of neighbors to be 
considered in the classification/regression process. The special case 
where the class is predicted to be the class of the closest training 
sample (i.e. when $K = 1$) is called the nearest neighbor algorithm. 
In binary (two class) classification problems, it is helpful to choose k to 
be an odd number as this avoids tied votes. The output depends on 
whether KNN is used for classification or regression:
\begin{itemize}
  \item In KNN classification, the output is a class membership. An 
  object is 
  classified by a majority vote of its neighbors, with the object being 
  assigned to the class most common among its $K$ nearest 
  neighbors 
  ($K$ is a positive integer, typically small). If $K = 1$, then the object 
  is simply assigned to the class of that single nearest neighbor.
  \item  In KNN regression, the output is the property value for the 
  object. 
  This value is the average of the values of its $K$ nearest neighbors.
\end{itemize}
Both for classification and regression, it can be useful to assign 
weight to the contributions of the neighbors, so that the nearer 
neighbors contribute more to the average than the more distant ones. 
For example, a common weighting scheme consists in giving each 
neighbor a weight of $1/d$, where d is the distance to the neighbor.

%%%%%%%%%
%%%%%%%%% Support Vector Machines- Classifier
%%%%%%%%%
\subsection{Support Vector Machines - Classifier}
\label{subsec:SVC}	
Given a set of $N$ multi-dimensional samples $x_{i}$ and their associated
results $\theta_{i} = \pm 1$ (e.g., $\theta_{i} = + 1$ for system success and $\theta_{i} = - 1$ for system failure), the SVM 
algorithm~\cite{SVMrom} finds the boundary (i.e., the decision 
function) that separates the set of points having different $\theta_{i}$. 
The decision function lies between the support hyper-planes, which 
are required to:
\begin{itemize}
  \item Pass through at least one sample of each class (called support 
vectors);
  \item Not contain samples within them.
\end{itemize}
For the linear case, see Figure~\ref{fig:SVMdecisionFunction}, the 
decision function is chosen 
such that distance between the support
hyper-planes is maximized.
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/SVMdecisionFunction.png}
  \caption{Support Vector Machine decision function.}
  \label{fig:SVMdecisionFunction}
\end{figure}
\\From a mathematical point of view, the hyper plane can be written in 
this form:
\begin{equation}
\label{eq:SVMhyperPlane}
  \overline{w}  \: \overline{x} - b = 0
\end{equation}
Given this formulation, the SVM algorithm aims to:
\begin{equation}
\label{eq:SVMalghoritm}
\begin{matrix}
minimize \: \: \left \| \overline{w} \right \| \\ 
subject \: \: to \: \: \theta_{i}\left ( \left \langle \overline{w},\overline{x}_{i} \right \rangle- b \right )
\end{matrix}
\end{equation}
Without going into the mathematical details, the determination of the 
hyper-planes is performed recursively and updated every time a new 
sample has been generated. Figure~\ref{fig:SVMdecisionFunction} 
shows the SVM decision 
function and the hyper-planes for a set of points in a 2-dimensional 
space having two different outcomes: $\theta_{i}=+1$ (green) and 
$\theta_{i}=-1$ (red).
\\The transition from a linear to a generic non-linear hyper-plane is 
performed using the kernel trick: i.e., by projecting of the original 
samples into a higher dimensional space known as featured space 
generated by kernel functions $K\left (\overline{x}_{i},\overline{x}_{j}  \right )$:
\begin{equation}
\label{eq:SVMalghoritmKernelTrick}
K\left (\overline{x}_{i},\overline{x}_{j}  \right ) =  exp \left ( - \frac{\left \|  \overline{x}_{i} - \overline{x}_{j} \right \|}{2\sigma^{2} }  \right )
\end{equation}

%%%%%%%%%
%%%%%%%%% Support Vector Machines- Regressor
%%%%%%%%%
\subsection{Support Vector Machines - Regressor}
\label{subsec:SVR}
 version of SVM for regression called support vector regression (SVR) 
 was proposed in~\cite{SVRrom}. The model produced by support 
 vector classification (as described above) depends only on a subset 
 of the training data, because the cost function for building the model 
 does not care about training points that lie beyond the margin. 
 Analogously, the model produced by SVR depends only on a subset 
 of the training data, because the cost function for building the model 
 ignores any training data close to the model prediction.
\\Training the original SVR means solving: 
\begin{equation}
\label{eq:SVRalghoritm}
\begin{matrix} 
minimize \: \:  \frac{1}{2}\left \| \overline{w}^{2} \right \|
\\
subject \: \:  to 
\left\{\begin{matrix}
\theta_{i} - \left \langle \overline{w}, \overline{x}_{i} \right \rangle - b \leq \varepsilon
\\ 
\left \langle \overline{w}, \overline{x}_{i} \right \rangle + b - \theta_{i}  \leq \varepsilon 
\end{matrix}\right.
\end{matrix}
\end{equation}
where $\overline{x}_{i}$ is a training sample with target value 
$\theta_{i}$. The inner product  $\left \langle \overline{w},
\overline{x}_{i} \right \rangle + b$ is the prediction for that sample, and 
$\varepsilon $ is a free parameter that serves as a threshold: all 
predictions have to be within an $\varepsilon $ range of the true 
predictions.

%%%%%%%%%
%%%%%%%%% Gaussian Process Models
%%%%%%%%%
\subsection{Gaussian Process Models}
\label{subsec:GPM}
Gaussian processes (GPs) ~\cite{GPMrom} are algorithms that 
extend multivariate Gaussian distributions to infinite dimensionality. A 
Gaussian process generates a dataset located throughout some 
domain such that any finite subset of the range follows a multivariate 
Gaussian distribution. Now, the $N$ observations in an arbitrary data 
set, $\overline{\theta} = \left \{ \theta_{i},..., \theta_{N} \right \}$, can 
always be imagined as a single point sampled from some multivariate 
(N-variate) Gaussian distribution.
\\What relates one observation to another in such cases is just the covariance function, $k\left ( x,x' \right )$. A popular choice is the squared exponential:
\begin{equation}
\label{eq:GPMsquaredExponential}
k\left ( x,x' \right ) = \sigma_{f}^{2}\: exp\left [ \frac{-\left ( x-x' \right )^{2}}{2l^{2}} \right ]
\end{equation}
where the maximum allowable covariance is defined as $\sigma_{f}^{2}$ - this should be high for functions which cover a broad range on the y axis. If $x \approx x'$, then $k\left ( x,x' \right )$
approach this maximum meaning $f(x)$ is very correlated to $f(x')$.
On the other hand, if  $x$ is very distant from $x'$, then $k\left ( x,x' \right ) \approx 0$, i.e. the two points cannot see each other. So, for example, during interpolation at new $x$ values, distant observations will have negligible effect. How much effect this separation has will depend on the length parameter $l$.
\\ Each observation $\theta$ can be thought of as related to an underlying function $f(x)$ through a Gaussian noise model:
\begin{equation}
\label{eq:GPMsquaredExponential}
\theta = f(x) + \mathcal{N}(0,\sigma_{n}^{2}) 
\end{equation}
The new kernel function can be written as:
\begin{equation}
\label{eq:GPMalgorithm}
k\left ( x,x' \right ) = \sigma_{f}^{2}\: exp\left [ \frac{-\left ( x-x' \right )^{2}}{2l^{2}} \right ] + \sigma_{n}^{2} \delta(x,x')
\end{equation}
So given $N$ observation $\overline{\theta}$, the objective is to predict the value $\theta_{*}$ at the new point $x_{*}$. This process is performed by following this sequence of steps:
\begin{enumerate}
  \item Calculate three matrices:
  \begin{equation}
  \label{eq:GPMalgorithmMatrixK}
  K = \begin{bmatrix}
  k\left ( x_{1},x_{1} \right ) & \cdots  & k\left ( x_{1},x_{N} \right )\\ 
  \vdots  & \ddots  & \vdots \\ 
  k\left ( x_{N},x_{1} \right ) & \cdots  &  k\left ( x_{N},x_{N} \right )
  \end{bmatrix}  
  \end{equation}
  \begin{equation}
  \label{eq:GPMalgorithmMatrixKstar}
  K_{*} = \left [ k\left ( x_{*},x_{1} \right ) \cdots  k\left ( x_{*},x_{N} 
  \right ) \right ]
  \end{equation}  
  \begin{equation}
  \label{eq:GPMalgorithmMatrixKstarstar}
  K_{**} = k\left ( x_{*},x_{*} \right )
  \end{equation}
  \item The basic assumption of GPM is that:
  \begin{equation}
  \label{eq:GPMalgorithmAssumption}
   \begin{bmatrix}
   \overline{\theta}
   \\ 
   \theta_{*}
   \end{bmatrix}
   \sim  \mathcal{N} \left ( 0, \begin{bmatrix}
   K & K_{*}^{T}\\ 
   K_{*} & K_{**}
   \end{bmatrix} \right )
  \end{equation}
   \item The estimate $\widetilde{\theta}_{*}$ for $\overline{\theta}_{*}$
    is the mean of this distribution:
   \begin{equation}
   \label{eq:GPMalgorithmEstimate}    
    \widetilde{\theta}_{*} = K_{*} K^{-1} \overline{\theta}
    \end{equation}    
    \item The uncertainty associated to the estimate 
    $\widetilde{\theta}_{*} $ can be expressed in terms of variance of 
    $\theta_{*} $:
   \begin{equation}
   \label{eq:GPMalgorithmEstimate}    
    var(\theta_{*}) = K_{**} -  K_{*} K^{-1} K_{*}^{T}
    \end{equation}      
\end{enumerate}

%%%%%%%%%
%%%%%%%%% Limit Surface Search Algorithm
%%%%%%%%%
\section{Limit Surface Search Algorithm}
\label{sec:LSSalgorithm}
The identification of the LS location is extremely challenging, 
depending on the particular physics/phenomena that are investigated. 
To identify the real location of the LS, the evaluation of system 
responses is needed, through the high fidelity code (e.g., RELAP 7, 
RELAP5 3D, etc.), in the full domain of uncertainty (infinite number of 
combinations of uncertainties represented by the respective PDFs). 
Obviously, this is not a feasible approach, and a reasonable 
approximation is to locate the LS on a Cartesian N-D grid, in the 
uncertain domain.
\\In reality, the location of the LS is not exactly determined but rather 
bounded. The algorithm determines the set of grid nodes between 
which the transition $0/1$ of the ``goal'' function happens. This set is 
also classified with respect to the value of the ``goal'' function. With 
reference to Figure~\ref{fig:LSgoalFunctionExample}, for example, 
green is used for grid nodes with a 
``goal'' function that equals $0$ and red when the ``goal'' function 
equals $1$.
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/LSgoalFunctionExample.png}
  \caption{Example of limit surface search evaluation grid (where $\overline{y}=\overline{\theta}$).}
  \label{fig:LSgoalFunctionExample}
\end{figure}
Each evaluation of the ``goal'' function in one of the grid nodes implies 
the evaluation of the high fidelity code (e.g. system simulator) for the 
corresponding set of entries in the uncertain space. As already 
mentioned, the evaluation of the high fidelity code is computationally 
expensive and, in order to identify the LS, one should appraise 
each point in the N-D grid covering the uncertainty space. 
Discretization depends on the accuracy requested by the user. In 
most cases, this approach is not feasible and, consequentially, the 
process needs to be accelerated using ``predicting'' methods that are 
represented by the employment of supervised learning algorithms 
(ROMs presented in previous section).
\\This approach is commonly referred to as an active learning process 
that ultimately results in training of a ROM of type classifier capable of 
predicting the outcome of the ``goal'' function for any given point of 
the uncertain space.
In an active learning process, a supervised learning algorithm is 
combined with criteria to choose the next node in the N D grid that 
needs explored, using the high fidelity physical model. This process is 
repeated until, under a particular metric, the prediction capabilities of 
the supervised learning algorithm do not improve by further increasing 
the training set.
\\In more detail, the iterative scheme, which has been defined in this 
research work, could be summarized through the following steps:
\begin{enumerate}
  \item A limited number of points in the uncertain space $\left \{ 
  \overline{x}_{k} \right \}$ are selected via one of the forward 
  sampling strategies (e.g., stratified or MC);
  \item The high fidelity code is used to compute the status of the 
  system for the set of points in the input set: 
  $
  \left \{ \overline{\theta}(t)\right \}_{k} = H\left ( \left \{ \overline{x} \right 
  \}_{k},t \right )
  $
  \item The ``goal'' function is evaluated at the phase space coordinate 
  of the system:
  $\left \{ c \right \}_{k} = C\left ( \left \{ \overline{\theta}(t)\right \}_{k} 
  \right )$.
   \item The set of pairs $\left \{ \left ( \overline{x},c \right )_{k} \right \}$
   are used to train a ROM of type classifier, $G\left ( \left \{ 
   \overline{x}_{k} \right \} \right )$.
   \item The ROM classifier is used to predict the values of the ``goal''  
   function for all the $N$ nodes of the N-D grid in the domain space:
   \begin{equation}
   \left (G\left ( \left \{ \overline{x} \right \}_{j} \right ) \sim \left \{ c \right 
   \}_{j}, j=1,...,N  \right )
    \end{equation}
    \item The values of the ``goal''  function are used to determine the 
    LS location based on the change of values of  $\left \{ c \right 
    \}_{j}$:
    \begin{equation}
    \left \{ c \right \}_{j}\rightarrow \partial V_{F}
     \end{equation}
     \item A new point is chosen to increase the training set and a new 
     pair is generated;
     \item The procedure is repeated starting from Step 3 until 
     convergence is achieved. The convergence is achieved when 
     there are no changes in the location of the LS after a certain 
     number of consecutive iterations.
\end{enumerate}
The iteration scheme is graphically shown in 
Figure~\ref{fig:LimitSurfaceAlgoFlow}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/LimitSurfaceAlgoFlow.png}
  \caption{Limit surface search algorithm conceptual scheme.}
  \label{fig:LimitSurfaceAlgoFlow}
\end{figure}
It is important to notice that there is an additional requirement 
regarding the LS search algorithm. It is required that the LS location 
stays constant for a certain number (user defined) of consecutive 
iterations. The reason for this choice is determined by the attempt to 
mitigate the effect of the build of non-linear bias in the searching 
pattern. Indeed, the searching algorithm might focus too much on a 
certain region of the LS while putting too few points in other zones 
and completely hiding undiscovered topological features of the LS.
Regarding the strategy to choose the nodes on the N-D grid that 
needs evaluated in the iterative process for the LS identification, it has
been decided (in this research) to employ a metric based on the 
distance between the predicted LS and the evaluations already 
performed. The points on the LS are ranked based on the distance 
from the closest training point already explored (the larger is the 
distance the higher is the score for the candidate point), and based on 
its persistence (the larger is the number of time the prediction of the 
``goal'' function for that point have changed the higher is the score).
Since this approach creates a queue of ranked candidates, it could be 
used also in the parallel implementation of the algorithm. When 
several training points are run in parallel, it is possible that the 
evaluation of one additional point does not alter dramatically the 
location of the LS. Consequently, it is possible that the candidate with 
the highest score is already being submitted for evaluation and 
possibly the simulation is not yet completed. In this case, to avoid 
submitting the same evaluation point twice, the algorithm searches 
among all the ranked candidates (in descending order) for the one 
that was not submitted for evaluation. Even if it is extremely unlikely 
that all the candidates were submitted, in this remote event, the 
method will choose the next point employing a MC strategy.
%%%%%%%%%
%%%%%%%%% Acceleration through Multi-grid Approach
%%%%%%%%%
\section{Acceleration through Multi-grid Approach}
\label{subsec:LSaccelerationMultiGrid}
The location of the LS, being a numerical iterative process, can be 
known given a certain tolerance. As already mentioned, the LS search 
is done by constructing an evaluation grid, on which the acceleration 
ROM is inquired. The tolerance of the iterative process determines how 
the evaluation grid is discretized. Before addressing the acceleration 
scheme that has been developed as part of this research, it is important 
to introduce some concepts on the employed numerical process.
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/DiscretizationGrid.png}
  \caption{Discretization grid.}
  \label{fig:DiscretizationGrid}
\end{figure}
\\Assume that each of $D$ dimensions of the uncertain domain is 
discretized with the same number of equally-spaced nodes $N$ (see 
Figure~\ref{fig:DiscretizationGrid}), with discretization size indicated 
by $h_{i}$. Hence, the Cartesian grid contains $N^{D}$ individual 
nodes, indexed through the multi-index vector $\overline{j} = \left ( 
j_{i=1\rightarrow D} \right ), j_{i} \leq N \forall i$. Introducing the 
vectors  $\overline{I} = (1, ..., 1)$ and
$\overline{N} = (N, ..., N)$, the ``goal'' function is expressed on this N-
D grid as:
\begin{equation}
C\left ( \overline{x} \right ) = 
\mathlarger{\sum}_{\overline{j}=\overline{I}}^{\overline{N}} \varphi_{\overline{j}}\left 
( \overline{x} \right ) C\left ( \overline{x}_{\overline{j}} \right )
\end{equation}
where $\varphi_{\overline{j}}$ is the characteristic function of the hyper-volume $\Omega_{\overline{j}}$ surrounding the node 
 $\overline{x}_{\overline{j}}$:
 \begin{equation}
 \varphi_{\overline{j}}\left ( \overline{x} \right ) = 
\left\{\begin{matrix}
1 & if \: \overline{x} \in \Omega_{\overline{j}} \\ 
0 & if \: \overline{x} \notin \Omega_{\overline{j}} 
\end{matrix}\right.
\end{equation}
where:
 \begin{equation}
 \label{eq:OmegaEq}
 \Omega_{\overline{j}} = \prod_{i=1}^{D}\left [ x_{j_{i}} - \frac{h_{i}}{2}, 
 x_{j_{i}} + \frac{h_{i}}{2}  \right ]
 \end{equation}
 The probability of the uncertain parameters is expressed as:
 \begin{equation} 
 pdf_{\overline{X}}\left ( \overline{x} \right ) = 
\mathlarger{\sum}_{\overline{j}=\overline{I}}^{\overline{N}}
\varphi_{\overline{j}}\left ( \overline{x} \right )
pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right )
 \end{equation}
Following the approach briefly explained in section 
\ref{sec:LSconcept}, the probability of the event (e.g., failure) could be 
expressed as: 
 \begin{equation} 
 \label{eq:Pf}
  P_{F}=\left ( \prod_{i=1}^{D}h_{i} \right ) 
  \mathlarger{\sum}_{\overline{j}=
  \overline{I}}^{\overline{N}}pdf_{\overline{X}}\left ( 
  \overline{x_{\overline{j}}} \right )
  C\left ( \overline{x}_{\overline{j}} \right )
 \end{equation}
Under certain assumptions, the concept of active hyper-volume 
$V_{A}$ as the region of the input space identified by the support of 
the uncertain parametersâ€™ probability density functions 
$pdf_{\overline{X}}\left ( \overline{x} \right )$ could be introduced; 
Eq.~\ref{eq:Pf} is recast, using a Taylor expansion, as follows: 
 \begin{equation} 
P_{F}= \bigintssss_{V} C\left ( \overline{x} \right )\: pdf_{\overline{X}}\left ( 
\overline{x}  \right )\: d\overline{x} =\bigintssss_{V_{A}}C\left ( \overline{x} 
\right )\: 
\left [  
\mathlarger{\sum}_{\overline{j}=\overline{I}}^{\overline{N}}\varphi_{\overline{j}}\left ( 
\overline{x} \right )
\left ( pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right ) +
\mathlarger{\sum}_{i=1}^{D} \frac{\partial pdf_{\overline{X}}}{\partial x_{i}}|
_{\overline{x}_{\overline{j}}} \: \left ( x_{i} - x_{j_{i}} \right ) \right ) \right 
] d\overline{x}
 \end{equation}
And, considering the evaluation grid as:
\begin{equation}
\label{eq:PfEq}
P_{F}= \mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \end{matrix}}^{\overline{N}} 
\bigintssss_{\overline{x}_{\overline{j}} - \overline{h}/2}^{\overline{x}_{\overline{j}} + \overline{h}/2} C\left ( \overline{x} \right )\: 
\left [  \mathlarger{\sum}_{\overline{j}=\overline{I}}^{\overline{N}}\varphi_{\overline{j}}\left ( \overline{x} \right )
\left ( pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right ) +
\mathlarger{\sum}_{i=1}^{D} \frac{\partial pdf_{\overline{X}}}{\partial x_{i}}|_{\overline{x}_{\overline{j}}} \: \left ( x_{i} - x_{j_{i}} \right ) \right ) \right ] d\overline{x}
\end{equation}
At this point, it is possible to label, in the active hyper-volume, the sub-
domain identified by the nodes where the ``goal'' function  
$C(\overline{x})$ changes its value (the frontier nodes between the 
region where $C(\overline{x})=1$ and $C(\overline{x})=0$) $V_{A} \cap 
V_{\partial V_{F}}$.
\\ Consequentially, it is possible to identify the sub-domains in which the  ``goal''  function $C(\overline{x})$ is equal to $0$ ($V_{A} \cap V_{\partial V_{C(\overline{x})=0}} \notin V_{A} \cap V_{\partial V_{F}}$):

\begin{equation}
\mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap V_{C(\overline{x})=0} \end{matrix}}^{\overline{N}} 
\bigintssss_{\overline{x}_{\overline{j}} - \overline{h}/2}^{\overline{x}_{\overline{j}} + \overline{h}/2} C\left ( \overline{x} \right )\: 
\left ( pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right ) +
\mathlarger{\sum}_{i=1}^{D} \frac{\partial pdf_{\overline{X}}}{\partial x_{i}}|_{\overline{x}_{\overline{j}}} \: \left ( x_{i} - x_{j_{i}} \right ) \right )   d\overline{x}
\end{equation}
in which the  ``goal''  function $C(\overline{x})$ is equal to $1$ ($V_{A} \cap V_{\partial V_{C(\overline{x})=1}} \notin V_{A} \cap V_{\partial V_{F}}$):
\begin{equation}
\begin{matrix} 
\mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap V_{C(\overline{x})=1} \end{matrix}}^{\overline{N}} 
\bigintssss_{\overline{x}_{\overline{j}} - \overline{h}/2}^{\overline{x}_{\overline{j}} + \overline{h}/2} C\left ( \overline{x} \right )\: 
\left ( pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right ) +
\mathlarger{\sum}_{i=1}^{D} \frac{\partial pdf_{\overline{X}}}{\partial x_{i}}|_{\overline{x}_{\overline{j}}} \: \left ( x_{i} - x_{j_{i}} \right ) \right )   d\overline{x} =
\\ 
= \mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap V_{C(\overline{x})=1} \end{matrix}}^{\overline{N}} 
\bigintssss_{\overline{x}_{\overline{j}} - \overline{h}/2}^{\overline{x}_{\overline{j}} + \overline{h}/2}
\left ( pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right ) +
\mathlarger{\sum}_{i=1}^{D} \frac{\partial pdf_{\overline{X}}}{\partial x_{i}}|_{\overline{x}_{\overline{j}}} \: \left ( x_{i} - x_{j_{i}} \right ) \right )   d\overline{x}
\end{matrix}
\end{equation}
Eq.~\ref{eq:PfEq} is now expressed as:
 \begin{equation}
 \label{eq:PfEqFinal}
 \begin{matrix}
P_{F} = \mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap 
V_{C(\overline{x})=1} \end{matrix}}^{\overline{N}} 
\left ( \prod_{i=1}^{D} 
 h_{i} \right ) 
\: pdf_{\overline{X}} ( \overline{x}_{\overline{j}}) + O(h^{N+1}) +
 \\
 + \mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap 
V_{\partial V_{f}} \end{matrix}}^{\overline{N}}
\bigintssss_{\overline{x}_{\overline{j}} - 
\overline{h}/2}^{\overline{x}_{\overline{j}} + \overline{h}/2} C\left ( 
\overline{x} \right )\: \left ( pdf_{\overline{X}}\left ( 
\overline{x}_{\overline{j}} \right ) +
\mathlarger{\sum}_{i=1}^{D} \frac{\partial pdf_{\overline{X}}}{\partial 
x_{i}}|_{\overline{x}_{\overline{j}}} \: \left ( x_{i} - x_{j_{i}} \right ) \right )   
d\overline{x}
\end{matrix} 
 \end{equation} 
As inferred from Eq.~\ref{eq:PfEqFinal}, the process is bounded if the 
surface area to volume ratio (amount of surface area per unit volume) is 
in favor of the volume:
\begin{equation}
\label{eq:convergenceCondition}
\mathlarger{\sum}_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap 
V_{C(\overline{x})=1} \end{matrix}}^{\overline{N}} 
\left ( \prod_{i=1}^{D} 
 h_{i} \right ) 
\: pdf_{\overline{X}} ( \overline{x}_{\overline{j}})
\gg 
\sum_{\begin{matrix}
\overline{j}=\overline{I} \\ \overline{x}_{\overline{j}} \in V_{A} \cap 
V_{\partial V_{f}} \end{matrix}}^{\overline{N}}
\left | \int_{\overline{x}_{\overline{j}} - 
\overline{h}/2}^{\overline{x}_{\overline{j}} + \overline{h}/2}   pdf_{\overline{X}}\left ( 
\overline{x}_{\overline{j}} \right )  \right |
d\overline{x}
\end{equation}
If the grid is built in the transformed space of probability (i.e., replacing the measure $d\overline{x}$ with $d \overline{\mu }\: \: pdf_{\overline{X}}\left ( \overline{x}_{\overline{j}} \right )$, the condition expressed in Eq.~\ref{eq:convergenceCondition} is reduced:
\begin{equation}
number \: \:  nodes \in V_{A} \cap V_{C(\overline{x})=1} \gg number \: \:  nodes  \in V_{A} \cap V_{\partial V_{F}}
\end{equation}
This means that error is bounded by the total probability contained in the cells on the frontier of the LS.
\\Based on this derivation, it is clear how important it is to keep the 
content of the total probability on the frontier of the LS as low as 
possible, and simultaneously, increase the importance of the volume of 
the failure/event region as much as possible (to improve the surface 
area to volume ratio).
\\In order to do that, the step size in probability should be significantly 
reduced ( $h_{i}^{p} \rightarrow 0^{+}$). Even if this is theoretically 
feasible, it is computational inapplicable. To approach a similar result, it 
is possible to learn from other numerical methods that use the 
technique of adaptive meshing for the resolution of the partial 
differential equation system (e.g., finite element methods).
\\For this reason, an acceleration scheme was designed and developed 
employing a multi-grid approach. The main idea, it is to recast the 
iterative process in two different sub-sequential steps. Firstly, 
performing the LS search on a coarse evaluation grid, and once 
converged, adaptively refining the cells that lie on the frontier of the LS 
($V_{A} \cap V_{\partial V_{F}}$) and, consequentially, converging on 
the new refined grid.
\\The iteration scheme is graphically shown in 
Figure~\ref{fig:LimitSurfaceMultiGridAlgoFlow}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]  {Pictures/LimitSurfaceMultiGridAlgoFlow.png}
  \caption{Multi-grid limit surface search scheme.}
  \label{fig:LimitSurfaceMultiGridAlgoFlow}
\end{figure}
In more detail, the iterative scheme could be summarized through the 
following steps:
\begin{enumerate}
  \item The user specifies two tolerances in probability $(CDF):
  \gamma^{g=1}$ for the initial coarse grid and $\gamma^{g=2}$
  for the refined grid, where  $ \gamma^{g=1} >  \gamma^{g=2}$;
  \item Following Eq.~\ref{eq:OmegaEq}, the initial coarse evaluation 
  grid $\Omega^{1}$ is constructed ($N^{g=1}$ total nodes). The 
  discretization of this grid is done to have cells with a content of 
  probability equal to $\gamma^{g=1}$;
  \item A limited number of points in the uncertain space $\left \{ 
  \overline{x}_{k} \right \}$ are selected via one of the forward 
  sampling strategies (e.g., stratified or MC);
  \item The high fidelity code is used to compute the status of the 
  system for the set of points in the input set: 
  $
  \left \{ \overline{\theta}(t)\right \}_{k} = H\left ( \left \{ \overline{x} \right 
  \}_{k},t \right )
  $
  \item The ``goal'' function is evaluated at the phase space coordinate 
  of the system:
  $\left \{ c \right \}_{k} = C\left ( \left \{ \overline{\theta}(t)\right \}_{k} 
  \right )$.
   \item The set of pairs $\left \{ \left ( \overline{x},c \right )_{k} \right \}$
   are used to train a ROM of type classifier, $G\left ( \left \{ 
   \overline{x}_{k} \right \} \right )$.
   \item The ROM classifier is used to predict the values of the ``goal''  
   function for all the $N^{g=1}$ nodes of the N-D grid in the domain 
   space:
   \begin{equation}
   \left (G\left ( \left \{ \overline{x} \right \}_{j} \right ) \sim \left \{ c \right 
   \}_{j}, j=1,...,N^{g=1}  \right )
    \end{equation}
    \item The values of the ``goal''  function are used to determine the 
    LS location based on the change of values of  $\left \{ c \right 
    \}_{j}$:
    \begin{equation}
    \left \{ c \right \}_{j}\rightarrow \partial V_{F}
     \end{equation}
     \item A new point is chosen to increase the training set and a new 
     pair is generated;     
     \item The procedure is repeated starting from Step 5 until 
     convergence is achieved on grid $\Omega^{g}$.The convergence 
     is reached when there are no changes in the location of the LS 
     after a certain number of consecutive iterations (user defined);
     \item When the convergence is achieved on the coarse grid 
     $\Omega^{g=1}$, all the cells that lie on the frontier of the LS 
     ($V_{A} \cap V_{\partial V_{F}}$) are refined to contain an amount of 
     probability equal to  $\gamma^{g=2}$;
     \item Steps 7 through 9 are performed based on the new refined 
     grid. Finally, the process starts again by performing Steps 5 
     through 10, until the convergence is achieved in the refined grid.
\end{enumerate}
As shown in Figure~\ref{fig:LimitSurfaceMultiGridAlgoFlow}, the 
algorithm consists in searching the location of the LS proceeding with 
sub-sequential refinement of the sub-domain, in the active space, that 
contains the LS. In this way, the computational burden is kept as low as 
possible. In addition, another advantage of this approach is that, since 
the refinement grid represents a constrained domain, the sub-
sequential ROM training process can be regularized, since the LS 
between an iteration and the other can move, at maximum, within the 
refinement domain.

%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  MONTE-CARLO %%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monte-Carlo}
\label{sub:MC}
The Monte-Carlo method is probably one of the most used methodologies in several disciplines. In this section, a brief theoretical 
background is reported. In addition,it is shown how to employ this methodology with RAVEN.
\subsubsection{Monte-Carlo theory introduction}
\label{subsub:MCtheory}
The Monte-Carlo method is aimed to approximate an expectation by the sample mean of a function of 
simulated random variables. It is based on the laws of large numbers in order to approximate expectations. 
In order words, it is aimed to approximate the average response of multiple FOMs 
relying on multiple random sampling of the input space. 
\\Let's consider a random variable (eventually multidimensional) $X$ having probability mass function or probability density function $pdf_{X}(x)$,
which is greater than zero on a set of values $\chi$. Then the expected value of a function $f$ of $X$ is as follows:
\begin{equation}
\begin{matrix}
\mathbb{E}(f(X)) =\sum_{x \in \chi} f(x)pdf_{X}(x) & if X \, discrete \\ 
\\ 
\mathbb{E}(f(X)) =\int_{x \in \chi} f(x)pdf_{X}(x) & \, if X \, \, continuous
\end{matrix}
\end{equation}
Let's now consider to take an $n$-sample of $X$, $(x_{1},...,x_{n})$, and compute the mean of $f(x)$ over the samples. This computation represents the Monte-Carlo estimate:
\begin{equation}
  \mathbb{E}(f(X)) \approx   \widetilde{f}_{n}(x) = \frac{1}{n} \sum_{i=1}^{n} f(x_{i})  
\end{equation}
If $\mathbb{E}(f(X))$ exists, then the law of large numbers determines that for any arbitrarily small $\varepsilon$:
\begin{equation}
  \lim_{n\rightarrow \, \infty} P( \left | \widetilde{f}_{n}(X) - \mathbb{E}(f(X))  \right |\geq \varepsilon) = 0
\end{equation}
The above equation suggests that as $n$ gets larger, then the probability that $\widetilde{f}_{n}(X)$ deviates 
from the $\mathbb{E}(f(X))$ becomes smaller. In other words, more samples are spooned, more closer the Monte-Carlo estimate of $X$ gets to the real value.
\\In addition $\widetilde{f}_{n}(X)$ represent an unbiased estimate for $\mathbb{E}(f(X))$:
\begin{equation}
\mathbb{E}(\widetilde{f}_{n}(X)) = \mathbb{E} \left ( \frac{1}{n} \sum_{i=1}^{n} f(x_{i})   \right ) = 
\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}(f(x_{i})) =   \mathbb{E}(f(X)) 
\end{equation}
%After this brief introduction, it is important to understand how the Monte-Carlo method can be employed 
%for the analysis of Dynamic Stochastic system.
%\\Referencing to the nomenclature defined in section~\ref{sub:mathBackground}, given:
%\begin{equation}
%\frac{\partial  \overline{\theta}^{c}\left ( t \right )}{\partial t}=f\left ( \overline{\theta}^{c},\overline{\theta}^{d}_{i}, \overline{\alpha}_{staz} ,\overline{\alpha}_{brow}, t \right )
%\end{equation}
%let's define a function $g_{i}$ that represents the solution of the previous equation (the trajectory in the $ \overline{\theta}^{c}$ space for a fixed $ \overline{\theta}^{d}_{i}$ and initial condition $\overline{\theta}^{c}_{0}$) :
%\begin{equation}
%  \overline{\theta}^{c}(t) = g_{i}(t,\overline{\theta}^{c}_{0})
%\end{equation}
%The Monte-Carlo analysis is performed as following:
%\begin{enumerate}
%  \item Sample:
%  \begin{itemize}
%    \item $\overline{\alpha}_{staz} $,$\overline{\alpha}_{DS}$, $\overline{t}$ depending on which 
%    approximations are valid (see~\ref{sub:mathBackground});
%    \item The initial conditions $\overline{\theta}^{c}_{0}$,$\overline{\theta}^{d}_{0}$;
%    \item Transition conditions from $W(\overline{\theta}^{d}|\overline{\theta}^{d}_{i},\overline{\theta}^{c},t)$
%  \end{itemize}
%  \item Run the system simulator using the previously sampled values and affected by the intrinsic stochasticity 
%  represented by $\overline{\alpha}_{brow}$;
%  \item Pause the simulation when a transition condition is reached and move from the current $\overline{\theta}^{d}_{0}$ to the new $\overline{\theta}^{d}$ (e.g. $\overline{\theta}^{d}_{1}$);
%  \item Run the simulation as performed in step 3, starting from the new coordinate and pause the simulation when a new transition is reached;
%  \item Repeat steps 3 and 4 until a stopping condition is reached;
%  \item Repeat 1 through 4 for a large number of runs $n$.
%\end{enumerate}
\subsubsection{Monte-Carlo sampling through RAVEN}
\label{subsub:MCexample}
The goals of this section are about learning how to:
 \begin{enumerate}
   \item Set up a simple Monte-Carlo sampling for perturbing the input 
   space of a driven code;
   \item Load the outputs of the code into the RAVEN DataObjects 
   system (HistorySet and PointSet);
   \item Print out what contained in the DataObjects;
   \item Generate plots of the code results.
\end{enumerate}  
In order to accomplish these tasks, the following RAVEN \textbf{Entities} (XML blocks in the input files) need to be defined:
\begin{enumerate}
   \item \textbf{\textit{RunInfo}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <RunInfo>
    <JobName>ChapterVII-I/MonteCarlo</JobName>
    <Sequence>sample,writeHistories</Sequence>
    <WorkingDir>ChapterVII-I/MonteCarlo</WorkingDir>
    <batchSize>12</batchSize>
  </RunInfo>
\end{lstlisting}   
   As reported in section~\ref{sub:EntitiesAndFlow}, the \textit{RunInfo} \textbf{Entity} is intended to set up the analysis 
   that the user wants to perform. In this specific case, two steps (\xmlNode{Sequence}) are going to be sequentially run 
   using 	12 processors (\xmlNode{batchSize}). This means that
   12 instances of the driven code are going to be run simultaneously. 
   Every time a simulation ends, a new one is launched.
   \item \textbf{\textit{Files}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Files>
    <Input name="referenceInput.xml" type="input">referenceInput.xml</Input>
  </Files>
\end{lstlisting}
   Since the driven code uses a single input file, in this section the original input is placed. As detailed in the user manual
   the attribute  \xmlAttr{name} represents the alias that is going to be used in all the other input blocks in order to refer to this file.
   \item \textbf{\textit{Models}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
   <Models>
      <Code name="testModel" subType="GenericCode">
        <executable>
          ../physicalCode/analyticalbateman/AnalyticalDplMain.py
        </executable>
        <clargs arg="python" type="prepend"/>
        <clargs arg="" extension=".xml" type="input"/>
        <clargs arg="" extension=".csv" type="output"/>
        <prepend>python</prepend>
      </Code>
    <Models>
\end{lstlisting}
 As in the previous chapters, the Model here is represented by the 
 \textbf{AnalyticalBateman}, which already dumps its output file in a 
 CSV format (standard format that RAVEN can read). For this reason,
 the \textit{GenericCode} interface is used.
   \item \textbf{\textit{Distributions}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Distributions>
      <Uniform name="sigma">
          <lowerBound>1</lowerBound>
          <upperBound>10</upperBound>
      </Uniform>
      <Uniform name="decayConstant">
          <lowerBound>0.000000005</lowerBound>
          <upperBound>0.000000010</upperBound>
      </Uniform>
  </Distributions>   
\end{lstlisting}
  In the Distributions XML section, the stochastic model for the 
  uncertainties  treated by the Monte-Carlo sampling are reported. In 
  this case two distributions are defined: 
  \begin{itemize}
    \item $sigma \sim \mathbb{U}(1,10)$, used to model the uncertainties 
    associated with  the Model \textit{sigma}(s);
    \item  $decayConstant \sim \mathbb{U}(0.5e-8,1e-8)$,  used to 
    model the uncertainties 
    associated with  the Model \textit{decay constants}.
  \end{itemize}
   \item \textbf{\textit{Samplers}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Samplers>
    <MonteCarlo name="monteCarlo">
        <samplerInit>
            <limit>100</limit>
        </samplerInit>
      <variable name="sigma-A">
        <distribution>sigma</distribution>
      </variable>
      <variable name="decay-A">
        <distribution>decayConstant</distribution>
      </variable>
      <variable name="sigma-B">
          <distribution>sigma</distribution>
      </variable>
      <variable name="decay-B">
          <distribution>decayConstant</distribution>
      </variable>
      <variable name="sigma-C">
          <distribution>sigma</distribution>
      </variable>
      <variable name="decay-C">
          <distribution>decayConstant</distribution>
      </variable>
      <variable name="sigma-D">
          <distribution>sigma</distribution>
      </variable>
      <variable name="decay-D">
          <distribution>decayConstant</distribution>
      </variable>
    </MonteCarlo>
  </Samplers>   
\end{lstlisting}
  In order to employ the Monte-Carlo sampling strategy, a 
  \xmlNode{MonteCarlo} node needs to be inputted. As it can be
  seen from above, a $100-$samples are here requested. The 
  Monte-Carlo method is going to be employed on $8$ Model variables.
  It is worth to notice that all the \textit{decay-\%} and 
  \textit{sigma-\%} variables are associated with the same distributions 
  $decayConstant$ and $sigma$, respectively.  
   \item \textbf{\textit{DataObjects}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <DataObjects>
    <PointSet name="samples">
      <Input>
        sigma-A,sigma-B,sigma-C,sigma-D,
        decay-A,decay-B,decay-C,decay-D
      </Input>
      <Output>A,B,C,D,time</Output>
    </PointSet>
    <HistorySet name="histories">
        <Input>
          sigma-A,sigma-B,sigma-C,sigma-D,
          decay-A,decay-B,decay-C,decay-D
        </Input>
        <Output>A,B,C,D,time</Output>
    </HistorySet>
  </DataObjects>
\end{lstlisting}
  Int this block, two \textit{DataObjects} are defined: 1) PointSet named 
  ``samples'', 2) HistorySet named ``histories''.
  As it can be noticed, in the \xmlNode{Input} node all the uncertainties 
  perturbed through the Monte-Carlo strategy are listed. In this way, any
  realization in the input space is linked to the outputs listed in  the 
  \xmlNode{Output} node.
   \item \textbf{\textit{OutStreamManager}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <OutStreamManager>
    <Print name="samples">
      <type>csv</type>
      <source>samples</source>
    </Print>
    <Print name="histories">
      <type>csv</type>
      <source>histories</source>
    </Print>
    <Plot dim="2" name="historiesPlot" overwrite="false" verbosity="debug">
        <plotSettings>
            <gridSpace>2 2</gridSpace>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|A</y>
                <color>blue</color>
                <gridLocation>
                  <x>0</x>
                  <y>0</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution A(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|B</y>
                <color>red</color>
                <gridLocation>
                    <x>1</x>
                    <y>0</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution B(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|C</y>
                <color>yellow</color>
                <gridLocation>
                    <x>0</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution C(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|D</y>
                <color>black</color>
                <gridLocation>
                    <x>1</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution D(kg)</ylabel>
            </plot>

        </plotSettings>
        <actions>
            <how>png,screen</how>
            <title>
                <text> </text>
            </title>
        </actions>
    </Plot>
    <Plot dim="3" name="samplesPlot3D" overwrite="false" verbosity="debug">
        <plotSettings>
            <gridSpace>2 2</gridSpace>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-A</x>
                <y>samples|Input|decay-A</y>
                <z>samples|Output|A</z>
                <color>blue</color>
                <gridLocation>
                  <x>0</x>
                  <y>0</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final A</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-B</x>
                <y>samples|Input|decay-B</y>
                <z>samples|Output|B</z>
                <color>red</color>
                <gridLocation>
                    <x>1</x>
                    <y>0</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final B</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <type>scatter</type>
                <x>samples|Input|sigma-C</x>
                <y>samples|Input|decay-C</y>
                <z>samples|Output|C</z>
                <color>yellow</color>
                <gridLocation>
                    <x>0</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final C</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-D</x>
                <y>samples|Input|decay-D</y>
                <z>samples|Output|D</z>
                <color>black</color>
                <gridLocation>
                    <x>1</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final D</zlabel>
            </plot>
            <xlabel>sigma</xlabel>
            <ylabel>decay</ylabel>
            <zlabel>final response</zlabel>
        </plotSettings>
        <actions>
            <how>png,screen</how>
            <title>
                <text> </text>
            </title>
        </actions>
    </Plot>
  </OutStreamManager>
\end{lstlisting}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure histories
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/MC_histories.png}
  \caption{Plot of the histories generated by the MC sampling for variables $A,B,C,D$.}
  \label{fig:historiesMCPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  In this block, both the Out-Stream types are constructed: 
  \begin{itemize}
    \item \textit{Print}: 
     \begin{itemize}
       \item named ``samples'' connected with the \textit{DataObjects} \textbf{Entity} ``samples'' 
                (\xmlNode{source})
       \item named ``histories'' connected with the \textit{DataObjects} \textbf{Entity} ``histories'' (\xmlNode{source})          
     \end{itemize}         
      When these objects get used, all the information contained in the 
      linked  \textit{DataObjects} are going 
    to be dumped in CSV files (\xmlNode{type}).
    \item \textit{Plot}: 
    \begin{itemize}
      \item named ``historiesPlot'' connected with the  \textit{DataObjects} 
      \textbf{Entity} ``samples''.  This plot will draw the final state of the
      variables $A,B,C,D$ with respect to the input variables $sigma$(s) 
      and $decay$(s) . 
      \item named ``samplesPlot3D'' connected with the  
      \textit{DataObjects} \textbf{Entity} ``histories''. This plot will draw the 
      evolution of the variables $A,B,C,D$;
    \end{itemize}
     As it can be noticed, both plots are of type \textit{SubPlot}. Four plots
     are going to be placed in each of the figures.
  \end{itemize}   
   \item \textbf{\textit{Steps}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Steps>
    <MultiRun name="sample">
      <Input 	    class="Files" 			 type="input">referenceInput.xml</Input>
      <Model 	    class="Models" 		 type="Code">testModel</Model>
      <Sampler 	class="Samplers" 		 type="MonteCarlo">monteCarlo</Sampler>
      <Output 	class="DataObjects"  type="PointSet">samples</Output>
      <Output 	class="DataObjects"  type="HistorySet">histories</Output>
    </MultiRun>
    <IOStep name="writeHistories" pauseAtEnd="True">
        <Input class="DataObjects" type="HistorySet">histories</Input>
        <Input class="DataObjects" type="PointSet">samples</Input>
        <Output 	class="OutStreamManager" type="Plot">samplesPlot3D</Output>
        <Output 	class="OutStreamManager" type="Plot">historyPlot</Output>
        <Output 	class="OutStreamManager" type="Print">samples</Output>
        <Output 	class="OutStreamManager" type="Print">histories</Output>
    </IOStep>
  </Steps>
\end{lstlisting}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure samples
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/MC_pointsets.png}
  \caption{Plot of the samples generated by the MC sampling for variables $A,B,C,D$.}
  \label{fig:samplesMCPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   Finally, all the previously defined \textbf{Entities} can be combined in 
   the \xmlNode{Steps} block. As inferable, 
   two \xmlNode{Steps} have been inputted:
   \begin{itemize}
     \item \xmlNode{MultiRun} named ``sample'', used to run the multiple  
     instances of the driven code and 
     collect the outputs in the two \textit{DataObjects}. As it can be
     seen, the \xmlNode{Sampler} is inputted to communicate to the 
     \textit{Step} that the driven code needs to
     be perturbed through the Monte-Carlo sampling;
     \item  \xmlNode{IOStep} named ``writeHistories'', used to 1) dump 
     the ``histories'' and ``samples'' \textit{DataObjects} 
     \textbf{Entity} in a CSV file and 2) plot the data in the PNG file and 
     on the screen.
   \end{itemize}
\end{enumerate} 
 As previously mentioned, Figures~\ref{fig:historiesMCPlotLine} and ~\ref{fig:samplesMCPlotLine}  report the evolution of the 
 variables $A,B,C,D$ and their final values, respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%          GRID          %%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Grid}
\label{sub:Grid}
The Grid sampling method (also known as Full Factorial Design of Experiment) represents one of the simplest methodologies that can be employed in order to explore the interaction of multiple random variables with respect
selected FOMs.
In this section, a brief theoretical 
background is reported. In addition,it is shown how to employ this methodology with RAVEN.
\subsubsection{Grid theory introduction}
\label{subsub:Gridtheory}
As already mentioned, the Grid-based sampling strategy is aimed to explore the interaction of multiple random 
variables (i.e. uncertainties) with respect to selected FOMs. Indeed, this kind of method is more aimed toward a 
parametric analysis of the system response rather than a probabilistic one. It consists in discretizing the
domain of the uncertainties in a user-defined number of intervals (see Fig.~\ref{fig:GridDiscretization}) and 
interrogate a physical model (e.g. a system code) on each coordinate (i.e. combination of the uncertainties) of the resulting grid.
\\ This method starts from the assumption that each coordinate on the grid is fully representative, with respect to the FOMs of interest, of the surrounding region. In other words, it is assumed that the response of a system does not significantly change within the hyper-volume surrounding each grid coordinate (red square in Fig.~\ref{fig:GridDiscretization}).
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure history
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/GridDiscretization.png}
  \caption{Example of 2-Dimensional grid discretization. }
  \label{fig:GridDiscretization}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\Similarly to what has been already reported for the Monte-Carlo sampling, let's consider a random variable $X$ having PDF $pdf_{X}(x)$ and, consequentially, CDF $cdf_{X}(x)$ in the domain $\chi$. Then the expected value of a function $f$ of $X$ is as follows:
\begin{equation}
\begin{matrix}
\mathbb{E}(f(X)) =\sum_{x \in \chi} f(x)pdf_{X}(x) & if X \, discrete \\ 
\\ 
\mathbb{E}(f(X)) =\int_{x \in \chi} f(x)pdf_{X}(x) & \, if X \, \, continuous
\end{matrix}
\end{equation}
In the Grid approach, the domain of $X$ is discretized in a finite number of intervals. As already mentioned,
each node of this discretization is representative of the surrounding hyper-volume. This means that a weight 
needs to be associated with each coordinate of the resulting grid:
\begin{equation}
\begin{matrix}
  w_{i}= cdf_{X}(x_{i+1/2}) - cdf_{X}(x_{i-1/2})
\end{matrix}  
\end{equation}
Let's now consider 
to take a $n-$discretization of the domain of  $X$, $(x_{1},...,x_{n})$ and compute the mean of $f(x)$ over the discretization. Based on the previous equation, the computation of the expected value of $f(x)$ is as follows:
\begin{equation}
 \mathbb{E}(f(X)) \approx   \widetilde{f}_{n}(x) = \frac{1}{\sum_{i=1}^{n}w_{i}} \sum_{i=1}^{n} f(x_{i}) \times w_{i}
\end{equation}
If the number of uncertainties under consideration is greater than one ($m$), the above equation
becomes:
\begin{equation}
\mathbb{E}(f(\overline{X})) \approx   \widetilde{f}_{n}(\overline{x}) = \frac{1}{\sum_{i=1}^{n}\prod_{j=1}^{m}w_{i,j}} \sum_{i=1}^{n} f(\overline{x}_{i}) \times \prod_{j=1}^{m}w_{i,j}
\end{equation}
\subsubsection{Grid sampling through RAVEN}
\label{subsub:Gridexample}
The goals of this section are about learning how to:
 \begin{enumerate}
   \item Set up a simple Grid sampling for performing a parametric analysis of a driven code;
   \item Load the outputs of the code into the RAVEN DataObjects system;
   \item Print out what contained in the DataObjects;
   \item Generate basic plots of the code result.
\end{enumerate}  
In order to accomplish these tasks, the following RAVEN \textbf{Entities} (XML blocks in the input files) need to be defined:
\begin{enumerate}
   \item \textbf{\textit{RunInfo}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <RunInfo>
    <JobName>ChapterVII-I/Grid</JobName>
    <Sequence>sample,writeHistories</Sequence>
    <WorkingDir>ChapterVII-I/Grid</WorkingDir>
    <batchSize>12</batchSize>
  </RunInfo>
\end{lstlisting}   
   As reported in section~\ref{sub:EntitiesAndFlow}, the \textit{RunInfo} \textbf{Entity} is intended to set up the analysis 
   that the user wants to perform. In this specific case, two steps (\xmlNode{Sequence}) are going to be sequentially run 
   using 12 processors (\xmlNode{batchSize}). This means that
   12 instances of the driven code are going to be run simultaneously. 
   Every time a simulation ends, a new one is launched.
   \item \textbf{\textit{Files}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Files>
    <Input name="referenceInput.xml" type="input">referenceInput.xml</Input>
  </Files>
\end{lstlisting}
   Since the driven code uses a single input file, in this section the original input is placed. As detailed in the user manual
   the attribute  \xmlAttr{name} represents the alias that is going to be used in all the other input blocks in order to refer to this file.
   \item \textbf{\textit{Models}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
   <Models>
      <Code name="testModel" subType="GenericCode">
        <executable>
          ../physicalCode/analyticalbateman/AnalyticalDplMain.py
        </executable>
        <clargs arg="python" type="prepend"/>
        <clargs arg="" extension=".xml" type="input"/>
        <clargs arg="" extension=".csv" type="output"/>
        <prepend>python</prepend>
      </Code>
    <Models>
\end{lstlisting}
 As in the previous chapters, the Model here is represented by the 
 \textbf{AnalyticalBateman}, which already dumps its output file in a 
 CSV format (standard format that RAVEN can read). For this reason,
 the \textit{GenericCode} interface is used.
   \item \textbf{\textit{Distributions}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Distributions>
      <Uniform name="sigma">
          <lowerBound>1</lowerBound>
          <upperBound>10</upperBound>
      </Uniform>
      <Uniform name="decayConstant">
          <lowerBound>0.000000005</lowerBound>
          <upperBound>0.000000010</upperBound>
      </Uniform>
  </Distributions>   
\end{lstlisting}
  In the Distributions XML section, the stochastic model for the 
  uncertainties  treated by the Grid sampling are reported. In 
  this case two distributions are defined: 
  \begin{itemize}
    \item $sigma \sim \mathbb{U}(1,10)$, used to model the uncertainties 
    associated with  the Model \textit{sigma}(s);
    \item  $decayConstant \sim \mathbb{U}(0.5e-8,1e-8)$,  used to 
    model the uncertainties 
    associated with  the Model \textit{decay constants}.
  \end{itemize}
   \item \textbf{\textit{Samplers}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Samplers>
    <Grid name="grid">
      <variable name="sigma-A">
        <distribution>sigma</distribution>
        <grid construction="equal" steps="1" type="value">2 4.0</grid>
      </variable>
      <variable name="decay-A">
        <distribution>decayConstant</distribution>
        <grid construction="custom" type="value">0.000000005  0.000000008</grid>
      </variable>
      <variable name="sigma-B">
          <distribution>sigma</distribution>
          <grid construction="equal" steps="1" type="CDF">0.1 0.8</grid>
      </variable>
      <variable name="decay-B">
          <distribution>decayConstant</distribution>
          <grid construction="equal" steps="1" type="CDF">0.1 0.8</grid>
      </variable>
      <variable name="sigma-C">
          <distribution>sigma</distribution>
          <grid construction="equal" steps="1" type="value">4 5</grid>
      </variable>
      <variable name="decay-C">
          <distribution>decayConstant</distribution>
          <grid construction="equal" steps="1" type="CDF">0.1 0.5</grid>
      </variable>
      <variable name="sigma-D">
          <distribution>sigma</distribution>
          <grid construction="equal" steps="1" type="CDF">0.4 0.8</grid>
      </variable>
      <variable name="decay-D">
          <distribution>decayConstant</distribution>
          <grid construction="equal" steps="1" type="CDF">0.1 0.8</grid>
      </variable>
    </Grid>
  </Samplers>  
\end{lstlisting}
  In order to employ the Grid sampling strategy, a 
  \xmlNode{Grid} node needs to be inputted. As it can be
  seen from above, in each variable section, the  \xmlNode{grid} is defined. 
  The number of samples finally requested are equal to $n_{samples} = \prod_{i=1}^{n} n_{steps_{i}+1} = 256$.
  It can be noticed that the grid, for each variable, can be defined either in probability (CDF) or in absolute value.  
   \item \textbf{\textit{DataObjects}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <DataObjects>
    <PointSet name="samples">
      <Input>
        sigma-A,sigma-B,sigma-C,sigma-D,
        decay-A,decay-B,decay-C,decay-D
      </Input>
      <Output>A,B,C,D,time</Output>
    </PointSet>
    <HistorySet name="histories">
        <Input>
          sigma-A,sigma-B,sigma-C,sigma-D,
          decay-A,decay-B,decay-C,decay-D
        </Input>
        <Output>A,B,C,D,time</Output>
    </HistorySet>
  </DataObjects>
\end{lstlisting}
  Int this block, two \textit{DataObjects} are defined: 1) PointSet named 
  ``samples'', 2) HistorySet named ``histories''.
  As it can be noticed, in the \xmlNode{Input} node all the variables 
  perturbed through the Grid strategy are listed. In this way, any
  realization in the input space is linked to the outputs listed in  the 
  \xmlNode{Output} node.
   \item \textbf{\textit{OutStreamManager}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <OutStreamManager>
    <Print name="samples">
      <type>csv</type>
      <source>samples</source>
    </Print>
    <Print name="histories">
      <type>csv</type>
      <source>histories</source>
    </Print>
    <Plot dim="2" name="historiesPlot" overwrite="false" verbosity="debug">
        <plotSettings>
            <gridSpace>2 2</gridSpace>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|A</y>
                <color>blue</color>
                <gridLocation>
                  <x>0</x>
                  <y>0</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution A(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|B</y>
                <color>red</color>
                <gridLocation>
                    <x>1</x>
                    <y>0</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution B(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|C</y>
                <color>yellow</color>
                <gridLocation>
                    <x>0</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution C(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|D</y>
                <color>black</color>
                <gridLocation>
                    <x>1</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution D(kg)</ylabel>
            </plot>
        </plotSettings>
        <actions>
            <how>png,screen</how>
            <title>
                <text> </text>
            </title>
        </actions>
    </Plot>
    <Plot dim="3" name="samplesPlot3D" overwrite="false" verbosity="debug">
        <plotSettings>
            <gridSpace>2 2</gridSpace>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-A</x>
                <y>samples|Input|decay-A</y>
                <z>samples|Output|A</z>
                <color>blue</color>
                <gridLocation>
                  <x>0</x>
                  <y>0</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final A</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-B</x>
                <y>samples|Input|decay-B</y>
                <z>samples|Output|B</z>
                <color>red</color>
                <gridLocation>
                    <x>1</x>
                    <y>0</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final B</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <type>scatter</type>
                <x>samples|Input|sigma-C</x>
                <y>samples|Input|decay-C</y>
                <z>samples|Output|C</z>
                <color>yellow</color>
                <gridLocation>
                    <x>0</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final C</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-D</x>
                <y>samples|Input|decay-D</y>
                <z>samples|Output|D</z>
                <color>black</color>
                <gridLocation>
                    <x>1</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final D</zlabel>
            </plot>
            <xlabel>sigma</xlabel>
            <ylabel>decay</ylabel>
            <zlabel>final response</zlabel>
        </plotSettings>
        <actions>
            <how>png,screen</how>
            <title>
                <text> </text>
            </title>
        </actions>
    </Plot>
  </OutStreamManager>
\end{lstlisting}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure histories
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/Grid_histories.png}
  \caption{Plot of the histories generated by the Grid sampling for variables $A,B,C,D$.}
  \label{fig:historiesGridPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  In this block, both the Out-Stream types are constructed: 
  \begin{itemize}
    \item \textit{Print}: 
     \begin{itemize}
       \item named ``samples'' connected with the \textit{DataObjects} \textbf{Entity} ``samples'' 
                (\xmlNode{source})
       \item named ``histories'' connected with the \textit{DataObjects} \textbf{Entity} ``histories'' (\xmlNode{source})          
     \end{itemize}         
      When these objects get used, all the information contained in the 
      linked  \textit{DataObjects} are going 
    to be dumped in CSV files (\xmlNode{type}).
    \item \textit{Plot}: 
    \begin{itemize}
      \item named ``historiesPlot'' connected with the  \textit{DataObjects} 
      \textbf{Entity} ``samples''.  This plot will draw the final state of the
      variables $A,B,C,D$ with respect to the input variables $sigma$(s) 
      and $decay$(s) . 
      \item named ``samplesPlot3D'' connected with the  
      \textit{DataObjects} \textbf{Entity} ``histories''. This plot will draw the 
      evolution of the variables $A,B,C,D$;
    \end{itemize}
     As it can be noticed, both plots are of type \textit{SubPlot}. Four plots
     are going to be placed in each of the figures.
  \end{itemize}   
   \item \textbf{\textit{Steps}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Steps>
    <MultiRun name="sample">
      <Input 	    class="Files" 			 type="input">referenceInput.xml</Input>
      <Model 	    class="Models" 		 type="Code">testModel</Model>
      <Sampler 	class="Samplers" 		 type="Grid">grid</Sampler>
      <Output 	class="DataObjects"  type="PointSet">samples</Output>
      <Output 	class="DataObjects"  type="HistorySet">histories</Output>
    </MultiRun>
    <IOStep name="writeHistories" pauseAtEnd="True">
        <Input class="DataObjects" type="HistorySet">histories</Input>
        <Input class="DataObjects" type="PointSet">samples</Input>
        <Output 	class="OutStreamManager" type="Plot">samplesPlot3D</Output>
        <Output 	class="OutStreamManager" type="Plot">historyPlot</Output>
        <Output 	class="OutStreamManager" type="Print">samples</Output>
        <Output 	class="OutStreamManager" type="Print">histories</Output>
    </IOStep>
  </Steps>
\end{lstlisting}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure samples
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/Grid_pointsets.png}
  \caption{Plot of the samples generated by the Grid sampling for variables $A,B,C,D$.}
  \label{fig:samplesGridPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   Finally, all the previously defined \textbf{Entities} can be combined in 
   the \xmlNode{Steps} block. As inferable, 
   two \xmlNode{Steps} have been inputted:
   \begin{itemize}
     \item \xmlNode{MultiRun} named ``sample'', used to run the multiple  
     instances of the driven code and 
     collect the outputs in the two \textit{DataObjects}. As it can be
     seen, the \xmlNode{Sampler} is inputted to communicate to the 
     \textit{Step} that the driven code needs to
     be perturbed through the Grid sampling;
     \item  \xmlNode{IOStep} named ``writeHistories'', used to 1) dump 
     the ``histories'' and ``samples'' \textit{DataObjects} 
     \textbf{Entity} in a CSV file and 2) plot the data in the PNG file and 
     on the screen.
   \end{itemize}
\end{enumerate} 
 As previously mentioned, Figures~\ref{fig:historiesGridPlotLine} and ~\ref{fig:samplesGridPlotLine}  report the evolution of the 
 variables $A,B,C,D$ and their final values, respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%    STRATIFIED    %%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stratified}
\label{sub:Stratified}
The Stratified sampling is a class of methods that relies on the assumption that the input space (i.e. uncertainties) 
can be separated in regions (strata) based on similarity of the response of the system for input set within the
same strata. Following this assumption, the most rewarding (in terms of computational cost vs. knowledge gain) 
sampling strategy would be to place one sample for each region. In this way, the same information is not 
collected more than once and all the prototypical behavior are sampled at least once. In 
Fig.~\ref{fig:StratifiedSamplingExample}, the Stratified sampling approach is exemplified. 
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.55]{pics/StratifiedSamplingExample.png}
  \caption{Example of Stratified sampling approach.}
  \label{fig:StratifiedSamplingExample}
 \end{figure}
\\In this section, a brief theoretical 
background is reported. In addition,it is shown how to employ this methodology with RAVEN.
\subsubsection{Stratified theory introduction}
\label{subsub:Stratifiedtheory}
As already mentioned, the Stratified sampling approach is a method for the exploration of the input space that consists of dividing the uncertain domain into subgroups before sampling. In the Stratified sampling, these subgroups must be:
\begin{itemize}
  \item mutually exclusive: every element in the population must be assigned to only one stratum (subgroup);
  \item collectively exhaustive: no population element can be excluded.
\end{itemize}
Then simple random sampling or systematic sampling is applied within each stratum. It is worthwhile to note that the well-known Latin Hyper-Cube sampling represents a specialized version of the stratified approach, when the domain strata are constructed in equally-probable CDF bins.
\\Similarly to what has been already reported for the Grid sampling, let's consider a set of  $m$ random variables $X_{j}, \, j=1,..,m$ having PDFs $pdf_{X_{j}}(x_{j})$ and, consequentially, CDF $cdf_{X_{j}}(x_{j})$ in the domain $\chi_{j}$. Then the expected value of a function $f$ of $X_{j}, \, j=1,..,m$ is as follows:
\begin{equation}
\begin{matrix}
\mathbb{E}(f(\overline{X})) =\sum f(x)   \prod_{j=1}^{m} pdf_{X_{j}}(x_{j}) & if \overline{X} \, discrete \\ 
\\ 
\mathbb{E}(f(\overline{X})) =\int f(x)\prod_{j=1}^{m} pdf_{X_{j}}(x_{j}) & \, if \overline{X} \, \, continuous
\end{matrix}
\end{equation}
In the Stratified approach, the domain of $\overline{X}$ is discretized in a set of strata. Consequentially, similarly to the Grid sampling, a weight needs to be associated with each realization in the input space:
\begin{equation}
\begin{matrix}
  w_{i}= \frac{\prod_{j=1}^{m} \left [  cdf_{X_{j}}(x_{i,j+1}) - cdf_{X_{j}}(x_{i,j}) \right ]}{\sum_{points}\prod_{j=1}^{m} \left [  cdf_{X_{j}}(x_{i,j+1}) - cdf_{X_{j}}(x_{i,j}) \right ]}
\end{matrix}  
\end{equation}
Each realization carries a weight that is representative of each stratum.
\\Let's now consider 
to take a $n-$strata of the domain of  $\overline{X}$, and compute the expected value of $f(x)$ over the discretization. Based on the previous equation, the computation of the expected value of $f(x)$ is as follows:
\begin{equation}
 \mathbb{E}(f(\overline{X})) \approx   \widetilde{f}_{n}(\overline{x}) = \frac{1}{\sum_{i=1}^{n}w_{i}} \sum_{i=1}^{n} f(\overline{x}_{i}) \times w_{i}
\end{equation}
\subsubsection{Stratified sampling through RAVEN}
\label{subsub:Stratifiedexample}
The goals of this section are about learning how to:
 \begin{enumerate}
   \item Set up a simple Stratified sampling for performing a parametric analysis of a driven code;
   \item Load the outputs of the code into the RAVEN DataObjects system;
   \item Print out what contained in the DataObjects;
   \item Generate basic plots of the code result.
\end{enumerate}  
In order to accomplish these tasks, the following RAVEN \textbf{Entities} (XML blocks in the input files) need to be defined:
\begin{enumerate}
   \item \textbf{\textit{RunInfo}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <RunInfo>
    <JobName>ChapterVII-I/Stratified</JobName>
    <Sequence>sample,writeHistories</Sequence>
    <WorkingDir>ChapterVII-I/Stratified</WorkingDir>
    <batchSize>12</batchSize>
  </RunInfo>
\end{lstlisting}   
   As reported in section~\ref{sub:EntitiesAndFlow}, the \textit{RunInfo} \textbf{Entity} is intended to set up the analysis 
   that the user wants to perform. In this specific case, two steps (\xmlNode{Sequence}) are going to be sequentially run 
   using 12 processors (\xmlNode{batchSize}). This means that
   12 instances of the driven code are going to be run simultaneously. 
   Every time a simulation ends, a new one is launched.
   \item \textbf{\textit{Files}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Files>
    <Input name="referenceInput.xml" type="input">referenceInput.xml</Input>
  </Files>
\end{lstlisting}
   Since the driven code uses a single input file, in this section the original input is placed. As detailed in the user manual
   the attribute  \xmlAttr{name} represents the alias that is going to be used in all the other input blocks in order to refer to this file.
   \item \textbf{\textit{Models}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
   <Models>
      <Code name="testModel" subType="GenericCode">
        <executable>
          ../physicalCode/analyticalbateman/AnalyticalDplMain.py
        </executable>
        <clargs arg="python" type="prepend"/>
        <clargs arg="" extension=".xml" type="input"/>
        <clargs arg="" extension=".csv" type="output"/>
        <prepend>python</prepend>
      </Code>
    <Models>
\end{lstlisting}
 As in the previous chapters, the Model here is represented by the 
 \textbf{AnalyticalBateman}, which already dumps its output file in a 
 CSV format (standard format that RAVEN can read). For this reason,
 the \textit{GenericCode} interface is used.
   \item \textbf{\textit{Distributions}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Distributions>
      <Uniform name="sigma">
          <lowerBound>1</lowerBound>
          <upperBound>10</upperBound>
      </Uniform>
      <Uniform name="decayConstant">
          <lowerBound>0.000000005</lowerBound>
          <upperBound>0.000000010</upperBound>
      </Uniform>
  </Distributions>   
\end{lstlisting}
  In the Distributions XML section, the stochastic model for the 
  uncertainties  treated by the Stratified sampling are reported. In 
  this case two distributions are defined: 
  \begin{itemize}
    \item $sigma \sim \mathbb{U}(1,10)$, used to model the uncertainties 
    associated with  the Model \textit{sigma}(s);
    \item  $decayConstant \sim \mathbb{U}(0.5e-8,1e-8)$,  used to 
    model the uncertainties 
    associated with  the Model \textit{decay constants}.
  \end{itemize}
   \item \textbf{\textit{Samplers}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
    <Stratified name="stratified">
      <variable name="sigma-A">
        <distribution>sigma</distribution>
        <grid construction="equal" steps="100" type="value">2 4.0</grid>
      </variable>
      <variable name="decay-A">
        <distribution>decayConstant</distribution>
        <grid construction="equal" steps="100" type="value">0.000000005 0.000000008</grid>
      </variable>
      <variable name="sigma-B">
          <distribution>sigma</distribution>
          <grid construction="equal" steps="100" type="CDF">0.1 0.8</grid>
      </variable>
      <variable name="decay-B">
          <distribution>decayConstant</distribution>
          <grid construction="equal" steps="100" type="CDF">0.1 0.8</grid>
      </variable>
      <variable name="sigma-C">
          <distribution>sigma</distribution>
          <grid construction="equal" steps="100" type="value">1.0 5</grid>
      </variable>
      <variable name="decay-C">
          <distribution>decayConstant</distribution>
          <grid construction="equal" steps="100" type="CDF">0.1 0.5</grid>
      </variable>
      <variable name="sigma-D">
          <distribution>sigma</distribution>
          <grid construction="equal" steps="100" type="CDF">0.4 0.8</grid>
      </variable>
      <variable name="decay-D">
          <distribution>decayConstant</distribution>
          <grid construction="equal" steps="100" type="CDF">0.1 0.8</grid>
      </variable>
    </Stratified> 
\end{lstlisting}
  In order to employ the Stratified sampling strategy, a 
  \xmlNode{Stratified} node needs to be inputted. As it can be
  seen from above, in each variable section, the  \xmlNode{grid} is defined. 
  It is important to mention that the number of \xmlAttr{steps} needs to be the same in each of the variables,
  since, as reported in previous section, the Stratified sampling strategy it discretize the domain in strata. 
  The number of samples finally requested are equal to $n_{samples} = n_{steps} = 100$.
  It is worth to be noticed that if grid for each variables is defined in CDF and of  \xmlAttr{type} = ``equal'', the Stratified
  sampling corresponds to the well-known Latin Hyper Cube sampling.
   \item \textbf{\textit{DataObjects}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <DataObjects>
    <PointSet name="samples">
      <Input>
        sigma-A,sigma-B,sigma-C,sigma-D,
        decay-A,decay-B,decay-C,decay-D
      </Input>
      <Output>A,B,C,D,time</Output>
    </PointSet>
    <HistorySet name="histories">
        <Input>
          sigma-A,sigma-B,sigma-C,sigma-D,
          decay-A,decay-B,decay-C,decay-D
        </Input>
        <Output>A,B,C,D,time</Output>
    </HistorySet>
  </DataObjects>
\end{lstlisting}
  Int this block, two \textit{DataObjects} are defined: 1) PointSet named 
  ``samples'', 2) HistorySet named ``histories''.
  As it can be noticed, in the \xmlNode{Input} node all the variables 
  perturbed through the Stratified strategy are listed. In this way, any
  realization in the input space is linked to the outputs listed in  the 
  \xmlNode{Output} node.
   \item \textbf{\textit{OutStreamManager}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <OutStreamManager>
    <Print name="samples">
      <type>csv</type>
      <source>samples</source>
    </Print>
    <Print name="histories">
      <type>csv</type>
      <source>histories</source>
    </Print>
    <Plot dim="2" name="historiesPlot" overwrite="false" verbosity="debug">
        <plotSettings>
            <gridSpace>2 2</gridSpace>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|A</y>
                <color>blue</color>
                <gridLocation>
                  <x>0</x>
                  <y>0</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution A(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|B</y>
                <color>red</color>
                <gridLocation>
                    <x>1</x>
                    <y>0</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution B(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|C</y>
                <color>yellow</color>
                <gridLocation>
                    <x>0</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution C(kg)</ylabel>
            </plot>
            <plot>
                <type>line</type>
                <x>histories|Output|time</x>
                <y>histories|Output|D</y>
                <color>black</color>
                <gridLocation>
                    <x>1</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>time (s)</xlabel>
                <ylabel>evolution D(kg)</ylabel>
            </plot>
        </plotSettings>
        <actions>
            <how>png,screen</how>
            <title>
                <text> </text>
            </title>
        </actions>
    </Plot>
    <Plot dim="3" name="samplesPlot3D" overwrite="false" verbosity="debug">
        <plotSettings>
            <gridSpace>2 2</gridSpace>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-A</x>
                <y>samples|Input|decay-A</y>
                <z>samples|Output|A</z>
                <color>blue</color>
                <gridLocation>
                  <x>0</x>
                  <y>0</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final A</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-B</x>
                <y>samples|Input|decay-B</y>
                <z>samples|Output|B</z>
                <color>red</color>
                <gridLocation>
                    <x>1</x>
                    <y>0</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final B</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <type>scatter</type>
                <x>samples|Input|sigma-C</x>
                <y>samples|Input|decay-C</y>
                <z>samples|Output|C</z>
                <color>yellow</color>
                <gridLocation>
                    <x>0</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final C</zlabel>
            </plot>
            <plot>
                <type>scatter</type>
                <x>samples|Input|sigma-D</x>
                <y>samples|Input|decay-D</y>
                <z>samples|Output|D</z>
                <color>black</color>
                <gridLocation>
                    <x>1</x>
                    <y>1</y>
                </gridLocation>
                <xlabel>sigma</xlabel>
                <ylabel>decay</ylabel>
                <zlabel>final D</zlabel>
            </plot>
            <xlabel>sigma</xlabel>
            <ylabel>decay</ylabel>
            <zlabel>final response</zlabel>
        </plotSettings>
        <actions>
            <how>png,screen</how>
            <title>
                <text> </text>
            </title>
        </actions>
    </Plot>
  </OutStreamManager>
\end{lstlisting}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure histories
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/Stratified_histories.png}
  \caption{Plot of the histories generated by the Stratified sampling for variables $A,B,C,D$.}
  \label{fig:historiesStratifiedPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  In this block, both the Out-Stream types are constructed: 
  \begin{itemize}
    \item \textit{Print}: 
     \begin{itemize}
       \item named ``samples'' connected with the \textit{DataObjects} \textbf{Entity} ``samples'' 
                (\xmlNode{source})
       \item named ``histories'' connected with the \textit{DataObjects} \textbf{Entity} ``histories'' (\xmlNode{source})          
     \end{itemize}         
      When these objects get used, all the information contained in the 
      linked  \textit{DataObjects} are going 
    to be dumped in CSV files (\xmlNode{type}).
    \item \textit{Plot}: 
    \begin{itemize}
      \item named ``historiesPlot'' connected with the  \textit{DataObjects} 
      \textbf{Entity} ``samples''.  This plot will draw the final state of the
      variables $A,B,C,D$ with respect to the input variables $sigma$(s) 
      and $decay$(s) . 
      \item named ``samplesPlot3D'' connected with the  
      \textit{DataObjects} \textbf{Entity} ``histories''. This plot will draw the 
      evolution of the variables $A,B,C,D$;
    \end{itemize}
     As it can be noticed, both plots are of type \textit{SubPlot}. Four plots
     are going to be placed in each of the figures.
  \end{itemize}   
   \item \textbf{\textit{Steps}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Steps>
    <MultiRun name="sample">
      <Input 	    class="Files" 			 type="input">referenceInput.xml</Input>
      <Model 	    class="Models" 		 type="Code">testModel</Model>
      <Sampler 	class="Samplers" 		 type="Stratified">stratified</Sampler>
      <Output 	class="DataObjects"  type="PointSet">samples</Output>
      <Output 	class="DataObjects"  type="HistorySet">histories</Output>
    </MultiRun>
    <IOStep name="writeHistories" pauseAtEnd="True">
        <Input class="DataObjects" type="HistorySet">histories</Input>
        <Input class="DataObjects" type="PointSet">samples</Input>
        <Output 	class="OutStreamManager" type="Plot">samplesPlot3D</Output>
        <Output 	class="OutStreamManager" type="Plot">historyPlot</Output>
        <Output 	class="OutStreamManager" type="Print">samples</Output>
        <Output 	class="OutStreamManager" type="Print">histories</Output>
    </IOStep>
  </Steps>
\end{lstlisting}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure samples
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/Stratified_pointsets.png}
  \caption{Plot of the samples generated by the Stratified sampling for variables $A,B,C,D$.}
  \label{fig:samplesStratifiedPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   Finally, all the previously defined \textbf{Entities} can be combined in 
   the \xmlNode{Steps} block. As inferable, 
   two \xmlNode{Steps} have been inputted:
   \begin{itemize}
     \item \xmlNode{MultiRun} named ``sample'', used to run the multiple  
     instances of the driven code and 
     collect the outputs in the two \textit{DataObjects}. As it can be
     seen, the \xmlNode{Sampler} is inputted to communicate to the 
     \textit{Step} that the driven code needs to
     be perturbed through the Stratified sampling;
     \item  \xmlNode{IOStep} named ``writeHistories'', used to 1) dump 
     the ``histories'' and ``samples'' \textit{DataObjects} 
     \textbf{Entity} in a CSV file and 2) plot the data in the PNG file and 
     on the screen.
   \end{itemize}
\end{enumerate} 
 As previously mentioned, Figures~\ref{fig:historiesStratifiedPlotLine} and ~\ref{fig:samplesStratifiedPlotLine}  report the evolution of the 
 variables $A,B,C,D$ and their final values, respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Sparse Grid Collocation}
\label{sub:Stratified}
The Sparse Grid Collocation sampler represents an advanced methodology to perform Uncertainty Quantification. It is aimed
to explore the input space leveraging the information contained in the associated probability density functions. It builds on generic Grid sampling by selecting evaluation points based on characteristic quadratures as part of stochastic collocation for generalized polynomial chaos uncertainty quantification. In collocation an N-dimensional grid is constructed, with each uncertain variable providing an axis. Along each axis, the points of evaluation correspond to quadrature points necessary to integrate polynomials. In the simplest (and most naive) case, a N-Dimensional tensor product of all possible combinations of points from each dimensionâ€™s quadrature is constructed as sampling points. The number of necessary samples can be reduced by employing Smolyak-like sparse grid algorithms, which use reduced combinations of polynomial orders to reduce the necessary sampling space.
\\In this section, a brief theoretical 
background is reported. In addition,it is shown how to employ this methodology with RAVEN.
\subsubsection{Sparse Grid Collocation theory introduction}
\label{subsub:SGctheory}
\paragraph{Generalized Polynomial Chaos}
In general, polynomial chaos expansion (PCE) methods seek to interpolate the simulation code as a combination of
polynomials of varying degree in each dimension of the input space.  Originally Wiener
proposed expanding in Hermite polynomials for Gaussian-normal distributed variables \cite{wiener}.  Askey and
Wilson generalized Hermite polynomials to include Jacobi polynomials, including Legendre and Laguerre
polynomials \cite{Wiener-Askey}.  Xiu and Karniadakis combines these concepts to perform PCE for a range of Gaussian-based
distributions with corresponding polynomials,
including Legendre polynomials for uniform distributions, Laguerre polynomials for Gamma distributions, and
Jacobi polynomials for Beta distributions \cite{xiu}.

In each of these cases, a probability-weighted
integral over the distribution can be cast in a way that the corresponding polynomials are orthogonal over the
same weight and interval.  These chaos Wiener-Askey polynomials were used by Xiu and Karniadakis to develop
the generalized polynomial chaos expansion method (gPC), including a transformation for applying the same
method to arbitrary distributions (as long as they have a known inverse CDF) \cite{xiu}.  Two significant
methodologies have grown from gPC application.  The first makes use of Lagrange polynomials to expand the
original function or simulation code, as they can be made orthogonal over the same domain as the
distributions \cite{SCLagrange}; the other uses the Wiener-Askey polynomials \cite{xiu}. 

Let's consider a simulation code that produces a quantity of interest $u$ as a function $u(Y)$ whose arguments are
the uncertain, distributed input
parameters $Y=(Y_1,\ldots,Y_n,\ldots,Y_N)$.  A particular realization $\omega$ of $Y_n$ is expressed by
$Y_n(\omega)$, and a single realization of the entire input space results in a solution to the function as
$u(Y(\omega))$. Obtaining a realization of $u(Y)$ may take considerable computation time and
effort.
$u(Y)$ gets expanded in orthonormal multidimensional polynomials $\Phi_k(Y)$, where $k$ is a multi-index tracking
the polynomial order in each axis of the polynomial Hilbert space, and $\Phi_k(Y)$ is constructed as
\begin{equation}\label{eq:gPC}
  \Phi_k(Y) = \prod_{n=1}^N \phi_{k_n}(Y_n),
\end{equation}
where $\phi_{k_n}(Y_n)$ is a single-dimension Wiener-Askey orthonormal polynomial of order $k_n$ and
$k=(k_1,\ldots,k_n,\ldots,k_N)$, $k_n\in\mathbb{N}^0$.  For example, given $u(y_1,y_2,y_3)$, $k=(2,1,4)$ 
is the multi-index of the
product of a second-order polynomial in $y_1$, a first-order polynomial in $y_2$, and a fourth-order
polynomial in $y_4$. The gPC for $u(Y)$ using this notation is
\begin{equation}
  u(Y) \approx \sum_{k\in\Lambda(L)} u_k\Phi_k(Y),
\end{equation}
where $u_k$ is a scalar weighting polynomial coefficient. The polynomials used in the expansion are determined
by the set of multi-indices $\Lambda(L)$, where $L$ is a truncation order.  In the limit
that $\Lambda$ contains all possible combinations of polynomials of any order, Eq. \ref{eq:gPC} is exact.
Practically, however, $\Lambda$ is truncated to some finite set of combinations, discussed in section
\ref{sec:indexSets}.

Using the orthonormal properties of the Wiener-Askey polynomials,
\begin{equation}
  \int_\Omega \Phi_k(Y)\Phi_{\hat k}(Y) \rho(Y) dY = \delta_{k\hat k},
\end{equation}
where $\rho(Y)$ is the combined PDF of $Y$, $\Omega$ is the multidimensional domain of $Y$, and $\delta_{nm}$
is the Dirac delta, we can isolate an expression of the polynomial expansion coefficients.
We multiply both sides of Eq. \ref{eq:gPC} by
$\Phi_{\hat k}(Y)$, integrate both sides over the probability-weighted input domain, and sum over all $\hat k$
to obtain the coefficients, sometimes referred to as polynomial expansion moments,
\begin{align}\label{eq:polycoeff}
  u_k &= \frac{\langle u(Y)\Phi_k(Y) \rangle}{\langle \Phi_k(Y)^2 \rangle},\\
      &= \langle u(Y)\Phi_k(Y) \rangle,
\end{align}
where we use the angled bracket notation to denote the probability-weighted inner product,
\begin{equation}
  \langle f(Y) \rangle \equiv \int_\Omega f(Y)\rho(Y) dY.
\end{equation}
When $u(Y)$ has an analytic form, these coefficients can be solved by integration; however, in general other
methods must be applied to numerically perform the integral.  While tools such as Monte Carlo integration can
be used to evaluate the integral, we can harness the properties of Gaussian quadratures because of the
probability weights and domain.  This stochastic collocation method is discussed in section \ref{sec:stoch
coll}.
\paragraph{Polynomial Index Set Construction}\label{sec:index sets}
The main concern in expanding a function in interpolating multidimensional polynomials is choosing appropriate polynomials to
make up the expansion.
There are many generic ways by which a polynomial set can be constructed.  Here  three static
approaches are presented: 
\begin{itemize}
  \item tensor product;
  \item total degree;
  \item hyperbolic cross.
\end{itemize}
In the nominal tensor
product case, $\Lambda(L)$ contains all possible combinations of polynomial indices up to truncation order $L$ in each
dimension, as
\begin{equation}
  \Lambda_\text{TP}(L)=\Big\{\bar p=(p_1,\cdots,p_N): \max_{1\leq n\leq N}p_n\leq L
\Big\}.
\end{equation}
The cardinality of this index set is $|\Lambda_\text{TP}(L)|=(L+1)^N$. For example, for a two-dimensional
input space ($N$=2) and truncation limit $L=3$, the index set $\Lambda_\text{TP}(3)$ is given in Table
\ref{tab:TP}, where the notation $(1,2)$ signifies the product of a polynomial that is first order in $Y_1$
and second order in $Y_2$.

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (3,0) & (3,1) & (3,2) & (3,3) \\
    (2,0) & (2,1) & (2,2) & (2,3) \\
    (1,0) & (1,1) & (1,2) & (1,3) \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Tensor Product Index Set, $N=2,L=3$}
  \label{tab:TP}
\end{table}

It is evident there is some inefficiencies in this index set.  First, it suffers dramatically from the
\emph{curse of dimensionality}; that is, the number of polynomials required grows exponentially with
increasing dimensions.  Second, the total order of polynomials is not considered.  Assuming the contribution of
each higher-order polynomial is smaller than lower-order polynomials, the (3,3) term is
contributing sixth-order corrections that are likely smaller than the error introduced by ignoring
fourth-order corrections (4,0) and (0,4).  This leads to the development of the \emph{total degree} (TD) and
\emph{hyperbolic cross} (HC) polynomial index set construction strategies \cite{hctd}.

In TD, only multidimensional polynomials whose \emph{total} order at most $L$ are permitted,
\begin{equation}
  \Lambda_\text{TD}(L)=\Big\{\bar p=(p_1,\cdots,p_N):\sum_{n=1}^N p_n \leq L
\Big\}.
\end{equation}
The cardinality of this index set is $|\Lambda_\text{TD}(L)|={L+N\choose N}$, which grows with increasing
dimensions much more slowly than TP.  For the same $N=2,L=3$ case above, the TD index set is given in Table
\ref{tab:TD}. 

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (3,0) &       &       &       \\
    (2,0) & (2,1) &       &       \\
    (1,0) & (1,1) & (1,2) &       \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Total Degree Index Set, $N=2,L=3$}
  \label{tab:TD}
\end{table}

In HC, the \emph{product} of polynomial orders is used to restrict allowed polynomials in the index set.  This
tends to polarize the expansion, emphasizing higher-order polynomials in each dimension but lower-order
polynomials in combinations of dimensions, as
\begin{equation}
  \Lambda_\text{HC}(L)=\Big\{\bar p=(p_1,\ldots,p_N):\prod_{n=1}^N p_n+1 \leq L+1
\Big\}.
\end{equation}
The cardinality of this index set is bounded by $|\Lambda_\text{HC}(L)|\leq (L+1)(1+\log(L+1))^{N-1}$. It
grows even more slowly than TD with increasing dimension, as shown in Table \ref{tab:HC} for $N=2,L=3$.

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (3,0) &       &       &       \\
    (2,0) &       &       &       \\
    (1,0) & (1,1) &       &       \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Hyperbolic Cross Index Set, $N=2,L=3$}
  \label{tab:HC}
\end{table}

It has been shown that the effectiveness of TD and HC as index set choices depends strongly on the regularity
of the responce \cite{hctd}.  TD tends to be most effective for infinitely-continuous response surfaces,
while HC is more effective for surfaces with limited smoothness or discontinuities.

\paragraph{Anisotropy}
While using TD or HC to construct the polynomial index set combats the curse of dimensionality present in TP,
it is not eliminated and continues to be an issue for problems of large dimensionality.  Another method that can
be applied to mitigate this issue is index set anisotropy, or the unequal treatment of various dimensions.
In this strategy, weighting factors $\alpha=(\alpha_1,\ldots,\alpha_n,\ldots,\alpha_N)$ are applied in each
dimension to allow additional polynomials in some dimensions and less in others.  This change adjusts the TD
and HC construction rules as follows, where $|\alpha|_1$ is the one-norm of $\alpha$.
\begin{equation}
  \tilde\Lambda_{TD}(L)=\Big\{\bar p=(p_1,\ldots,p_N):\sum_{n=1}^{N} \alpha_n p_{n} \leq |\vec\alpha|_1 L\Big\},
\end{equation}
agagaag
\begin{equation}
  \tilde\Lambda_\text{HC}(L)=\Big\{\bar p=(p_1,\cdots,p_N):\prod_{n=1}^N (p_n+1)^{\alpha_n} \leq
  (L+1)^{|\vec\alpha|_1} \Big\}
\end{equation}
As it is desirable to obtain the isotropic case from a reduction of the anisotropic cases, let's define the
one-norm for the weights as
\begin{equation}
  |\alpha|_1 = \frac{\sum_{n=1}^N \alpha_n}{N}.
\end{equation}
Considering the same case above ($N=2,L=3$), it can be applied weights $\alpha_1=5,\alpha_2=3$, and the resulting index
sets are Tables \ref{tab:aniTD} (TD) and \ref{tab:aniHC} (HC).

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c c}
    (2,0) &       &       &       & \\
    (1,0) & (1,1) & (1,2) &       & \\
    (0,0) & (0,1) & (0,2) & (0,3) & (0,4)
  \end{tabular}
  \caption{Anisotropic Total Degree Index Set, $N=2,L=3$}
  \label{tab:aniTD}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (1,0) &       &       &       \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Anisotropic Hyperbolic Cross Index Set, $N=2,L=3$}
  \label{tab:aniHC}
\end{table}

There are many methods by which anisotropy weights can be assigned.  Often, if a problem is well-known to an 
analyst, it may be enough to use heuristics to assign importance arbitrarily.  Otherwise, a smaller
uncertainty quantification solve can be used to roughly determine sensitivity coefficients (such as Pearson
coefficients), and the inverse of those can then be applied as anisotropy weights.  Sobol coefficients
obtained from first- or second-order HDMR, an additional sampling strategy present in RAVEN, could also serve as a basis for these weights.
A good choice of anisotropy weight can greatly speed up convergence; however, a
poor choice can slow convergence considerably, as computational resources are used to resolve low-importance
dimensions.

\paragraph{Stochastic Collocation}\label{sec:stoch coll}
Stochastic collocation is the process of using collocated points to approximate integrals of stochastic space
numerically.  In particular let's consider using Gaussian quadratures (Legendre, Hermite, Laguerre, and Jacobi)
corresponding to the polynomial expansion polynomials for numerical integration.  Quadrature integration takes
the form
\begin{align}
  \int_a^b f(x)\rho(x) &= \sum_{\ell=1}^\infty w_\ell f(x_\ell),\\
  &\approx \sum_{\ell=1}^{\hat L} w_\ell f(x_\ell),
\end{align}
where $w_\ell,x_\ell$ are corresponding points and weights belonging to the quadrature set, truncated at order
$\hat L$.  At this point, this $\hat L$ should not be confused with the polynomial expansion truncation order $L$.  This expression can be simplified using the operator notation
\begin{equation}\label{eq:quad op}
  q^{(\hat L)}[f(x)] \equiv \sum_{\ell=1}^{\hat L} w_\ell f(x_\ell).
\end{equation}
A nominal multidimensional quadrature is the tensor product of
individual quadrature weights and points, and can be written
\begin{align}
  Q^{(\vec{L})} &= q^{(\hat L_1)}_1 \otimes q^{(\hat L_2)}_2 \otimes \cdots,\\
                     &= \bigotimes_{n=1}^N q^{(\hat L_n)}_n.
\end{align}
It is worth noting each quadrature may have distinct points and weights; they need to not be constructed using
the same quadrature rule.
In general, one-dimensional Gaussian
quadrature excels in exactly integrating polynomials of order $2p-1$ using $p$ points and weights;
equivalently, it requires $(p+1)/2$ points to integrate an order $p$ polynomial. 
 
For convenience the coefficient integral to be evaluated is here reported again, Eq.
\ref{eq:polycoeff}.
\begin{equation}
  u_k = \langle u(Y)\Phi_k(Y) \rangle.
\end{equation}
This integral can be approximated with the appropriate Gaussian quadrature as
\begin{align}
  u_k &\approx Q^{(\vec{\hat L})}[u(Y)\Phi_k(Y)],
\end{align}
where bold vector notation is used to note the order of each individual quadrature,
$\vec{\hat L} = [\hat L_1, \ldots,\hat L_n,\ldots,\hat L_N]$. For clarity, the bold notation is removed and
it is assumed a one-dimensional problem, which extrapolates as expected into the multidimensional case.
\begin{align}
  u_k &\approx q^{(\hat L)}[u(Y)\Phi_k(Y)],\\
      &= \sum_{\ell=1}^{\hat L} w_\ell u(Y_\ell)\Phi_k(Y_\ell).
\end{align}
In order to determine the quadrature order $\hat L$ needed to accurately integrate this expression, let's consider the
gPC formulation for $u(Y)$ in Eq. \ref{eq:gPC} and replace it in the sum,
\begin{equation}
  u_k\approx \sum_{\ell=1}^{\hat L} w_\ell \Phi_k(Y_\ell) \sum_{k\in\Lambda(L)}u_{\hat k}\Phi_{\hat k}(Y_\ell).
\end{equation}
Using orthogonal properties of the polynomials, this reduces as $\hat L\to\infty$ to
\begin{equation}
  u_k\approx \sum_{\ell=1}^{\hat L} w_\ell u_k \Phi_k(Y_\ell)^2.
\end{equation}
Thus, the integral, to the same error introduced by truncating the  gPC expansion, the quadrature is
approximating an integral of order $2k$. As a result, the quadrature order should be order 
\begin{equation}
  p=\frac{2k+1}{2}=k+\frac{1}{2}<k+1,
\end{equation}
so it  can conservatively used  $p=k+1$.  In the case of the largest polynomials with order
$k=L$, the quadrature size $\hat L$ is the same as $L+1$.  It is worth noting that if $u(Y)$ is effectively of
much higher-order polynomial than $L$, this equality for quadrature order does not hold true; however, it also
means that gPC of order $L$ will be a poor approximation.

While a tensor product of highest-necessary quadrature orders could serve as a suitable multidimensional
quadrature set, we can make use of Smolyak-like sparse quadratures to reduce the number of function
evaluations necessary for the TD and HC polynomial index set construction strategies.

\paragraph{Smolyak Sparse Grids}
Smolyak sparse grids \cite{smolyak} are an attempt to discover the smallest necessary quadrature set to
integrate a multidimensional integral with varying orders of predetermined quadrature sets.  In RAVEN case, the
polynomial index sets determine the quadrature orders each one needs in each dimension to be integrated
accurately.  For example, the polynomial index set point (2,1,3) requires three points in $Y_1$, two in $Y_2$,
and four in $Y_3$,or
\begin{equation}
  Q^{(2,1,3)} = q^{(3)}_1 \otimes q^{(2)}_2 \otimes q^{(4)}_3.
\end{equation}
The full tensor grid of all collocation points would be the tensor product of all quadrature for all points,
or
\begin{equation}
  Q^{(\Lambda(L))} = \bigotimes_{k\in\Lambda}Q^{(k)}.
\end{equation}
Smolyak sparse grids consolidate this tensor form by adding together the points from tensor products of subset
quadrature sets.  Returning momentarily to a one-dimensional problem, let's introduce the notation
\begin{equation}
  \Delta_k^{(\hat L)}[f(x)] \equiv (q_k^{(\hat L)} - q_{k-1}^{(\hat L)})[f(x)],
\end{equation}
\begin{equation}
  q_0^{(\hat L)}[f(x)] = 0.
\end{equation}
A Smolyak sparse grid is then defined and applied to the desired integral in Eq. \ref{eq:polycoeff},
\begin{equation}
  S^{(\vec{\hat L})}_{\Lambda,N}[u(Y)\Phi_k(Y)] = \sum_{k\in\Lambda(L)} \left(\Delta_{k_1}^{(\hat L_1)} \otimes \cdots \otimes
  \Delta_{k_N}^{(\hat L_N)}\right)[u(Y)\Phi_k(Y)].
\end{equation}
Equivalently, and in a more algorithm-friendly approach,
\begin{equation}
  S^{(\vec{\hat L})}_{\Lambda,N}[u(Y)\Phi_k(Y)] = \sum_{k\in\Lambda(L)} c(k)\bigotimes_{n=1}^N
  q^{(\hat L_n)}_n[u(Y)\Phi_k(Y)]
\end{equation}
where
\begin{equation}
  c(k) = \sum_{\substack{j=\{0,1\}^N,\\k+j\in\Lambda}} (-1)^{|j|_1},
\end{equation}
using the traditional 1-norm for $|j|_1$.
The values for $u_k$ can then be calculated as
\begin{align}
  u_k &= \langle u(Y)\Phi_k(Y) \rangle,\\
      &\approx S^{(\vec{\hat L})}_{\Lambda,N}[u(Y)\Phi_k(Y)].
\end{align}
With this numerical method to determine coefficients,  a complete method for performing SCgPC
analysis in an algorithmic manner is obtained.
\subsubsection{Sparse Grid Collocation sampling through RAVEN}
\label{subsub:SGcsamplingExample}
The goals of this section are about learning how to:
 \begin{enumerate}
   \item Set up a Sparse Grid Collocation sampling for the construction of a suitable surrogate model of a driven code;
   \item Construct a GaussPolynomialRom surrogate model (training stage);
   \item Use the constructed GaussPolynomialRom surrogate model instead of the driven code.
\end{enumerate}  
In order to accomplish these tasks, the following RAVEN \textbf{Entities} (XML blocks in the input files) need to be defined:
\begin{enumerate}
   \item \textbf{\textit{RunInfo}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <RunInfo>
    <JobName>ChapterVII-I/SparseGrid</JobName>
    <Sequence>sample,Ntrain,sampleROM,writeHistories</Sequence>
    <WorkingDir>ChapterVII-I/SparseGrid</WorkingDir>
    <batchSize>12</batchSize>
  </RunInfo>
\end{lstlisting}   
   As reported in section~\ref{sub:EntitiesAndFlow}, the \textit{RunInfo} \textbf{Entity} is intended to set up the analysis 
   that the user wants to perform. In this specific case, four steps (\xmlNode{Sequence}) are going to be sequentially run 
   using 12 processors (\xmlNode{batchSize}). 
   \item \textbf{\textit{Files}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Files>
    <Input name="referenceInput.xml" type="input">referenceInput.xml</Input>
  </Files>
\end{lstlisting}
   Since the driven code uses a single input file, in this section the original input is placed. As detailed in the user manual
   the attribute  \xmlAttr{name} represents the alias that is going to be used in all the other input blocks in order to refer to this file.
   \item \textbf{\textit{Models}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Models>
    <Code name="testModel" subType="GenericCode">
      <executable>../physicalCode/analyticalbateman/AnalyticalDplMain.py</executable>
      <clargs arg="python" type="prepend"/>
      <clargs arg="" extension=".xml" type="input"/>
      <clargs arg=" " extension=".csv" type="output"/>
      <prepend>python</prepend>
    </Code>
    <ROM name="NROM" subType="GaussPolynomialRom">
        <Target>A,B,C,D</Target>
        <Features>sigma-A,sigma-B,sigma-C,sigma-D,decay-A,decay-B,decay-C,decay-D</Features>
        <IndexSet>TotalDegree</IndexSet>
        <PolynomialOrder>3</PolynomialOrder>
        <Interpolation poly="Legendre" quad="Legendre">sigma-A</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">sigma-B</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">sigma-C</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">sigma-D</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">decay-A</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">decay-B</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">decay-C</Interpolation>
        <Interpolation poly="Legendre" quad="Legendre">decay-D</Interpolation>
    </ROM>
  </Models>
\end{lstlisting}
 As mentioned above, the goal of this example is the generation of a \text{GaussPolynomialRom} 
 for sub-sequential usage instead of the original code. Indeed, in addition to the previously explained Code model,
 the ROM of type \textit{GaussPolynomialRom} is here specified. The ROM will be generated through a Sparse Grid
 Collocation sampling strategy. All the 4 targets $A,B,C,D$ are going to be modeled through this ROM as function
 of the uncertain parameters $sigmas$ and $decays$.
   \item \textbf{\textit{Distributions}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Distributions>
      <Uniform name="sigmaA">
          <lowerBound>6.9</lowerBound>
          <upperBound>8.1</upperBound>
      </Uniform>
      <Uniform name="sigmaB">
          <lowerBound>3.9</lowerBound>
          <upperBound>5.1</upperBound>
      </Uniform>
      <Uniform name="sigmaC">
          <lowerBound>1.9</lowerBound>
          <upperBound>3.1</upperBound>
      </Uniform>
      <Uniform name="sigmaD">
          <lowerBound>0.9</lowerBound>
          <upperBound>1.1</upperBound>
      </Uniform>
      <Uniform name="decayConstantA">
          <lowerBound>0.0000000038</lowerBound>
          <upperBound>0.0000000052</upperBound>
      </Uniform>
      <Uniform name="decayConstantB">
          <lowerBound>0.0000000058</lowerBound>
          <upperBound>0.0000000072</upperBound>
      </Uniform>
      <Uniform name="decayConstantC">
          <lowerBound>0.0000000068</lowerBound>
          <upperBound>0.0000000082</upperBound>
      </Uniform>
      <Uniform name="decayConstantD">
          <lowerBound>0.0000000078</lowerBound>
          <upperBound>0.0000000092</upperBound>
      </Uniform>
  </Distributions>
\end{lstlisting}
  In the Distributions XML section, the stochastic model for the 
  uncertainties  treated by the Sparse Grid Collocation sampling are reported. In 
  this case 8 distributions are defined: 
  \begin{itemize}
    \item $sigmaA \sim \mathbb{U}(6.9,8.1)$, used to model the uncertainty 
    associated with  the Model \textit{sigma-A};
    \item $sigmaB \sim \mathbb{U}(3.9,5.1)$, used to model the uncertainty 
    associated with  the Model \textit{sigma-B};
    \item $sigmaC \sim \mathbb{U}(1.9,3.1)$, used to model the uncertainty 
    associated with  the Model \textit{sigma-C};
    \item $sigmaD \sim \mathbb{U}(0.9,1.1)$, used to model the uncertainty 
    associated with  the Model \textit{sigma-D};
    \item  $decayConstantA \sim \mathbb{U}(3.8e-9,5.2e-9)$,  used to 
    model the uncertainty 
    associated with  the Model \textit{decay-A}.
    \item  $decayConstantB \sim \mathbb{U}(5.8e-9,7.2e-9)$,  used to 
    model the uncertainty 
    associated with  the Model \textit{decay-B}.
    \item  $decayConstantC \sim \mathbb{U}(6.8e-9,8.2e-9)$,  used to 
    model the uncertainty 
    associated with  the Model \textit{decay-C}.
    \item  $decayConstantD \sim \mathbb{U}(7.8e-9,9.2e-9)$,  used to 
    model the uncertainty 
    associated with  the Model \textit{decay-D}.
  \end{itemize}
   \item \textbf{\textit{Samplers}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Samplers>
    <SparseGridCollocation name="NSG" parallel="1">
      <variable name="sigma-A">
        <distribution>sigmaA</distribution>
      </variable>
      <variable name="sigma-B">
          <distribution>sigmaB</distribution>
      </variable>
      <variable name="sigma-C">
          <distribution>sigmaC</distribution>
      </variable>
      <variable name="sigma-D">
          <distribution>sigmaD</distribution>
      </variable>
      <variable name="decay-A">
          <distribution>decayConstantA</distribution>
      </variable>
      <variable name="decay-B">
          <distribution>decayConstantB</distribution>
      </variable>
      <variable name="decay-C">
          <distribution>decayConstantC</distribution>
      </variable>
      <variable name="decay-D">
          <distribution>decayConstantD</distribution>
      </variable>
      <ROM class="Models" type="ROM">NROM</ROM>
    </SparseGridCollocation>
  </Samplers> 
\end{lstlisting}
  In order to employ the Sparse Grid Collocation sampling strategy, a 
  \xmlNode{SparseGridCollocation} node needs to be inputted. 
  As it can be
  seen from above, each variable is associated to a different distribution,
  defined in the  \xmlNode{Distributions} block.
  In addition, the \textit{GaussPolynomialRom}  \xmlNode{ROM} is inputted. The setting of this ROM (e.g. polynomial order, Index set method, etc.) determines how the Stochastic Collocation Method is 
  employed.
   \item \textbf{\textit{DataObjects}}:
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <DataObjects>
    <PointSet name="inputPlaceHolder">
      <Input>sigma-A,sigma-B,sigma-C,sigma-D,decay-A,decay-B,decay-C,decay-D</Input>
      <Output>OutputPlaceHolder</Output>
    </PointSet>
    <PointSet name="samples">
      <Input>sigma-A,sigma-B,sigma-C,sigma-D,decay-A,decay-B,decay-C,decay-D</Input>
      <Output>A,B,C,D,time</Output>
    </PointSet>
    <PointSet name="samplesROM">
        <Input>sigma-A,sigma-B,sigma-C,sigma-D,decay-A,decay-B,decay-C,decay-D</Input>
        <Output>A,B,C,D</Output>
    </PointSet>
    <HistorySet name="histories">
        <Input>sigma-A,sigma-B,sigma-C,sigma-D,decay-A,decay-B,decay-C,decay-D</Input>
        <Output>A,B,C,D,time</Output>
    </HistorySet>
  </DataObjects>
\end{lstlisting}
  Int this block, four \textit{DataObjects} are defined: 1) PointSet named 
  ``samples'' used to collect the final outcomes of the code, 2) HistorySet named ``histories'' in which the full time responses of the variables $A,B,C,D$ are going to be stored, 3) PointSet named    
  ``inputPlaceHolder'' used as input of the sampling applied on the constructed ROM and 4) PointSet named ``samplesROM'' used to collect the final outcomes of the ROM perturbations.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure samples
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/histories_SparseGrid.png}
  \caption{Plot of the samples generated by the Stratified sampling for variables $A,B,C,D$.}
  \label{fig:historiesSparseGridPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
   \item \textbf{\textit{Steps}}:   
\begin{lstlisting}[style=XML,morekeywords={arg,extension,pauseAtEnd,overwrite}]
  <Steps>
    <MultiRun name="sample">
      <Input class="Files" type="input">referenceInput.xml</Input>
      <Model class="Models" type="Code">testModel</Model>
      <Sampler class="Samplers" type="SparseGridCollocation">NSG</Sampler>
      <Output class="DataObjects" type="PointSet">samples</Output>
      <Output class="DataObjects" type="HistorySet">histories</Output>
    </MultiRun>
    <RomTrainer name="Ntrain">
        <Input class="DataObjects" type="PointSet">samples</Input>
        <Output class="Models" type="ROM">NROM</Output>
    </RomTrainer>
    <MultiRun name="sampleROM" pauseAtEnd="false">
        <Input class="DataObjects" type="PointSet">inputPlaceHolder</Input>
        <Model class="Models" type="ROM">NROM</Model>
        <Sampler class="Samplers" type="SparseGridCollocation">NSG</Sampler>
        <Output class="DataObjects" type="PointSet">samplesROM</Output>
    </MultiRun>
    <IOStep name="writeHistories" pauseAtEnd="True">
        <Input class="DataObjects" type="HistorySet">histories</Input>
        <Input class="DataObjects" type="PointSet">samples</Input>
        <Input class="DataObjects" type="PointSet">samplesROM</Input>
        <Output 	class="OutStreamManager" type="Plot">samplesPlot3D</Output>
        <Output 	class="OutStreamManager" type="Plot">historyPlot</Output>
        <Output 	class="OutStreamManager" type="Plot">samplesROMPlot3D</Output>
        <Output 	class="OutStreamManager" type="Print">samples</Output>
        <Output 	class="OutStreamManager" type="Print">samplesROM</Output>
        <Output 	class="OutStreamManager" type="Print">histories</Output>
    </IOStep>
  </Steps>
\end{lstlisting}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure samples
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/samples_SparseGrid.png}
  \caption{Plot of the samples generated by the Stratified sampling for variables $A,B,C,D$.}
  \label{fig:samplesSparseGridPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   Finally, all the previously defined \textbf{Entities} can be combined in 
   the \xmlNode{Steps} block. As inferable, 
   4 \xmlNode{Steps} have been inputted:
   \begin{itemize}
     \item \xmlNode{MultiRun} named ``sample'', used to run the multiple  
     instances of the driven code and 
     collect the outputs in the two \textit{DataObjects}. As it can be
     seen, the \xmlNode{Sampler} is inputted to communicate to the 
     \textit{Step} that the driven code needs to
     be perturbed through the Sparse Grid Collocation  sampling;
     \item \xmlNode{RomTrainer} named ``Ntrain'', used to train (i.e. 
     construct) the Gauss Polynomial ROM. This step is essential if the
     user want to use the ROM in later steps;
     \item \xmlNode{MultiRun} named ``sampleROM'', used to run the multiple  
     instances of the previously constructed ROM and 
     collect the outputs in the \textit{samplesROM} \textit{DataObjects}.  
     As it can be seen, the same Sparse Grid Collocation sampler is
     here used.
     \item  \xmlNode{IOStep} named ``writeHistories'', used to 1) dump 
     the ``histories'', ``samples'' and ``samplesROM'' \textit{DataObjects} 
     \textbf{Entity} in a CSV file and 2) plot the data in the PNG file and 
     on the screen.
   \end{itemize}
\end{enumerate} 
 As previously mentioned, Figure~\ref{fig:historiesSparseGridPlotLine} 
 shows the evolution of the outputs $A,B,C,D$ under uncertainties. 
 Figures~\ref{fig:samplesSparseGridPlotLine} and 
 \ref{fig:samplesROMSparseGridPlotLine} show the final responses 
 of the sampling employed using the driven code and the ROM, 
 respectively. As it can be seen, the constructed ROM can perfectly
 represent the response of the driven code. This example shows the
 potential of reduced order modeling, in general, and of the 
 \textit{GaussPolynomialRom}, in particular.
 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %figure samples
 \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.7]{pics/samplesROM_SparseGrid.png}
  \caption{Plot of the samples generated by the Stratified sampling for variables $A,B,C,D$.}
  \label{fig:samplesROMSparseGridPlotLine}
 \end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








